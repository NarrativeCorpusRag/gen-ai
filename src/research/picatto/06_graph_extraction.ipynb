{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae1a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "from relik import Relik\n",
    "import fsspec\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1415264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"gs://gen-ai-tu/news/chunks/\"\n",
    "OUTPUT_PATH = \"gs://gen-ai-tu/news/graph_rag/\"\n",
    "CHECKPOINT_FILE = Path(OUTPUT_PATH) / \".checkpoint.json\"\n",
    "LOCAL_CHECKPOINT = \"/home/user/hernan/graph_rag_checkpoint.json\"\n",
    "\n",
    "NUM_GPUS = 2\n",
    "BATCH_SIZE = 256  # Can be higher with small models\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Both models\n",
    "ENTITY_MODEL = \"sapienzanlp/relik-entity-linking-small\"\n",
    "RELATION_MODEL = \"sapienzanlp/relik-relation-extraction-sre-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b350d8",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# GCS FILE LISTING\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff1138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gcs_parquet_files(gcs_path: str) -> List[str]:\n",
    "    \"\"\"List all parquet files in GCS path\"\"\"\n",
    "    fs = fsspec.filesystem(\"gcs\")\n",
    "    bucket_path = gcs_path.replace(\"gs://\", \"\")\n",
    "    all_files = fs.glob(f\"{bucket_path}**/*.parquet\")\n",
    "    return [f\"gs://{f}\" for f in all_files]\n",
    "\n",
    "\n",
    "def read_parquet_gcs(gcs_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Read parquet from GCS\"\"\"\n",
    "    return pl.read_parquet(gcs_path)\n",
    "\n",
    "\n",
    "def write_parquet_gcs(df: pl.DataFrame, gcs_path: str):\n",
    "    \"\"\"Write parquet to GCS\"\"\"\n",
    "    df.write_parquet(gcs_path, compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5910432",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PROCESS OUTPUT FROM BOTH MODELS\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55f7d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dual_model_output(\n",
    "    entity_response, relation_response, chunk_id: str, confidence_threshold: float\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Combine outputs from entity linking and relation extraction models.\n",
    "\n",
    "    - entity_response: from relik-entity-linking (has Wikipedia IDs)\n",
    "    - relation_response: from relik-relation-extraction (has triplets)\n",
    "    \"\"\"\n",
    "\n",
    "    entities = []\n",
    "    entity_text_to_id = {}\n",
    "    entity_text_to_wikipedia = {}\n",
    "\n",
    "    # --- ENTITIES (from entity linking model) ---\n",
    "    if entity_response and hasattr(entity_response, \"spans\") and entity_response.spans:\n",
    "        for span in entity_response.spans:\n",
    "            ent_id = hashlib.sha256(\n",
    "                f\"{chunk_id}:{span.text}:{span.start}\".encode()\n",
    "            ).hexdigest()[:16]\n",
    "            entity_text_to_id[span.text] = ent_id\n",
    "\n",
    "            # Get Wikipedia ID (entity linking provides this)\n",
    "            wikipedia_id = None\n",
    "            if hasattr(span, \"id\") and span.id:\n",
    "                wikipedia_id = span.id\n",
    "                entity_text_to_wikipedia[span.text] = wikipedia_id\n",
    "\n",
    "            ent = {\n",
    "                \"entity_id\": ent_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": span.text,\n",
    "                \"label\": getattr(span, \"label\", \"ENTITY\"),\n",
    "                \"start\": span.start,\n",
    "                \"end\": span.end,\n",
    "                \"wikipedia_id\": wikipedia_id,\n",
    "            }\n",
    "            entities.append(ent)\n",
    "\n",
    "    # --- RELATIONS (from relation extraction model) ---\n",
    "    relations = []\n",
    "    if (\n",
    "        relation_response\n",
    "        and hasattr(relation_response, \"triplets\")\n",
    "        and relation_response.triplets\n",
    "    ):\n",
    "        for triplet in relation_response.triplets:\n",
    "            conf = getattr(triplet, \"confidence\", 1.0)\n",
    "            if conf >= confidence_threshold:\n",
    "                head_text = triplet.subject.text\n",
    "                tail_text = triplet.object.text\n",
    "                rel_id = hashlib.sha256(\n",
    "                    f\"{chunk_id}:{head_text}:{triplet.label}:{tail_text}\".encode()\n",
    "                ).hexdigest()[:16]\n",
    "\n",
    "                # Link to entities from entity linking model\n",
    "                relations.append(\n",
    "                    {\n",
    "                        \"relation_id\": rel_id,\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"head_text\": head_text,\n",
    "                        \"head_id\": entity_text_to_id.get(head_text),\n",
    "                        \"head_wikipedia_id\": entity_text_to_wikipedia.get(head_text),\n",
    "                        \"relation\": triplet.label,\n",
    "                        \"tail_text\": tail_text,\n",
    "                        \"tail_id\": entity_text_to_id.get(tail_text),\n",
    "                        \"tail_wikipedia_id\": entity_text_to_wikipedia.get(tail_text),\n",
    "                        \"confidence\": float(conf),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return entities, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a288",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# GPU WORKER\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d40e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_worker(\n",
    "    gpu_id: int,\n",
    "    file_queue: mp.Queue,\n",
    "    result_queue: mp.Queue,\n",
    "    batch_size: int,\n",
    "    output_path: str,\n",
    "    entity_model_name: str,\n",
    "    relation_model_name: str,\n",
    "    confidence_threshold: float,\n",
    "):\n",
    "    \"\"\"Worker process with both entity linking and relation extraction models\"\"\"\n",
    "    import torch\n",
    "    from relik import Relik\n",
    "\n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "\n",
    "    # Load BOTH models on same GPU\n",
    "    print(f\"[GPU {gpu_id}] Loading entity model: {entity_model_name}\")\n",
    "    entity_model = Relik.from_pretrained(entity_model_name, device=device)\n",
    "\n",
    "    print(f\"[GPU {gpu_id}] Loading relation model: {relation_model_name}\")\n",
    "    relation_model = Relik.from_pretrained(relation_model_name, device=device)\n",
    "\n",
    "    print(f\"[GPU {gpu_id}] Both models loaded!\")\n",
    "\n",
    "    while True:\n",
    "        item = file_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "\n",
    "        file_idx, parquet_file = item\n",
    "\n",
    "        try:\n",
    "            df = pl.read_parquet(parquet_file)\n",
    "\n",
    "            if df.is_empty():\n",
    "                result_queue.put((file_idx, 0, 0, 0, True))\n",
    "                continue\n",
    "\n",
    "            all_chunks = []\n",
    "            all_entities = []\n",
    "            all_relations = []\n",
    "\n",
    "            for batch_start in range(0, len(df), batch_size):\n",
    "                batch_df = df.slice(batch_start, batch_size)\n",
    "                texts = batch_df[\"chunk_text\"].to_list()\n",
    "\n",
    "                # Filter empty\n",
    "                valid_mask = [bool(t and len(str(t).strip()) > 10) for t in texts]\n",
    "                valid_texts = [t for t, v in zip(texts, valid_mask) if v]\n",
    "\n",
    "                # Initialize predictions\n",
    "                entity_preds = [None] * len(texts)\n",
    "                relation_preds = [None] * len(texts)\n",
    "\n",
    "                if valid_texts:\n",
    "                    # --- ENTITY LINKING ---\n",
    "                    try:\n",
    "                        ent_results = entity_model(valid_texts)\n",
    "                        if not isinstance(ent_results, list):\n",
    "                            ent_results = [ent_results]\n",
    "\n",
    "                        valid_idx = 0\n",
    "                        for i, v in enumerate(valid_mask):\n",
    "                            if v:\n",
    "                                entity_preds[i] = ent_results[valid_idx]\n",
    "                                valid_idx += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"[GPU {gpu_id}] Entity model error: {e}\")\n",
    "\n",
    "                    # --- RELATION EXTRACTION ---\n",
    "                    try:\n",
    "                        rel_results = relation_model(valid_texts)\n",
    "                        if not isinstance(rel_results, list):\n",
    "                            rel_results = [rel_results]\n",
    "\n",
    "                        valid_idx = 0\n",
    "                        for i, v in enumerate(valid_mask):\n",
    "                            if v:\n",
    "                                relation_preds[i] = rel_results[valid_idx]\n",
    "                                valid_idx += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"[GPU {gpu_id}] Relation model error: {e}\")\n",
    "\n",
    "                # Process results\n",
    "                chunk_ids = batch_df[\"chunk_id\"].to_list()\n",
    "                doc_ids = batch_df[\"doc_id\"].to_list()\n",
    "                chunk_indices = batch_df[\"chunk_index\"].to_list()\n",
    "\n",
    "                meta_cols = [\"uri\", \"host\", \"year\", \"month\"]\n",
    "                meta_data = {\n",
    "                    col: batch_df[col].to_list()\n",
    "                    for col in meta_cols\n",
    "                    if col in batch_df.columns\n",
    "                }\n",
    "\n",
    "                for i in range(len(texts)):\n",
    "                    chunk_id = chunk_ids[i]\n",
    "\n",
    "                    # Combine outputs from both models\n",
    "                    entities, relations = process_dual_model_output(\n",
    "                        entity_preds[i],\n",
    "                        relation_preds[i],\n",
    "                        chunk_id,\n",
    "                        confidence_threshold,\n",
    "                    )\n",
    "\n",
    "                    all_entities.extend(entities)\n",
    "                    all_relations.extend(relations)\n",
    "\n",
    "                    chunk_doc = {\n",
    "                        \"chunk_id\": chunk_id,\n",
    "                        \"doc_id\": doc_ids[i],\n",
    "                        \"chunk_index\": chunk_indices[i],\n",
    "                        \"chunk_text\": texts[i],\n",
    "                        \"entities\": entities,\n",
    "                        \"relations\": relations,\n",
    "                        \"entity_count\": len(entities),\n",
    "                        \"relation_count\": len(relations),\n",
    "                    }\n",
    "                    for col in meta_cols:\n",
    "                        if col in meta_data:\n",
    "                            chunk_doc[col] = meta_data[col][i]\n",
    "\n",
    "                    all_chunks.append(chunk_doc)\n",
    "\n",
    "                del batch_df\n",
    "\n",
    "            # Write outputs to GCS\n",
    "            if all_chunks:\n",
    "                chunks_df = pl.DataFrame(all_chunks)\n",
    "                chunks_df.write_parquet(\n",
    "                    f\"{output_path}chunks_graph/chunks_{file_idx:05d}.parquet\",\n",
    "                    compression=\"zstd\",\n",
    "                )\n",
    "                del chunks_df\n",
    "\n",
    "            if all_entities:\n",
    "                entities_df = pl.DataFrame(all_entities)\n",
    "                entities_df.write_parquet(\n",
    "                    f\"{output_path}entities/entities_{file_idx:05d}.parquet\",\n",
    "                    compression=\"zstd\",\n",
    "                )\n",
    "                del entities_df\n",
    "\n",
    "            if all_relations:\n",
    "                relations_df = pl.DataFrame(all_relations)\n",
    "                relations_df.write_parquet(\n",
    "                    f\"{output_path}relations/relations_{file_idx:05d}.parquet\",\n",
    "                    compression=\"zstd\",\n",
    "                )\n",
    "                del relations_df\n",
    "\n",
    "            result_queue.put(\n",
    "                (file_idx, len(all_chunks), len(all_entities), len(all_relations), True)\n",
    "            )\n",
    "\n",
    "            del df, all_chunks, all_entities, all_relations\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[GPU {gpu_id}] Error on {parquet_file}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            result_queue.put((file_idx, 0, 0, 0, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f7f3e",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CHECKPOINTING\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c69b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    if Path(LOCAL_CHECKPOINT).exists():\n",
    "        with open(LOCAL_CHECKPOINT) as f:\n",
    "            data = json.load(f)\n",
    "            return (\n",
    "                set(data.get(\"completed_files\", [])),\n",
    "                data.get(\"total_chunks\", 0),\n",
    "                data.get(\"total_entities\", 0),\n",
    "                data.get(\"total_relations\", 0),\n",
    "            )\n",
    "    return set(), 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b3ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(completed_files, total_chunks, total_entities, total_relations):\n",
    "    with open(LOCAL_CHECKPOINT, \"w\") as f:\n",
    "        json.dump(\n",
    "            {\n",
    "                \"completed_files\": list(completed_files),\n",
    "                \"total_chunks\": total_chunks,\n",
    "                \"total_entities\": total_entities,\n",
    "                \"total_relations\": total_relations,\n",
    "                \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            },\n",
    "            f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ee242",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# MAIN - MULTI-GPU\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c271ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multi_gpu(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    num_gpus: int = 2,\n",
    "    batch_size: int = 256,\n",
    "    entity_model: str = ENTITY_MODEL,\n",
    "    relation_model: str = RELATION_MODEL,\n",
    "    confidence_threshold: float = CONFIDENCE_THRESHOLD,\n",
    "):\n",
    "    \"\"\"Process with multiple GPUs, each running both models\"\"\"\n",
    "\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "\n",
    "    print(f\"Entity model: {entity_model}\")\n",
    "    print(f\"Relation model: {relation_model}\")\n",
    "\n",
    "    print(f\"Listing files from {input_path}...\")\n",
    "    input_files = list_gcs_parquet_files(input_path)\n",
    "    input_files = sorted(input_files)\n",
    "    print(f\"Found {len(input_files)} files\")\n",
    "\n",
    "    if not input_files:\n",
    "        print(\"No files found!\")\n",
    "        return\n",
    "\n",
    "    completed_files, total_chunks, total_entities, total_relations = load_checkpoint()\n",
    "    files_to_process = [\n",
    "        (i, f) for i, f in enumerate(input_files) if i not in completed_files\n",
    "    ]\n",
    "    print(f\"Files to process: {len(files_to_process)} / {len(input_files)}\")\n",
    "\n",
    "    if not files_to_process:\n",
    "        print(\"All files already processed!\")\n",
    "        return\n",
    "\n",
    "    file_queue = mp.Queue()\n",
    "    result_queue = mp.Queue()\n",
    "\n",
    "    # Start GPU workers\n",
    "    workers = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        p = mp.Process(\n",
    "            target=gpu_worker,\n",
    "            args=(\n",
    "                gpu_id,\n",
    "                file_queue,\n",
    "                result_queue,\n",
    "                batch_size,\n",
    "                output_path,\n",
    "                entity_model,\n",
    "                relation_model,\n",
    "                confidence_threshold,\n",
    "            ),\n",
    "        )\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "        print(f\"Started worker for GPU {gpu_id}\")\n",
    "\n",
    "    # Feed files\n",
    "    for file_idx, parquet_file in files_to_process:\n",
    "        file_queue.put((file_idx, parquet_file))\n",
    "\n",
    "    for _ in workers:\n",
    "        file_queue.put(None)\n",
    "\n",
    "    # Collect results\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(total=len(files_to_process), desc=\"Processing\")\n",
    "\n",
    "    results_received = 0\n",
    "    while results_received < len(files_to_process):\n",
    "        file_idx, chunks, entities, relations, success = result_queue.get()\n",
    "\n",
    "        if success:\n",
    "            completed_files.add(file_idx)\n",
    "            total_chunks += chunks\n",
    "            total_entities += entities\n",
    "            total_relations += relations\n",
    "\n",
    "        results_received += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        chunks_per_sec = total_chunks / elapsed if elapsed > 0 else 0\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\n",
    "                \"chunks\": f\"{total_chunks:,}\",\n",
    "                \"ent\": f\"{total_entities:,}\",\n",
    "                \"rel\": f\"{total_relations:,}\",\n",
    "                \"c/s\": f\"{chunks_per_sec:.1f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if results_received % 20 == 0:\n",
    "            save_checkpoint(\n",
    "                completed_files, total_chunks, total_entities, total_relations\n",
    "            )\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    for p in workers:\n",
    "        p.join()\n",
    "\n",
    "    save_checkpoint(completed_files, total_chunks, total_entities, total_relations)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"COMPLETE!\")\n",
    "    print(f\"Chunks: {total_chunks:,}\")\n",
    "    print(f\"Entities: {total_entities:,}\")\n",
    "    print(f\"Relations: {total_relations:,}\")\n",
    "    print(f\"Time: {elapsed / 3600:.2f} hours\")\n",
    "    print(f\"Speed: {total_chunks / elapsed:.1f} chunks/sec\")\n",
    "    print(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30758515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity model: sapienzanlp/relik-entity-linking-small\n",
      "Relation model: sapienzanlp/relik-relation-extraction-sre-small\n",
      "Listing files from gs://gen-ai-tu/news/chunks/...\n",
      "Found 2643 files\n",
      "Files to process: 2643 / 2643\n",
      "Started worker for GPU 0\n",
      "Started worker for GPU 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/2643 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/user/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/user/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/user/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'gpu_worker' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_multi_gpu\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mINPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_GPUS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mprocess_multi_gpu\u001b[39m\u001b[34m(input_path, output_path, num_gpus, batch_size, entity_model, relation_model, confidence_threshold)\u001b[39m\n\u001b[32m     71\u001b[39m results_received = \u001b[32m0\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m results_received < \u001b[38;5;28mlen\u001b[39m(files_to_process):\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     file_idx, chunks, entities, relations, success = \u001b[43mresult_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m     76\u001b[39m         completed_files.add(file_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/queues.py:103\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._rlock:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m         res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28mself\u001b[39m._sem.release()\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/connection.py:216\u001b[39m, in \u001b[36m_ConnectionBase.recv_bytes\u001b[39m\u001b[34m(self, maxlength)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength < \u001b[32m0\u001b[39m:\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mnegative maxlength\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mself\u001b[39m._bad_message_length()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/connection.py:430\u001b[39m, in \u001b[36mConnection._recv_bytes\u001b[39m\u001b[34m(self, maxsize)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     buf = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m     size, = struct.unpack(\u001b[33m\"\u001b[39m\u001b[33m!i\u001b[39m\u001b[33m\"\u001b[39m, buf.getvalue())\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m size == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/multiprocessing/connection.py:395\u001b[39m, in \u001b[36mConnection._recv\u001b[39m\u001b[34m(self, size, read)\u001b[39m\n\u001b[32m    393\u001b[39m remaining = size\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     chunk = \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n == \u001b[32m0\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "process_multi_gpu(INPUT_PATH, OUTPUT_PATH, num_gpus=NUM_GPUS, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-bis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
