{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae1a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/user/hernan/gen-ai/.pixi/envs/graph-bis/lib/python3.12/site-packages/lightning/fabric/__init__.py:41: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "import gc\n",
    "from relik import Relik\n",
    "import fsspec\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1415264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_PATH = \"gs://gen-ai-tu/news/chunks/\"\n",
    "OUTPUT_PATH = \"gs://gen-ai-tu/news/graph_rag/\"\n",
    "CHECKPOINT_FILE = Path(OUTPUT_PATH) / \".checkpoint.json\"\n",
    "LOCAL_CHECKPOINT = \"/home/user/hernan/graph_rag_checkpoint.json\"\n",
    "\n",
    "NUM_GPUS = 2\n",
    "BATCH_SIZE = 256  # Can be higher with small models\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# Both models\n",
    "ENTITY_MODEL = \"sapienzanlp/relik-entity-linking-small\"\n",
    "RELATION_MODEL = \"sapienzanlp/relik-relation-extraction-sre-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b350d8",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# GCS FILE LISTING\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ff1138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_gcs_parquet_files(gcs_path: str) -> List[str]:\n",
    "    \"\"\"List all parquet files in GCS path\"\"\"\n",
    "    fs = fsspec.filesystem('gcs')\n",
    "    bucket_path = gcs_path.replace(\"gs://\", \"\")\n",
    "    all_files = fs.glob(f\"{bucket_path}**/*.parquet\")\n",
    "    return [f\"gs://{f}\" for f in all_files]\n",
    "\n",
    "\n",
    "def read_parquet_gcs(gcs_path: str) -> pl.DataFrame:\n",
    "    \"\"\"Read parquet from GCS\"\"\"\n",
    "    return pl.read_parquet(gcs_path)\n",
    "\n",
    "\n",
    "def write_parquet_gcs(df: pl.DataFrame, gcs_path: str):\n",
    "    \"\"\"Write parquet to GCS\"\"\"\n",
    "    df.write_parquet(gcs_path, compression='zstd')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5910432",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PROCESS OUTPUT FROM BOTH MODELS\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f7d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dual_model_output(\n",
    "    entity_response,\n",
    "    relation_response,\n",
    "    chunk_id: str,\n",
    "    confidence_threshold: float\n",
    ") -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"\n",
    "    Combine outputs from entity linking and relation extraction models.\n",
    "    \n",
    "    - entity_response: from relik-entity-linking (has Wikipedia IDs)\n",
    "    - relation_response: from relik-relation-extraction (has triplets)\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    entity_text_to_id = {}\n",
    "    entity_text_to_wikipedia = {}\n",
    "    \n",
    "    # --- ENTITIES (from entity linking model) ---\n",
    "    if entity_response and hasattr(entity_response, 'spans') and entity_response.spans:\n",
    "        for span in entity_response.spans:\n",
    "            ent_id = hashlib.sha256(f\"{chunk_id}:{span.text}:{span.start}\".encode()).hexdigest()[:16]\n",
    "            entity_text_to_id[span.text] = ent_id\n",
    "            \n",
    "            # Get Wikipedia ID (entity linking provides this)\n",
    "            wikipedia_id = None\n",
    "            if hasattr(span, 'id') and span.id:\n",
    "                wikipedia_id = span.id\n",
    "                entity_text_to_wikipedia[span.text] = wikipedia_id\n",
    "            \n",
    "            ent = {\n",
    "                \"entity_id\": ent_id,\n",
    "                \"chunk_id\": chunk_id,\n",
    "                \"text\": span.text,\n",
    "                \"label\": getattr(span, 'label', 'ENTITY'),\n",
    "                \"start\": span.start,\n",
    "                \"end\": span.end,\n",
    "                \"wikipedia_id\": wikipedia_id,\n",
    "            }\n",
    "            entities.append(ent)\n",
    "    \n",
    "    # --- RELATIONS (from relation extraction model) ---\n",
    "    relations = []\n",
    "    if relation_response and hasattr(relation_response, 'triplets') and relation_response.triplets:\n",
    "        for triplet in relation_response.triplets:\n",
    "            conf = getattr(triplet, 'confidence', 1.0)\n",
    "            if conf >= confidence_threshold:\n",
    "                head_text = triplet.subject.text\n",
    "                tail_text = triplet.object.text\n",
    "                rel_id = hashlib.sha256(\n",
    "                    f\"{chunk_id}:{head_text}:{triplet.label}:{tail_text}\".encode()\n",
    "                ).hexdigest()[:16]\n",
    "                \n",
    "                # Link to entities from entity linking model\n",
    "                relations.append({\n",
    "                    \"relation_id\": rel_id,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"head_text\": head_text,\n",
    "                    \"head_id\": entity_text_to_id.get(head_text),\n",
    "                    \"head_wikipedia_id\": entity_text_to_wikipedia.get(head_text),\n",
    "                    \"relation\": triplet.label,\n",
    "                    \"tail_text\": tail_text,\n",
    "                    \"tail_id\": entity_text_to_id.get(tail_text),\n",
    "                    \"tail_wikipedia_id\": entity_text_to_wikipedia.get(tail_text),\n",
    "                    \"confidence\": float(conf)\n",
    "                })\n",
    "    \n",
    "    return entities, relations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf4a288",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# GPU WORKER\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d40e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_worker(\n",
    "    gpu_id: int,\n",
    "    file_queue: mp.Queue,\n",
    "    result_queue: mp.Queue,\n",
    "    batch_size: int,\n",
    "    output_path: str,\n",
    "    entity_model_name: str,\n",
    "    relation_model_name: str,\n",
    "    confidence_threshold: float\n",
    "):\n",
    "    \"\"\"Worker process with both entity linking and relation extraction models\"\"\"\n",
    "    import torch\n",
    "    from relik import Relik\n",
    "    \n",
    "    device = f\"cuda:{gpu_id}\"\n",
    "    \n",
    "    # Load BOTH models on same GPU\n",
    "    print(f\"[GPU {gpu_id}] Loading entity model: {entity_model_name}\")\n",
    "    entity_model = Relik.from_pretrained(entity_model_name, device=device)\n",
    "    \n",
    "    print(f\"[GPU {gpu_id}] Loading relation model: {relation_model_name}\")\n",
    "    relation_model = Relik.from_pretrained(relation_model_name, device=device)\n",
    "    \n",
    "    print(f\"[GPU {gpu_id}] Both models loaded!\")\n",
    "    \n",
    "    while True:\n",
    "        item = file_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "        \n",
    "        file_idx, parquet_file = item\n",
    "        \n",
    "        try:\n",
    "            df = pl.read_parquet(parquet_file)\n",
    "            \n",
    "            if df.is_empty():\n",
    "                result_queue.put((file_idx, 0, 0, 0, True))\n",
    "                continue\n",
    "            \n",
    "            all_chunks = []\n",
    "            all_entities = []\n",
    "            all_relations = []\n",
    "            \n",
    "            for batch_start in range(0, len(df), batch_size):\n",
    "                batch_df = df.slice(batch_start, batch_size)\n",
    "                texts = batch_df['chunk_text'].to_list()\n",
    "                \n",
    "                # Filter empty\n",
    "                valid_mask = [bool(t and len(str(t).strip()) > 10) for t in texts]\n",
    "                valid_texts = [t for t, v in zip(texts, valid_mask) if v]\n",
    "                \n",
    "                # Initialize predictions\n",
    "                entity_preds = [None] * len(texts)\n",
    "                relation_preds = [None] * len(texts)\n",
    "                \n",
    "                if valid_texts:\n",
    "                    # --- ENTITY LINKING ---\n",
    "                    try:\n",
    "                        ent_results = entity_model(valid_texts)\n",
    "                        if not isinstance(ent_results, list):\n",
    "                            ent_results = [ent_results]\n",
    "                        \n",
    "                        valid_idx = 0\n",
    "                        for i, v in enumerate(valid_mask):\n",
    "                            if v:\n",
    "                                entity_preds[i] = ent_results[valid_idx]\n",
    "                                valid_idx += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"[GPU {gpu_id}] Entity model error: {e}\")\n",
    "                    \n",
    "                    # --- RELATION EXTRACTION ---\n",
    "                    try:\n",
    "                        rel_results = relation_model(valid_texts)\n",
    "                        if not isinstance(rel_results, list):\n",
    "                            rel_results = [rel_results]\n",
    "                        \n",
    "                        valid_idx = 0\n",
    "                        for i, v in enumerate(valid_mask):\n",
    "                            if v:\n",
    "                                relation_preds[i] = rel_results[valid_idx]\n",
    "                                valid_idx += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"[GPU {gpu_id}] Relation model error: {e}\")\n",
    "                \n",
    "                # Process results\n",
    "                chunk_ids = batch_df['chunk_id'].to_list()\n",
    "                doc_ids = batch_df['doc_id'].to_list()\n",
    "                chunk_indices = batch_df['chunk_index'].to_list()\n",
    "                \n",
    "                meta_cols = ['uri', 'host', 'year', 'month']\n",
    "                meta_data = {col: batch_df[col].to_list() for col in meta_cols if col in batch_df.columns}\n",
    "                \n",
    "                for i in range(len(texts)):\n",
    "                    chunk_id = chunk_ids[i]\n",
    "                    \n",
    "                    # Combine outputs from both models\n",
    "                    entities, relations = process_dual_model_output(\n",
    "                        entity_preds[i],\n",
    "                        relation_preds[i],\n",
    "                        chunk_id,\n",
    "                        confidence_threshold\n",
    "                    )\n",
    "                    \n",
    "                    all_entities.extend(entities)\n",
    "                    all_relations.extend(relations)\n",
    "                    \n",
    "                    chunk_doc = {\n",
    "                        'chunk_id': chunk_id,\n",
    "                        'doc_id': doc_ids[i],\n",
    "                        'chunk_index': chunk_indices[i],\n",
    "                        'chunk_text': texts[i],\n",
    "                        'entities': entities,\n",
    "                        'relations': relations,\n",
    "                        'entity_count': len(entities),\n",
    "                        'relation_count': len(relations),\n",
    "                    }\n",
    "                    for col in meta_cols:\n",
    "                        if col in meta_data:\n",
    "                            chunk_doc[col] = meta_data[col][i]\n",
    "                    \n",
    "                    all_chunks.append(chunk_doc)\n",
    "                \n",
    "                del batch_df\n",
    "            \n",
    "            # Write outputs to GCS\n",
    "            if all_chunks:\n",
    "                chunks_df = pl.DataFrame(all_chunks)\n",
    "                chunks_df.write_parquet(\n",
    "                    f\"{output_path}chunks_graph/chunks_{file_idx:05d}.parquet\",\n",
    "                    compression='zstd'\n",
    "                )\n",
    "                del chunks_df\n",
    "            \n",
    "            if all_entities:\n",
    "                entities_df = pl.DataFrame(all_entities)\n",
    "                entities_df.write_parquet(\n",
    "                    f\"{output_path}entities/entities_{file_idx:05d}.parquet\",\n",
    "                    compression='zstd'\n",
    "                )\n",
    "                del entities_df\n",
    "            \n",
    "            if all_relations:\n",
    "                relations_df = pl.DataFrame(all_relations)\n",
    "                relations_df.write_parquet(\n",
    "                    f\"{output_path}relations/relations_{file_idx:05d}.parquet\",\n",
    "                    compression='zstd'\n",
    "                )\n",
    "                del relations_df\n",
    "            \n",
    "            result_queue.put((file_idx, len(all_chunks), len(all_entities), len(all_relations), True))\n",
    "            \n",
    "            del df, all_chunks, all_entities, all_relations\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"[GPU {gpu_id}] Error on {parquet_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            result_queue.put((file_idx, 0, 0, 0, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f7f3e",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CHECKPOINTING\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c69b76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint():\n",
    "    if Path(LOCAL_CHECKPOINT).exists():\n",
    "        with open(LOCAL_CHECKPOINT) as f:\n",
    "            data = json.load(f)\n",
    "            return (\n",
    "                set(data.get('completed_files', [])),\n",
    "                data.get('total_chunks', 0),\n",
    "                data.get('total_entities', 0),\n",
    "                data.get('total_relations', 0)\n",
    "            )\n",
    "    return set(), 0, 0, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b3ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(completed_files, total_chunks, total_entities, total_relations):\n",
    "    with open(LOCAL_CHECKPOINT, 'w') as f:\n",
    "        json.dump({\n",
    "            'completed_files': list(completed_files),\n",
    "            'total_chunks': total_chunks,\n",
    "            'total_entities': total_entities,\n",
    "            'total_relations': total_relations,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ee242",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# MAIN - MULTI-GPU\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c271ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multi_gpu(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    num_gpus: int = 2,\n",
    "    batch_size: int = 256,\n",
    "    entity_model: str = ENTITY_MODEL,\n",
    "    relation_model: str = RELATION_MODEL,\n",
    "    confidence_threshold: float = CONFIDENCE_THRESHOLD\n",
    "):\n",
    "    \"\"\"Process with multiple GPUs, each running both models\"\"\"\n",
    "    \n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    \n",
    "    print(f\"Entity model: {entity_model}\")\n",
    "    print(f\"Relation model: {relation_model}\")\n",
    "    \n",
    "    print(f\"Listing files from {input_path}...\")\n",
    "    input_files = list_gcs_parquet_files(input_path)\n",
    "    input_files = sorted(input_files)\n",
    "    print(f\"Found {len(input_files)} files\")\n",
    "    \n",
    "    if not input_files:\n",
    "        print(\"No files found!\")\n",
    "        return\n",
    "    \n",
    "    completed_files, total_chunks, total_entities, total_relations = load_checkpoint()\n",
    "    files_to_process = [\n",
    "        (i, f) for i, f in enumerate(input_files)\n",
    "        if i not in completed_files\n",
    "    ]\n",
    "    print(f\"Files to process: {len(files_to_process)} / {len(input_files)}\")\n",
    "    \n",
    "    if not files_to_process:\n",
    "        print(\"All files already processed!\")\n",
    "        return\n",
    "    \n",
    "    file_queue = mp.Queue()\n",
    "    result_queue = mp.Queue()\n",
    "    \n",
    "    # Start GPU workers\n",
    "    workers = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        p = mp.Process(\n",
    "            target=gpu_worker,\n",
    "            args=(\n",
    "                gpu_id,\n",
    "                file_queue,\n",
    "                result_queue,\n",
    "                batch_size,\n",
    "                output_path,\n",
    "                entity_model,\n",
    "                relation_model,\n",
    "                confidence_threshold\n",
    "            )\n",
    "        )\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "        print(f\"Started worker for GPU {gpu_id}\")\n",
    "    \n",
    "    # Feed files\n",
    "    for file_idx, parquet_file in files_to_process:\n",
    "        file_queue.put((file_idx, parquet_file))\n",
    "    \n",
    "    for _ in workers:\n",
    "        file_queue.put(None)\n",
    "    \n",
    "    # Collect results\n",
    "    start_time = time.time()\n",
    "    pbar = tqdm(total=len(files_to_process), desc=\"Processing\")\n",
    "    \n",
    "    results_received = 0\n",
    "    while results_received < len(files_to_process):\n",
    "        file_idx, chunks, entities, relations, success = result_queue.get()\n",
    "        \n",
    "        if success:\n",
    "            completed_files.add(file_idx)\n",
    "            total_chunks += chunks\n",
    "            total_entities += entities\n",
    "            total_relations += relations\n",
    "        \n",
    "        results_received += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        chunks_per_sec = total_chunks / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'chunks': f'{total_chunks:,}',\n",
    "            'ent': f'{total_entities:,}',\n",
    "            'rel': f'{total_relations:,}',\n",
    "            'c/s': f'{chunks_per_sec:.1f}'\n",
    "        })\n",
    "        \n",
    "        if results_received % 20 == 0:\n",
    "            save_checkpoint(completed_files, total_chunks, total_entities, total_relations)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    for p in workers:\n",
    "        p.join()\n",
    "    \n",
    "    save_checkpoint(completed_files, total_chunks, total_entities, total_relations)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE!\")\n",
    "    print(f\"Chunks: {total_chunks:,}\")\n",
    "    print(f\"Entities: {total_entities:,}\")\n",
    "    print(f\"Relations: {total_relations:,}\")\n",
    "    print(f\"Time: {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Speed: {total_chunks/elapsed:.1f} chunks/sec\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30758515",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_multi_gpu(\n",
    "    INPUT_PATH,\n",
    "    OUTPUT_PATH,\n",
    "    num_gpus=NUM_GPUS,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-bis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
