{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b825f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import polars as pl\n",
    "import botocore\n",
    "import tarfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from fastwarc import ArchiveIterator\n",
    "from fastwarc.stream_io import GZipStream\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.lang import detect_fast\n",
    "from resiliparse.parse.html import HTMLTree\n",
    "from fastwarc.warc import is_http\n",
    "from surt import surt\n",
    "import tldextract\n",
    "import idna\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from resiliparse.parse.encoding import detect_encoding, bytes_to_str\n",
    "import boto3\n",
    "import polars as pl\n",
    "from fastwarc.warc import ArchiveIterator, WarcRecordType\n",
    "from fastwarc.stream_io import GZipStream\n",
    "import gcsfs # Ensure you have this installed for GCS filesystem handling\n",
    "import os\n",
    "\n",
    "client = boto3.client('s3', \n",
    "                      aws_access_key_id=os.getenv('ASCII_AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.getenv('ASCII_AWS_SECRET_ACCESS_KEY'))\n",
    "#client = boto3.client('s3', \n",
    "#        region_name='us-east-1',  # Common Crawl is in us-east-1\n",
    "#        config=Config(signature_version=UNSIGNED)\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawls = [\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251115220136-05256.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251115234907-05257.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116014712-05258.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116034034-05259.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116054722-05260.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116072531-05261.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116085944-05262.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116103419-05263.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116120640-05264.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116134805-05265.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116152516-05266.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116170848-05267.warc.gz',\n",
    "'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116190217-05268.warc.gz']\n",
    "key = 'crawl-data/CC-NEWS/index.html'\n",
    "response = client.get_object(Bucket='commoncrawl', Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "054d1077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html lang=\"en\">\\n<head>\\n  <title>Common Crawl News Dataset</title>\\n  <meta charset=\"utf-8\">\\n  <link rel=\"stylesheet\" href=\"https://data.commoncrawl.org/static/bucket.css\">\\n  <style>\\n    th,td { padding: 0.2em 0.5em; }\\n    th { background-color: #e0e0e0; }\\n    tr { background-color: #f0f0f0; }\\n    tr:nth-child(odd) { background-color: #fcfcfc; }\\n    footer { width: 100%; margin-top: 1em; }\\n  </style>\\n</head>\\n\\n<body>\\n\\n<h1>Common Crawl News Dataset</h1>\\n\\n<p>The news dataset includes articles from news sites all over the world. WARC files are released on a daily basis. The news crawl was started in 2016, please see the <a href=\"https://commoncrawl.org/2016/10/news-dataset-available/\">news dataset announcement</a> for further information.</p>\\n\\n<p>The <a href=\"https://github.com/commoncrawl/news-crawl/\">source code of the news crawler</a> is available on our <a href=\"https://github.com/commoncrawl/\">GitHub account</a>.\\n\\nPlease, <a href=\"https://github.com/commoncrawl/news-crawl/issues\">report issues</a> there and share your suggestions for improvements with us.</p>\\n\\n<h2>News Dataset WARC File Location</h2>\\n\\nThe WARC file names of the news data set follow the pattern:\\n<pre>\\ncrawl-data/CC-NEWS/yyyy/mm/CC-NEWS-yyyymmddHHMMSS-nnnnn.warc.gz\\n</pre>\\nwith\\n<dl>\\n  <dt><code>yyyy</code></dt>\\n  <dd>year</dd>\\n  <dt><code>mm</code></dt>\\n  <dd>month (01..12)</dd>\\n  <dt><code>dd</code></dt>\\n  <dd>day of month (01, etc.)</dd>\\n  <dt><code>HH</code></dt>\\n  <dd>hour (00..23)</dd>\\n  <dt><code>MM</code></dt>\\n  <dd>minute (00..59)</dd>\\n  <dt><code>SS</code></dt>\\n  <dd>second (00..59)</dd>\\n  <dt><code>nnnnn</code></dt>\\n  <dd>serial WARC file number. The serial number is reset when the\\n  crawl process is resumed.</dd>\\n</dl>\\n\\n<p>The timestamp (<code>yyyymmddHHMMSS</code>) indicates the time the\\nfirst record in the WARC file was created.</p>\\n\\nWe provide WARC file listings by month. The path listings\\nare found at\\n<pre>\\ns3://commoncrawl/crawl-data/CC-NEWS/yyyy/mm/warc.paths.gz\\n</pre>\\nresp.\\n<pre>\\nhttps://data.commoncrawl.org/crawl-data/CC-NEWS/yyyy/mm/warc.paths.gz\\n</pre>\\n\\nFor accessing the data please see our <a href=\"https://commoncrawl.org/the-data/get-started/\">Get Started</a> page.\\n\\n<h2>News Dataset Size By Year</h2>\\n\\n<p>For every year (linked) we provide an overview by month including\\nlinks to the WARC file listings.</p>\\n\\n<table>\\n  <thead>\\n    <tr>\\n      <th>Year</th>\\n      <th>Num. WARC files</th>\\n      <th>Total WARC Size<br/>Compressed (TiB)</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td><a href=\"./2025/index.html\">2025</a></td>\\n      <td align=\"right\">n/a</td>\\n      <td align=\"right\">n/a</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2024/index.html\">2024</a></td>\\n      <td align=\"right\">6224</td>\\n      <td align=\"right\">6.072</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2023/index.html\">2023</a></td>\\n      <td align=\"right\">8318</td>\\n      <td align=\"right\">8.102</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2022/index.html\">2022</a></td>\\n      <td align=\"right\">7956</td>\\n      <td align=\"right\">7.754</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2021/index.html\">2021</a></td>\\n      <td align=\"right\">6605</td>\\n      <td align=\"right\">6.435</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2020/index.html\">2020</a></td>\\n      <td align=\"right\">5395</td>\\n      <td align=\"right\">5.263</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2019/index.html\">2019</a></td>\\n      <td align=\"right\">3536</td>\\n      <td align=\"right\">3.449</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2018/index.html\">2018</a></td>\\n      <td align=\"right\">2613</td>\\n      <td align=\"right\">2.548</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2017/index.html\">2017</a></td>\\n      <td align=\"right\">1583</td>\\n      <td align=\"right\">1.504</td>\\n    </tr>\\n    <tr>\\n      <td><a href=\"./2016/index.html\">2016</a></td>\\n      <td align=\"right\">207</td>\\n      <td align=\"right\">0.151</td>\\n    </tr>\\n  </tbody>\\n</table>\\n\\n<footer>\\n<hr/>\\n<p>\\n<a href=\"https://commoncrawl.org/\">Common Crawl</a> is a California 501(c)(3) registered non-profit organization.\\nHosting of <a href=\"https://commoncrawl.org/the-data/\">Common Crawl data</a> is covered by <a href=\"https://aws.amazon.com/opendata/open-data-sponsorship-program/\">Amazon Web Services&apos; Open Data Sponsorship Program</a>.\\n</p>\\n<p>\\n<a href=\"https://commoncrawl.org/terms-of-use\" target=\"_blank\">Terms of Use</a>\\n<a href=\"https://commoncrawl.org/privacy-policy\" target=\"_blank\">Privacy</a>\\n</p>\\n</footer>\\n\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['Body'].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb680a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'crawl-data/CC-NEWS/2025/index.html'\n",
    "response = client.get_object(Bucket='commoncrawl', Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cba6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_gz = 'crawl-data/CC-NEWS/2025/11/warc.paths.gz'\n",
    "\n",
    "response = client.get_object(Bucket='commoncrawl', Key=key_gz)\n",
    "    \n",
    "gzipped_body_bytes = response['Body'].read()\n",
    "    \n",
    "decompressed_bytes = gzip.decompress(gzipped_body_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95269a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(527, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_1</th></tr><tr><td>str</td></tr></thead><tbody><tr><td>&quot;crawl-data/CC-NEWS/2025/11/CC-…</td></tr><tr><td>&quot;crawl-data/CC-NEWS/2025/11/CC-…</td></tr><tr><td>&quot;crawl-data/CC-NEWS/2025/11/CC-…</td></tr><tr><td>&quot;crawl-data/CC-NEWS/2025/11/CC-…</td></tr><tr><td>&quot;crawl-data/CC-NEWS/2025/11/CC-…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 1)\n",
       "┌─────────────────────────────────┐\n",
       "│ column_1                        │\n",
       "│ ---                             │\n",
       "│ str                             │\n",
       "╞═════════════════════════════════╡\n",
       "│ crawl-data/CC-NEWS/2025/11/CC-… │\n",
       "│ crawl-data/CC-NEWS/2025/11/CC-… │\n",
       "│ crawl-data/CC-NEWS/2025/11/CC-… │\n",
       "│ crawl-data/CC-NEWS/2025/11/CC-… │\n",
       "│ crawl-data/CC-NEWS/2025/11/CC-… │\n",
       "└─────────────────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data = pl.read_csv(BytesIO(decompressed_bytes), has_header=False)\n",
    "print(current_data.shape)\n",
    "current_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529d4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_pattern = re.compile(r\"^(?:www\\.)?\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\Z\")\n",
    "host_part_pattern = re.compile(\n",
    "    r\"^[a-z0-9]([a-z0-9_-]{0,61}[a-z0-9])?\\Z\", re.IGNORECASE | re.ASCII\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382e7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "key_path = \"../../../gcp_service_account.json\"  \n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246518f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surt_host(url):  # noqa: C901\n",
    "    extracted = tldextract.extract(url, include_psl_private_domains=True)\n",
    "    registered_domain = extracted.top_domain_under_public_suffix\n",
    "\n",
    "    if registered_domain == \"\":\n",
    "        registered_domain = f\"{extracted.subdomain}.{extracted.domain}\"\n",
    "        if registered_domain == \"\":\n",
    "            try:\n",
    "                # Fallback to urlparse if tldextract fails\n",
    "                host = urlparse(url).hostname\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse URL {url}: {e}\")\n",
    "                return None\n",
    "            if not host:\n",
    "                return None\n",
    "        else:\n",
    "            host = registered_domain\n",
    "    else:\n",
    "        host = registered_domain\n",
    "\n",
    "    host = host.strip().lower()\n",
    "    if len(host) < 1 or len(host) > 253:\n",
    "        return None\n",
    "    if ip_pattern.match(host):\n",
    "        return None\n",
    "    parts = host.split(\".\")\n",
    "    if parts[-1] == \"\":\n",
    "        # trailing dot is allowed, strip it\n",
    "        parts = parts[0:-1]\n",
    "    if len(parts) <= 1:\n",
    "        # do not accept single-word hosts, must be at least `domain.tld'\n",
    "        return None\n",
    "    if len(parts) > 2 and parts[0] == \"www\":\n",
    "        # strip leading 'www' to reduce number of \"duplicate\" hosts,\n",
    "        # but leave at least 2 trailing parts (www.com is a valid domain)\n",
    "        parts = parts[1:]\n",
    "    for i, part in enumerate(parts):\n",
    "        if len(part) > 63:\n",
    "            return None\n",
    "        if not host_part_pattern.match(part):\n",
    "            try:\n",
    "                idn = idna.encode(part).decode(\"ascii\")\n",
    "            except (\n",
    "                idna.IDNAError,\n",
    "                idna.core.InvalidCodepoint,\n",
    "                UnicodeError,\n",
    "                IndexError,\n",
    "                Exception,\n",
    "            ):\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "\n",
    "            # TODO: idna verifies the resulting string for length restrictions or invalid chars,\n",
    "            #       maybe no further verification is required:\n",
    "            if host_part_pattern.match(idn):\n",
    "                parts[i] = idn\n",
    "            else:\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "    parts.reverse()\n",
    "    return \".\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1baaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7d1ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_PATH = \"../../../gcp_service_account.json\"\n",
    "GCS_BUCKET_ROOT = \"gen-ai-tu/news/raw\"\n",
    "LOCAL_TEMP_DIR = \"./temp_staging\"\n",
    "\n",
    "# Initialize GCS FileSystem (Pure Python, more robust than C++ writer)\n",
    "fs = gcsfs.GCSFileSystem(token=KEY_PATH)\n",
    "\n",
    "# Clean start: ensure temp dir exists and is empty\n",
    "if os.path.exists(LOCAL_TEMP_DIR):\n",
    "    shutil.rmtree(LOCAL_TEMP_DIR)\n",
    "os.makedirs(LOCAL_TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing crawl-data/CC-NEWS/2025/11/CC-NEWS-20251101004549-04954.warc.gz...\n",
      "Failed to write parquet to GCS for crawl-data/CC-NEWS/2025/11/CC-NEWS-20251101004549-04954.warc.gz: google::cloud::Status(UNAVAILABLE: Retry policy exhausted, with a last message of Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: PerformWork() - CURL error [6]=Could not resolve hostname error_info={reason=, domain=, metadata={gcloud-cpp.retry.on-entry=false, gcloud-cpp.retry.function=CreateResumableUpload, gcloud-cpp.retry.reason=retry-policy-exhausted, gcloud-cpp.retry.original-message=Could not create a OAuth2 access token to authenticate the request. The request was not sent, as such an access token is required to complete the request successfully. Learn more about Google Cloud authentication at https://cloud.google.com/docs/authentication. The underlying error message was: PerformWork() - CURL error [6]=Could not resolve hostname}})\n",
      "Processing crawl-data/CC-NEWS/2025/11/CC-NEWS-20251101021758-04955.warc.gz...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m body_bytes = record.reader.read()\n\u001b[32m     15\u001b[39m html = bytes_to_str(body_bytes, detect_encoding(body_bytes))\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m text = \u001b[43mextract_plain_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m http_date =record.http_date \n\u001b[32m     18\u001b[39m http_last_modified = record.http_last_modified \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "output_path = 'gs://gen-ai-tu/news/raw'\n",
    "for key in current_data['column_1'].to_list():\n",
    "    print(f\"Processing {key}...\")\n",
    "    try:\n",
    "        # --- FIX 1: Initialize stream INSIDE the loop ---\n",
    "        s3_response = client.get_object(Bucket='commoncrawl', Key=key)\n",
    "        s3_stream = s3_response['Body']\n",
    "        stream = GZipStream(s3_stream)\n",
    "        \n",
    "        tmp = []\n",
    "        for record in ArchiveIterator(stream, record_types=WarcRecordType.response, func_filter=is_http):\n",
    "            try:\n",
    "                uri = record.headers.get('WARC-Target-URI')\n",
    "                body_bytes = record.reader.read()\n",
    "                html = bytes_to_str(body_bytes, detect_encoding(body_bytes))\n",
    "                text = extract_plain_text(html)\n",
    "                http_date =record.http_date \n",
    "                http_last_modified = record.http_last_modified \n",
    "                http_charset= record.http_charset \n",
    "                surt_uri = surt(uri)\n",
    "                host = get_surt_host(uri)\n",
    "                r = detect_fast(text, n_results=3)\n",
    "                langs = []\n",
    "                confs = []\n",
    "                for i in range(len(r)):\n",
    "                    langs.append(r[i][0])\n",
    "                    confs.append(r[i][1])\n",
    "                tmp.append({\n",
    "                    'uri': uri,\n",
    "                    'tree': html,\n",
    "                    'text': text,\n",
    "                    'main_lang': r[0][0], \n",
    "                    'langs': langs, \n",
    "                    'confs': confs,\n",
    "                    'http_date': http_date,\n",
    "                    'http_last_modified': http_last_modified,\n",
    "                    'http_charset': http_charset,\n",
    "                    'surt_uri': surt_uri,\n",
    "                    'host': host})\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record: {e}\")\n",
    "        if not tmp:\n",
    "            print(f\"No valid records found in {key}, skipping write.\")\n",
    "            continue\n",
    "        try:\n",
    "            path_parts = key.split('/')\n",
    "            filename = path_parts[-1]\n",
    "            year_str = path_parts[-3]  # '2025'\n",
    "            month_str = path_parts[-2] # '11'\n",
    "            timestamp = filename.split('-')[2] # '20251116190217'\n",
    "            day_str = timestamp[6:8]\n",
    "            day_str\n",
    "\n",
    "            df = pl.from_dicts(tmp).with_columns(\n",
    "                pl.lit(filename).alias('path'),\n",
    "                pl.lit(year_str).alias('year'),\n",
    "                pl.lit(month_str).alias('month'),\n",
    "                pl.lit(day_str).alias('day')\n",
    "            )\n",
    "local_write_path = f\"{LOCAL_TEMP_DIR}/{filename}\" # Use subfolder to avoid collisions\n",
    "        \n",
    "        df.write_parquet(\n",
    "            local_write_path,\n",
    "            partition_by=['year', 'month', 'day', 'path', 'main_lang'],\n",
    "            use_pyarrow=True \n",
    "        )\n",
    "\n",
    "        # 5. Upload to GCS using Python GCSFS\n",
    "        # recursive=True ensures the hive partition folders are uploaded correctly\n",
    "        # We copy from local_write_path to the bucket root\n",
    "        print(f\"Uploading {filename} to GCS...\")\n",
    "        \n",
    "        # fs.put will copy the *contents* of the local folder to the remote path\n",
    "        fs.put(local_write_path, f\"gs://{GCS_BUCKET_ROOT}\", recursive=True)\n",
    "\n",
    "        # 6. Cleanup Local Disk\n",
    "        shutil.rmtree(local_write_path)\n",
    "        print(f\"Done with {key}\")\n",
    "        except Exception as write_err:\n",
    "            print(f\"Failed to write parquet to GCS for {key}: {write_err}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing key {key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7436b190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = 'crawl-data/CC-NEWS/2025/11/CC-NEWS-20251101004549-04954.warc.gz'\n",
    "path_parts = key.split('/')\n",
    "filename = path_parts[-1]\n",
    "year_str = path_parts[-3]  # '2025'\n",
    "month_str = path_parts[-2] # '11'\n",
    "        \n",
    "        # Extract Day from filename timestamp\n",
    "timestamp = filename.split('-')[2] # '20251116190217'\n",
    "day_str = timestamp[6:8]\n",
    "day_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f7477de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a720127",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "pa.Table requires 'pyarrow' module to be installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_str\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mday\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpartition_by\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmonth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmain_lang\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                \u001b[49m\u001b[43muse_pyarrow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Often more stable for remote writes\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtoken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_path\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/polars/dataframe/frame.py:4123\u001b[39m, in \u001b[36mDataFrame.write_parquet\u001b[39m\u001b[34m(self, file, compression, compression_level, statistics, row_group_size, data_page_size, use_pyarrow, pyarrow_options, partition_by, partition_chunk_size_bytes, storage_options, credential_provider, retries, metadata, mkdir)\u001b[39m\n\u001b[32m   4120\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mwrite_parquet with `use_pyarrow=True` cannot be combined with `mkdir`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4121\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m4123\u001b[39m tbl = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto_arrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4124\u001b[39m data = {}\n\u001b[32m   4126\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, column \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tbl):\n\u001b[32m   4127\u001b[39m     \u001b[38;5;66;03m# extract the name before casting\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/polars/_utils/deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/polars/dataframe/frame.py:1744\u001b[39m, in \u001b[36mDataFrame.to_arrow\u001b[39m\u001b[34m(self, compat_level)\u001b[39m\n\u001b[32m   1741\u001b[39m     compat_level_py = compat_level._version\n\u001b[32m   1743\u001b[39m record_batches = \u001b[38;5;28mself\u001b[39m._df.to_arrow(compat_level_py)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTable\u001b[49m.from_batches(record_batches)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/polars/_dependencies.py:102\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    100\u001b[39m pfx = \u001b[38;5;28mself\u001b[39m._mod_pfx.get(\u001b[38;5;28mself\u001b[39m._module_name, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpfx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._module_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m module to be installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: pa.Table requires 'pyarrow' module to be installed"
     ]
    }
   ],
   "source": [
    "df.with_columns(pl.lit(day_str).alias('day')).write_parquet(\n",
    "                output_path, \n",
    "                partition_by=['year','month', 'path', 'main_lang'],\n",
    "                use_pyarrow=True, # Often more stable for remote writes\n",
    "                storage_options={\"token\": key_path}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75aa1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffd94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading CC-NEWS-20251101004549-04954.warc.gz to GCS...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'shutil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m fs.put(local_write_path, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgs://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGCS_BUCKET_ROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, recursive=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m         \u001b[38;5;66;03m# 6. Cleanup Local Disk\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mshutil\u001b[49m.rmtree(local_write_path)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'shutil' is not defined"
     ]
    }
   ],
   "source": [
    "local_write_path = f\"{LOCAL_TEMP_DIR}/{filename}\" # Use subfolder to avoid collisions\n",
    "df.write_parquet(\n",
    "            local_write_path,\n",
    "            partition_by=['year', 'month', 'day', 'path', 'main_lang'],\n",
    "            use_pyarrow=True \n",
    "        )\n",
    "\n",
    "        # 5. Upload to GCS using Python GCSFS\n",
    "        # recursive=True ensures the hive partition folders are uploaded correctly\n",
    "        # We copy from local_write_path to the bucket root\n",
    "print(f\"Uploading {filename} to GCS...\")\n",
    "        \n",
    "        # fs.put will copy the *contents* of the local folder to the remote path\n",
    "fs.put(local_write_path, f\"gs://{GCS_BUCKET_ROOT}\", recursive=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75fe37e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: './temp_staging/CC-NEWS-20251101004549-04954.warc.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotADirectoryError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m         \u001b[38;5;66;03m# 6. Cleanup Local Disk\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_write_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/shutil.py:759\u001b[39m, in \u001b[36mrmtree\u001b[39m\u001b[34m(path, ignore_errors, onerror, onexc, dir_fd)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    761\u001b[39m     \u001b[38;5;66;03m# Close any file descriptors still on the stack.\u001b[39;00m\n\u001b[32m    762\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m stack:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/shutil.py:703\u001b[39m, in \u001b[36m_rmtree_safe_fd\u001b[39m\u001b[34m(stack, onexc)\u001b[39m\n\u001b[32m    701\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    702\u001b[39m     err.filename = path\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m     \u001b[43monexc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/shutil.py:686\u001b[39m, in \u001b[36m_rmtree_safe_fd\u001b[39m\u001b[34m(stack, onexc)\u001b[39m\n\u001b[32m    683\u001b[39m     stack.append((os.close, topfd, path, orig_entry))\n\u001b[32m    685\u001b[39m func = os.scandir  \u001b[38;5;66;03m# For error reporting.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m686\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopfd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m scandir_it:\n\u001b[32m    687\u001b[39m     entries = \u001b[38;5;28mlist\u001b[39m(scandir_it)\n\u001b[32m    688\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m entries:\n",
      "\u001b[31mNotADirectoryError\u001b[39m: [Errno 20] Not a directory: './temp_staging/CC-NEWS-20251101004549-04954.warc.gz'"
     ]
    }
   ],
   "source": [
    "        # 6. Cleanup Local Disk\n",
    "shutil.rmtree(local_write_path)\n",
    "print(f\"Done with {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69120c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import re\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import idna\n",
    "from datetime import datetime\n",
    "\n",
    "# Spark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, LongType\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Processing Imports (Must be installed on all worker nodes)\n",
    "from fastwarc.warc import ArchiveIterator, WarcRecordType, is_http\n",
    "from fastwarc.stream_io import GZipStream\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.lang import detect_fast\n",
    "from resiliparse.parse.encoding import detect_encoding, bytes_to_str\n",
    "from surt import surt\n",
    "\n",
    "# --- Configuration ---\n",
    "AWS_ACCESS_KEY = os.getenv('ASCII_AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_KEY = os.getenv('ASCII_AWS_SECRET_ACCESS_KEY')\n",
    "GCP_SA_KEY_PATH = \"../../../gcp_service_account.json\"\n",
    "GCS_BUCKET_ROOT = \"gs://gen-ai-tu/news/raw\"\n",
    "\n",
    "# --- 1. Helper Functions (Must be available to workers) ---\n",
    "ip_pattern = re.compile(r\"^(?:www\\.)?\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\Z\")\n",
    "host_part_pattern = re.compile(r\"^[a-z0-9]([a-z0-9_-]{0,61}[a-z0-9])?\\Z\", re.IGNORECASE | re.ASCII)\n",
    "\n",
    "def get_surt_host(url):  # noqa: C901\n",
    "    extracted = tldextract.extract(url, include_psl_private_domains=True)\n",
    "    registered_domain = extracted.top_domain_under_public_suffix\n",
    "\n",
    "    if registered_domain == \"\":\n",
    "        registered_domain = f\"{extracted.subdomain}.{extracted.domain}\"\n",
    "        if registered_domain == \"\":\n",
    "            try:\n",
    "                # Fallback to urlparse if tldextract fails\n",
    "                host = urlparse(url).hostname\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse URL {url}: {e}\")\n",
    "                return None\n",
    "            if not host:\n",
    "                return None\n",
    "        else:\n",
    "            host = registered_domain\n",
    "    else:\n",
    "        host = registered_domain\n",
    "\n",
    "    host = host.strip().lower()\n",
    "    if len(host) < 1 or len(host) > 253:\n",
    "        return None\n",
    "    if ip_pattern.match(host):\n",
    "        return None\n",
    "    parts = host.split(\".\")\n",
    "    if parts[-1] == \"\":\n",
    "        # trailing dot is allowed, strip it\n",
    "        parts = parts[0:-1]\n",
    "    if len(parts) <= 1:\n",
    "        # do not accept single-word hosts, must be at least `domain.tld'\n",
    "        return None\n",
    "    if len(parts) > 2 and parts[0] == \"www\":\n",
    "        # strip leading 'www' to reduce number of \"duplicate\" hosts,\n",
    "        # but leave at least 2 trailing parts (www.com is a valid domain)\n",
    "        parts = parts[1:]\n",
    "    for i, part in enumerate(parts):\n",
    "        if len(part) > 63:\n",
    "            return None\n",
    "        if not host_part_pattern.match(part):\n",
    "            try:\n",
    "                idn = idna.encode(part).decode(\"ascii\")\n",
    "            except (\n",
    "                idna.IDNAError,\n",
    "                idna.core.InvalidCodepoint,\n",
    "                UnicodeError,\n",
    "                IndexError,\n",
    "                Exception,\n",
    "            ):\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "\n",
    "            # TODO: idna verifies the resulting string for length restrictions or invalid chars,\n",
    "            #       maybe no further verification is required:\n",
    "            if host_part_pattern.match(idn):\n",
    "                parts[i] = idn\n",
    "            else:\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "    parts.reverse()\n",
    "    return \".\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff0b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_warc_partition(iterator):\n",
    "    \"\"\"\n",
    "    This runs on the worker nodes. \n",
    "    It receives an iterator of rows (each row contains a 'warc_path').\n",
    "    It initializes its own boto3 client and processes files.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3', \n",
    "                             aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                             aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    \n",
    "    for row in iterator:\n",
    "        key = row.warc_path\n",
    "        \n",
    "        # Extract filename metadata for partitioning\n",
    "        try:\n",
    "            path_parts = key.split('/')\n",
    "            filename = path_parts[-1]\n",
    "            year_str = path_parts[-3] \n",
    "            month_str = path_parts[-2]\n",
    "            timestamp = filename.split('-')[2]\n",
    "            day_str = timestamp[6:8]\n",
    "        except Exception:\n",
    "            print(f\"Skipping malformed path: {key}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Stream directly from S3\n",
    "            response = s3_client.get_object(Bucket='commoncrawl', Key=key)\n",
    "            # Fastwarc handles the stream\n",
    "            stream = GZipStream(response['Body'])\n",
    "            \n",
    "            for record in ArchiveIterator(stream, record_types=WarcRecordType.response, func_filter=is_http):\n",
    "                try:\n",
    "                    uri = record.headers.get('WARC-Target-URI')\n",
    "                    body_bytes = record.reader.read()\n",
    "                    \n",
    "                    # Encoding and Text Extraction\n",
    "                    encoding = detect_encoding(body_bytes)\n",
    "                    html = bytes_to_str(body_bytes, encoding)\n",
    "                    text = extract_plain_text(html)\n",
    "                    \n",
    "                    # Metadata\n",
    "                    http_date = record.http_date\n",
    "                    http_last_modified = record.http_last_modified\n",
    "                    http_charset = record.http_charset\n",
    "                    surt_uri = surt(uri)\n",
    "                    host = get_surt_host(uri)\n",
    "                    \n",
    "                    # Language Detection\n",
    "                    r = detect_fast(text, n_results=3)\n",
    "                    main_lang = r[0][0] if r else 'unknown'\n",
    "                    langs = [x[0] for x in r]\n",
    "                    confs = [float(x[1]) for x in r]\n",
    "\n",
    "                    # Yield a dictionary representing the row\n",
    "                    yield {\n",
    "                        'uri': uri,\n",
    "                        'text': text, # Storing full text\n",
    "                        'html': html, # Optional: Storing full HTML\n",
    "                        'main_lang': main_lang,\n",
    "                        'langs': langs,\n",
    "                        'confs': confs,\n",
    "                        'http_date': http_date,\n",
    "                        'http_last_modified': http_last_modified,\n",
    "                        'http_charset': http_charset,\n",
    "                        'surt_uri': surt_uri,\n",
    "                        'host': host,\n",
    "                        # Partition columns\n",
    "                        'path': filename,\n",
    "                        'year': year_str,\n",
    "                        'month': month_str,\n",
    "                        'day': day_str\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    # Log internal record errors but don't stop the stream\n",
    "                    continue\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing WARC file {key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ce9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # Initialize Spark with GCS connector support\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"CommonCrawlProcessor\") \\\n",
    "        .config(\"spark.jars\", \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\") \\\n",
    "        .config(\"spark.jars.packages\", \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16\") \\\n",
    "        .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "        .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "        .config(\"google.cloud.auth.service.account.json.keyfile\", GCP_SA_KEY_PATH) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # A. Get the list of WARC paths (Driver side - lightweight)\n",
    "    s3_client = boto3.client('s3', \n",
    "                             aws_access_key_id=AWS_ACCESS_KEY,\n",
    "                             aws_secret_access_key=AWS_SECRET_KEY)\n",
    "    \n",
    "    paths_gz_key = 'crawl-data/CC-NEWS/2025/11/warc.paths.gz'\n",
    "    response = s3_client.get_object(Bucket='commoncrawl', Key=paths_gz_key)\n",
    "    decompressed_bytes = gzip.decompress(response['Body'].read())\n",
    "    \n",
    "    # Read paths into a list\n",
    "    warc_paths = [line.decode('utf-8').strip() for line in decompressed_bytes.splitlines()]\n",
    "    \n",
    "    # B. Create a simple DataFrame of paths to distribute work\n",
    "    # Repartition determines parallelism. e.g., if you have 1000 files and 100 partitions, \n",
    "    # each task processes ~10 files.\n",
    "    paths_df = spark.createDataFrame([(p,) for p in warc_paths], [\"warc_path\"]).repartition(100)\n",
    "\n",
    "    # C. Define Output Schema\n",
    "    schema = StructType([\n",
    "        StructField(\"uri\", StringType(), True),\n",
    "        StructField(\"text\", StringType(), True),\n",
    "        StructField(\"html\", StringType(), True),\n",
    "        StructField(\"main_lang\", StringType(), True),\n",
    "        StructField(\"langs\", ArrayType(StringType()), True),\n",
    "        StructField(\"confs\", ArrayType(FloatType()), True),\n",
    "        StructField(\"http_date\", StringType(), True),\n",
    "        StructField(\"http_last_modified\", StringType(), True),\n",
    "        StructField(\"http_charset\", StringType(), True),\n",
    "        StructField(\"surt_uri\", StringType(), True),\n",
    "        StructField(\"host\", StringType(), True),\n",
    "        # Partition cols\n",
    "        StructField(\"path\", StringType(), True),\n",
    "        StructField(\"year\", StringType(), True),\n",
    "        StructField(\"month\", StringType(), True),\n",
    "        StructField(\"day\", StringType(), True),\n",
    "    ])\n",
    "\n",
    "    # D. Execute Processing (Map Partitions)\n",
    "    # mapPartitions is more efficient than map because we init the S3 client once per partition\n",
    "    processed_rdd = paths_df.rdd.mapPartitions(process_warc_partition)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    final_df = spark.createDataFrame(processed_rdd, schema=schema)\n",
    "\n",
    "    # E. Write to GCS\n",
    "    print(f\"Writing to {GCS_BUCKET_ROOT}...\")\n",
    "    final_df.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .partitionBy(\"year\", \"month\", \"day\", \"path\", \"main_lang\") \\\n",
    "        .parquet(GCS_BUCKET_ROOT)\n",
    "\n",
    "    print(\"Job Complete.\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe859a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: google.cloud.auth.service.account.json.keyfile\n",
      "25/11/28 17:13:29 WARN Utils: Your hostname, utf resolves to a loopback address: 127.0.1.1; using 172.24.0.1 instead (on interface br-df65483b261a)\n",
      "25/11/28 17:13:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/picatto/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Provided Maven Coordinates must be in the form 'groupId:artifactId:version'. The coordinate provided is: gcs-connector-hadoop3-2.2.16-shaded.jar\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.$anonfun$extractMavenCoordinates$1(SparkSubmit.scala:1226)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.extractMavenCoordinates(SparkSubmit.scala:1224)\n",
      "\tat org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:1546)\n",
      "\tat org.apache.spark.util.DependencyUtils$.resolveMavenDependencies(DependencyUtils.scala:185)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:339)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:969)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)\n",
      "\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)\n",
      "\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkRuntimeError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m():\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# Initialize Spark with GCS connector support\u001b[39;00m\n\u001b[32m      4\u001b[39m     spark = \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCommonCrawlProcessor\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.jars.packages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcom.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16,gcs-connector-hadoop3-2.2.16-shaded.jar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.gs.impl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcom.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.fs.AbstractFileSystem.gs.impl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcom.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.hadoop.google.cloud.auth.service.account.enable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle.cloud.auth.service.account.json.keyfile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGCP_SA_KEY_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# A. Get the list of WARC paths (Driver side - lightweight)\u001b[39;00m\n\u001b[32m     15\u001b[39m     s3_client = boto3.client(\u001b[33m'\u001b[39m\u001b[33ms3\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     16\u001b[39m                              aws_access_key_id=AWS_ACCESS_KEY,\n\u001b[32m     17\u001b[39m                              aws_secret_access_key=AWS_SECRET_KEY)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m     sparkConf.set(key, value)\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m sc = \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m    500\u001b[39m session = SparkSession(sc, options=\u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/pyspark/context.py:515\u001b[39m, in \u001b[36mSparkContext.getOrCreate\u001b[39m\u001b[34m(cls, conf)\u001b[39m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    514\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext._active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext._active_spark_context\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/pyspark/context.py:201\u001b[39m, in \u001b[36mSparkContext.__init__\u001b[39m\u001b[34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway.gateway_parameters.auth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    197\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is not allowed as it is a security risk.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    203\u001b[39m     \u001b[38;5;28mself\u001b[39m._do_init(\n\u001b[32m    204\u001b[39m         master,\n\u001b[32m    205\u001b[39m         appName,\n\u001b[32m   (...)\u001b[39m\u001b[32m    215\u001b[39m         memory_profiler_cls,\n\u001b[32m    216\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/pyspark/context.py:436\u001b[39m, in \u001b[36mSparkContext._ensure_initialized\u001b[39m\u001b[34m(cls, instance, gateway, conf)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext._lock:\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext._gateway:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         SparkContext._gateway = gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m         SparkContext._jvm = SparkContext._gateway.jvm\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/basic-research/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[39m, in \u001b[36mlaunch_gateway\u001b[39m\u001b[34m(conf, popen_kwargs)\u001b[39m\n\u001b[32m    104\u001b[39m     time.sleep(\u001b[32m0.1\u001b[39m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(conn_info_file):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[32m    108\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mJAVA_GATEWAY_EXITED\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    109\u001b[39m         message_parameters={},\n\u001b[32m    110\u001b[39m     )\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[32m    113\u001b[39m     gateway_port = read_int(info)\n",
      "\u001b[31mPySparkRuntimeError\u001b[39m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
