{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import polars as pl\n",
    "import botocore\n",
    "import tarfile\n",
    "import os\n",
    "from io import BytesIO\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "import gzip\n",
    "from io import BytesIO\n",
    "from fastwarc import ArchiveIterator\n",
    "from fastwarc.stream_io import GZipStream\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.lang import detect_fast\n",
    "from resiliparse.parse.html import HTMLTree\n",
    "from fastwarc.warc import is_http\n",
    "from surt import surt\n",
    "import tldextract\n",
    "import idna\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from resiliparse.parse.encoding import detect_encoding, bytes_to_str\n",
    "import boto3\n",
    "import polars as pl\n",
    "from fastwarc.warc import ArchiveIterator, WarcRecordType\n",
    "from fastwarc.stream_io import GZipStream\n",
    "import gcsfs  # Ensure you have this installed for GCS filesystem handling\n",
    "import os\n",
    "\n",
    "client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=os.getenv(\"ASCII_AWS_ACCESS_KEY_ID\"),\n",
    "    aws_secret_access_key=os.getenv(\"ASCII_AWS_SECRET_ACCESS_KEY\"),\n",
    ")\n",
    "# client = boto3.client('s3',\n",
    "#        region_name='us-east-1',  # Common Crawl is in us-east-1\n",
    "#        config=Config(signature_version=UNSIGNED)\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15c303",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawls = [\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251115220136-05256.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251115234907-05257.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116014712-05258.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116034034-05259.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116054722-05260.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116072531-05261.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116085944-05262.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116103419-05263.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116120640-05264.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116134805-05265.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116152516-05266.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116170848-05267.warc.gz\",\n",
    "    \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251116190217-05268.warc.gz\",\n",
    "]\n",
    "key = \"crawl-data/CC-NEWS/index.html\"\n",
    "response = client.get_object(Bucket=\"commoncrawl\", Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"Body\"].read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb680a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"crawl-data/CC-NEWS/2025/index.html\"\n",
    "response = client.get_object(Bucket=\"commoncrawl\", Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cba6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_gz = \"crawl-data/CC-NEWS/2025/11/warc.paths.gz\"\n",
    "\n",
    "response = client.get_object(Bucket=\"commoncrawl\", Key=key_gz)\n",
    "\n",
    "gzipped_body_bytes = response[\"Body\"].read()\n",
    "\n",
    "decompressed_bytes = gzip.decompress(gzipped_body_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95269a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = pl.read_csv(BytesIO(decompressed_bytes), has_header=False)\n",
    "print(current_data.shape)\n",
    "current_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd8581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"crawl-data/CC-NEWS/index.html\"\n",
    "response = client.get_object(Bucket=\"commoncrawl\", Key=key)\n",
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f2c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "soup = BeautifulSoup(response[\"Body\"].read(), \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa720a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"crawl-data/CC-NEWS/index.html\"\n",
    "output_path = \"gs://gen-ai-tu/news/index/\"\n",
    "res0 = client.get_object(Bucket=\"commoncrawl\", Key=key)\n",
    "years = pd.read_html(res0[\"Body\"].read())\n",
    "index = []\n",
    "for i1, r1 in years[0].iterrows():\n",
    "    monthly_key = f\"crawl-data/CC-NEWS/{int(r1['Year'])}/index.html\"\n",
    "    res1 = client.get_object(Bucket=\"commoncrawl\", Key=monthly_key)\n",
    "    month = pd.read_html(res1[\"Body\"].read())\n",
    "    for i2, r2 in month[0].iterrows():\n",
    "        monty_key = f\"crawl-data/CC-NEWS/{int(r1['Year'])}/{r2['WARC file list']}\"\n",
    "        res2 = client.get_object(Bucket=\"commoncrawl\", Key=monty_key)\n",
    "        decompressed_bytes = gzip.decompress(res2[\"Body\"].read())\n",
    "        data = pl.read_csv(BytesIO(decompressed_bytes), has_header=False)\n",
    "        index.append(\n",
    "            data.with_columns(\n",
    "                pl.lit(int(r1[\"Year\"])).alias(\"year\"),\n",
    "                pl.lit(int(r2[\"Month\"])).alias(\"month\"),\n",
    "                pl.col(\"column_1\")\n",
    "                .str.split(\"-\")\n",
    "                .list.get(4)\n",
    "                .str.slice(6, 2)\n",
    "                .cast(pl.Int8)\n",
    "                .alias(\"day\"),\n",
    "            ).rename({\"column_1\": \"warc_destination\"})\n",
    "        )\n",
    "index_df = pl.concat(index)\n",
    "# index_df.write_parquet(\n",
    "#                output_path,\n",
    "#                partition_by=['year', 'month', 'day'],\n",
    "#                #use_pyarrow=True ,\n",
    "#                storage_options={\n",
    "#                    \"project_id\": os.getenv('GCP_PROJECT'),\n",
    "#                    \"private_key_id\": os.getenv('GCP_PRIVATE_KEY_ID'),\n",
    "#                    \"private_key\": os.getenv('GCP_PRIVATE_KEY'),\n",
    "#                    \"client_email\": os.getenv('GCP_EMAIL')},\n",
    "#                mkdir=True,\n",
    "#            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae27d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_key = f\"crawl-data/CC-NEWS/2026/index.html\"\n",
    "res1 = client.get_object(Bucket=\"commoncrawl\", Key=monthly_key)\n",
    "month = pd.read_html(res1[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ca7b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fc14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8bf393",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251101004549-04954.warc.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a1968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import re\n",
    "import polars as pl\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. Configure S3 for anonymous access in US-EAST-1\n",
    "# Specifying the region is crucial for public buckets to avoid redirect errors\n",
    "s3_client = boto3.client(\n",
    "    \"s3\", region_name=\"us-east-1\", config=Config(signature_version=UNSIGNED)\n",
    ")\n",
    "BUCKET = \"commoncrawl\"\n",
    "\n",
    "# CC-NEWS roughly started mid-2016\n",
    "start_year = 2016\n",
    "current_date = datetime.now()\n",
    "end_year = current_date.year\n",
    "\n",
    "# Regex to find WARC files inside the HTML\n",
    "# Looks for: href=\"CC-NEWS-20240101...\"\n",
    "warc_pattern = re.compile(r'href=\"(CC-NEWS-.*?\\.warc\\.gz)\"')\n",
    "\n",
    "all_records = []\n",
    "\n",
    "print(f\"Scanning CC-NEWS indices from {start_year} to {end_year}...\")\n",
    "\n",
    "for year in range(start_year, end_year + 1):\n",
    "    end_month = 12 if year < end_year else current_date.month\n",
    "\n",
    "    for month in range(1, end_month + 1):\n",
    "        month_str = f\"{month:02d}\"\n",
    "\n",
    "        # We access the index.html explicitly\n",
    "        # This file exists to allow browsers to view the files, and we can scrape it.\n",
    "        prefix = f\"crawl-data/CC-NEWS/{year}/{month_str}/\"\n",
    "        key_html = f\"{prefix}index.html\"\n",
    "\n",
    "        try:\n",
    "            print(f\"Scanning: {key_html}...\", end=\"\\r\")\n",
    "\n",
    "            # Download the HTML content\n",
    "            response = s3_client.get_object(Bucket=BUCKET, Key=key_html)\n",
    "            html_content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "            # Extract all WARC filenames from this month's index\n",
    "            warc_files = warc_pattern.findall(html_content)\n",
    "\n",
    "            # Create a record for each file found\n",
    "            for filename in warc_files:\n",
    "                # We extract the date from the filename itself for accuracy\n",
    "                # Filename: CC-NEWS-20251101004549-04954.warc.gz\n",
    "                # Slicing:  [8:12]=Year, [12:14]=Month, [14:16]=Day\n",
    "                file_year = filename[8:12]\n",
    "                file_month = filename[12:14]\n",
    "                file_day = filename[14:16]\n",
    "\n",
    "                full_address = f\"{prefix}{filename}\"\n",
    "\n",
    "                all_records.append(\n",
    "                    {\n",
    "                        \"year\": file_year,\n",
    "                        \"month\": file_month,\n",
    "                        \"day\": file_day,\n",
    "                        \"address\": full_address,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        except ClientError as e:\n",
    "            # 404/403 means the month is empty or didn't exist (common in 2016)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {key_html}: {e}\")\n",
    "\n",
    "print(\"\\n\\nProcessing complete.\")\n",
    "\n",
    "if all_records:\n",
    "    # Create Polars DataFrame\n",
    "    df = pl.DataFrame(all_records)\n",
    "\n",
    "    # Sort just to be neat\n",
    "    df = df.sort([\"year\", \"month\", \"day\", \"address\"])\n",
    "\n",
    "    print(f\"Total WARC files found: {len(df)}\")\n",
    "    print(df.head())\n",
    "\n",
    "    # df.write_csv(\"cc_news_full_list.csv\")\n",
    "else:\n",
    "    print(\"No WARC files found. Check your network connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_pattern = re.compile(r\"^(?:www\\.)?\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\Z\")\n",
    "host_part_pattern = re.compile(\n",
    "    r\"^[a-z0-9]([a-z0-9_-]{0,61}[a-z0-9])?\\Z\", re.IGNORECASE | re.ASCII\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382e7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "\n",
    "key_path = \"../../../gcp_service_account.json\"\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246518f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surt_host(url):  # noqa: C901\n",
    "    extracted = tldextract.extract(url, include_psl_private_domains=True)\n",
    "    registered_domain = extracted.top_domain_under_public_suffix\n",
    "\n",
    "    if registered_domain == \"\":\n",
    "        registered_domain = f\"{extracted.subdomain}.{extracted.domain}\"\n",
    "        if registered_domain == \"\":\n",
    "            try:\n",
    "                # Fallback to urlparse if tldextract fails\n",
    "                host = urlparse(url).hostname\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse URL {url}: {e}\")\n",
    "                return None\n",
    "            if not host:\n",
    "                return None\n",
    "        else:\n",
    "            host = registered_domain\n",
    "    else:\n",
    "        host = registered_domain\n",
    "\n",
    "    host = host.strip().lower()\n",
    "    if len(host) < 1 or len(host) > 253:\n",
    "        return None\n",
    "    if ip_pattern.match(host):\n",
    "        return None\n",
    "    parts = host.split(\".\")\n",
    "    if parts[-1] == \"\":\n",
    "        # trailing dot is allowed, strip it\n",
    "        parts = parts[0:-1]\n",
    "    if len(parts) <= 1:\n",
    "        # do not accept single-word hosts, must be at least `domain.tld'\n",
    "        return None\n",
    "    if len(parts) > 2 and parts[0] == \"www\":\n",
    "        # strip leading 'www' to reduce number of \"duplicate\" hosts,\n",
    "        # but leave at least 2 trailing parts (www.com is a valid domain)\n",
    "        parts = parts[1:]\n",
    "    for i, part in enumerate(parts):\n",
    "        if len(part) > 63:\n",
    "            return None\n",
    "        if not host_part_pattern.match(part):\n",
    "            try:\n",
    "                idn = idna.encode(part).decode(\"ascii\")\n",
    "            except (\n",
    "                idna.IDNAError,\n",
    "                idna.core.InvalidCodepoint,\n",
    "                UnicodeError,\n",
    "                IndexError,\n",
    "                Exception,\n",
    "            ):\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "\n",
    "            # TODO: idna verifies the resulting string for length restrictions or invalid chars,\n",
    "            #       maybe no further verification is required:\n",
    "            if host_part_pattern.match(idn):\n",
    "                parts[i] = idn\n",
    "            else:\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "    parts.reverse()\n",
    "    return \".\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1baaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_PATH = \"../../../gcp_service_account.json\"\n",
    "GCS_BUCKET_ROOT = \"gen-ai-tu/news/raw\"\n",
    "LOCAL_TEMP_DIR = \"./temp_staging\"\n",
    "\n",
    "# Initialize GCS FileSystem (Pure Python, more robust than C++ writer)\n",
    "fs = gcsfs.GCSFileSystem(token=KEY_PATH)\n",
    "\n",
    "# Clean start: ensure temp dir exists and is empty\n",
    "if os.path.exists(LOCAL_TEMP_DIR):\n",
    "    shutil.rmtree(LOCAL_TEMP_DIR)\n",
    "os.makedirs(LOCAL_TEMP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8e526",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'gs://gen-ai-tu/news/raw'\n",
    "for key in current_data['column_1'].to_list():\n",
    "    print(f\"Processing {key}...\")\n",
    "    try:\n",
    "        # --- FIX 1: Initialize stream INSIDE the loop ---\n",
    "        s3_response = client.get_object(Bucket='commoncrawl', Key=key)\n",
    "        s3_stream = s3_response['Body']\n",
    "        stream = GZipStream(s3_stream)\n",
    "        \n",
    "        tmp = []\n",
    "        for record in ArchiveIterator(stream, record_types=WarcRecordType.response, func_filter=is_http):\n",
    "            try:\n",
    "                uri = record.headers.get('WARC-Target-URI')\n",
    "                body_bytes = record.reader.read()\n",
    "                html = bytes_to_str(body_bytes, detect_encoding(body_bytes))\n",
    "                text = extract_plain_text(html)\n",
    "                http_date =record.http_date \n",
    "                http_last_modified = record.http_last_modified \n",
    "                http_charset= record.http_charset \n",
    "                surt_uri = surt(uri)\n",
    "                host = get_surt_host(uri)\n",
    "                r = detect_fast(text, n_results=3)\n",
    "                langs = []\n",
    "                confs = []\n",
    "                for i in range(len(r)):\n",
    "                    langs.append(r[i][0])\n",
    "                    confs.append(r[i][1])\n",
    "                tmp.append({\n",
    "                    'uri': uri,\n",
    "                    'tree': html,\n",
    "                    'text': text,\n",
    "                    'main_lang': r[0][0], \n",
    "                    'langs': langs, \n",
    "                    'confs': confs,\n",
    "                    'http_date': http_date,\n",
    "                    'http_last_modified': http_last_modified,\n",
    "                    'http_charset': http_charset,\n",
    "                    'surt_uri': surt_uri,\n",
    "                    'host': host})\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record: {e}\")\n",
    "        if not tmp:\n",
    "            print(f\"No valid records found in {key}, skipping write.\")\n",
    "            continue\n",
    "        try:\n",
    "            path_parts = key.split('/')\n",
    "            filename = path_parts[-1]\n",
    "            year_str = path_parts[-3]  # '2025'\n",
    "            month_str = path_parts[-2] # '11'\n",
    "            timestamp = filename.split('-')[2] # '20251116190217'\n",
    "            day_str = timestamp[6:8]\n",
    "            day_str\n",
    "\n",
    "            df = pl.from_dicts(tmp).with_columns(\n",
    "                pl.lit(filename).alias('path'),\n",
    "                pl.lit(year_str).alias('year'),\n",
    "                pl.lit(month_str).alias('month'),\n",
    "                pl.lit(day_str).alias('day')\n",
    "            )\n",
    "            local_write_path = f\"{LOCAL_TEMP_DIR}/{filename}\" # Use subfolder to avoid collisions\n",
    "                    \n",
    "            df.write_parquet(\n",
    "                local_write_path,\n",
    "                partition_by=['year', 'month', 'day', 'path', 'main_lang'],\n",
    "                use_pyarrow=True \n",
    "            )\n",
    "\n",
    "                    # 5. Upload to GCS using Python GCSFS\n",
    "                    # recursive=True ensures the hive partition folders are uploaded correctly\n",
    "                    # We copy from local_write_path to the bucket root\n",
    "            print(f\"Uploading {filename} to GCS...\")\n",
    "                    \n",
    "                    # fs.put will copy the *contents* of the local folder to the remote path\n",
    "            fs.put(local_write_path, f\"gs://{GCS_BUCKET_ROOT}\", recursive=True)\n",
    "\n",
    "                    # 6. Cleanup Local Disk\n",
    "            shutil.rmtree(local_write_path)\n",
    "            print(f\"Done with {key}\")\n",
    "        except Exception as write_err:\n",
    "            print(f\"Failed to write parquet to GCS for {key}: {write_err}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing key {key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7436b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"crawl-data/CC-NEWS/2025/11/CC-NEWS-20251101004549-04954.warc.gz\"\n",
    "path_parts = key.split(\"/\")\n",
    "filename = path_parts[-1]\n",
    "year_str = path_parts[-3]  # '2025'\n",
    "month_str = path_parts[-2]  # '11'\n",
    "\n",
    "# Extract Day from filename timestamp\n",
    "timestamp = filename.split(\"-\")[2]  # '20251116190217'\n",
    "day_str = timestamp[6:8]\n",
    "day_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7477de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a720127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.with_columns(pl.lit(day_str).alias(\"day\")).write_parquet(\n",
    "    output_path,\n",
    "    partition_by=[\"year\", \"month\", \"path\", \"main_lang\"],\n",
    "    use_pyarrow=True,  # Often more stable for remote writes\n",
    "    storage_options={\"token\": key_path},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_write_path = f\"{LOCAL_TEMP_DIR}/{filename}\"  # Use subfolder to avoid collisions\n",
    "df.write_parquet(\n",
    "    local_write_path,\n",
    "    partition_by=[\"year\", \"month\", \"day\", \"path\", \"main_lang\"],\n",
    "    use_pyarrow=True,\n",
    ")\n",
    "\n",
    "# 5. Upload to GCS using Python GCSFS\n",
    "# recursive=True ensures the hive partition folders are uploaded correctly\n",
    "# We copy from local_write_path to the bucket root\n",
    "print(f\"Uploading {filename} to GCS...\")\n",
    "\n",
    "# fs.put will copy the *contents* of the local folder to the remote path\n",
    "fs.put(local_write_path, f\"gs://{GCS_BUCKET_ROOT}\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe37e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cleanup Local Disk\n",
    "shutil.rmtree(local_write_path)\n",
    "print(f\"Done with {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69120c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import re\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import idna\n",
    "from datetime import datetime\n",
    "\n",
    "# Spark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    ArrayType,\n",
    "    FloatType,\n",
    "    LongType,\n",
    ")\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Processing Imports (Must be installed on all worker nodes)\n",
    "from fastwarc.warc import ArchiveIterator, WarcRecordType, is_http\n",
    "from fastwarc.stream_io import GZipStream\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from resiliparse.parse.lang import detect_fast\n",
    "from resiliparse.parse.encoding import detect_encoding, bytes_to_str\n",
    "from surt import surt\n",
    "\n",
    "# --- Configuration ---\n",
    "AWS_ACCESS_KEY = os.getenv(\"ASCII_AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_KEY = os.getenv(\"ASCII_AWS_SECRET_ACCESS_KEY\")\n",
    "GCP_SA_KEY_PATH = \"../../../gcp_service_account.json\"\n",
    "GCS_BUCKET_ROOT = \"gs://gen-ai-tu/news/raw\"\n",
    "\n",
    "# --- 1. Helper Functions (Must be available to workers) ---\n",
    "ip_pattern = re.compile(r\"^(?:www\\.)?\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\Z\")\n",
    "host_part_pattern = re.compile(\n",
    "    r\"^[a-z0-9]([a-z0-9_-]{0,61}[a-z0-9])?\\Z\", re.IGNORECASE | re.ASCII\n",
    ")\n",
    "\n",
    "\n",
    "def get_surt_host(url):  # noqa: C901\n",
    "    extracted = tldextract.extract(url, include_psl_private_domains=True)\n",
    "    registered_domain = extracted.top_domain_under_public_suffix\n",
    "\n",
    "    if registered_domain == \"\":\n",
    "        registered_domain = f\"{extracted.subdomain}.{extracted.domain}\"\n",
    "        if registered_domain == \"\":\n",
    "            try:\n",
    "                # Fallback to urlparse if tldextract fails\n",
    "                host = urlparse(url).hostname\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse URL {url}: {e}\")\n",
    "                return None\n",
    "            if not host:\n",
    "                return None\n",
    "        else:\n",
    "            host = registered_domain\n",
    "    else:\n",
    "        host = registered_domain\n",
    "\n",
    "    host = host.strip().lower()\n",
    "    if len(host) < 1 or len(host) > 253:\n",
    "        return None\n",
    "    if ip_pattern.match(host):\n",
    "        return None\n",
    "    parts = host.split(\".\")\n",
    "    if parts[-1] == \"\":\n",
    "        # trailing dot is allowed, strip it\n",
    "        parts = parts[0:-1]\n",
    "    if len(parts) <= 1:\n",
    "        # do not accept single-word hosts, must be at least `domain.tld'\n",
    "        return None\n",
    "    if len(parts) > 2 and parts[0] == \"www\":\n",
    "        # strip leading 'www' to reduce number of \"duplicate\" hosts,\n",
    "        # but leave at least 2 trailing parts (www.com is a valid domain)\n",
    "        parts = parts[1:]\n",
    "    for i, part in enumerate(parts):\n",
    "        if len(part) > 63:\n",
    "            return None\n",
    "        if not host_part_pattern.match(part):\n",
    "            try:\n",
    "                idn = idna.encode(part).decode(\"ascii\")\n",
    "            except (\n",
    "                idna.IDNAError,\n",
    "                idna.core.InvalidCodepoint,\n",
    "                UnicodeError,\n",
    "                IndexError,\n",
    "                Exception,\n",
    "            ):\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "\n",
    "            # TODO: idna verifies the resulting string for length restrictions or invalid chars,\n",
    "            #       maybe no further verification is required:\n",
    "            if host_part_pattern.match(idn):\n",
    "                parts[i] = idn\n",
    "            else:\n",
    "                print(\"Invalid host name: {}\".format(url))\n",
    "                return None\n",
    "    parts.reverse()\n",
    "    return \".\".join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff0b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_warc_partition(iterator):\n",
    "    \"\"\"\n",
    "    This runs on the worker nodes.\n",
    "    It receives an iterator of rows (each row contains a 'warc_path').\n",
    "    It initializes its own boto3 client and processes files.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY\n",
    "    )\n",
    "\n",
    "    for row in iterator:\n",
    "        key = row.warc_path\n",
    "\n",
    "        # Extract filename metadata for partitioning\n",
    "        try:\n",
    "            path_parts = key.split(\"/\")\n",
    "            filename = path_parts[-1]\n",
    "            year_str = path_parts[-3]\n",
    "            month_str = path_parts[-2]\n",
    "            timestamp = filename.split(\"-\")[2]\n",
    "            day_str = timestamp[6:8]\n",
    "        except Exception:\n",
    "            print(f\"Skipping malformed path: {key}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Stream directly from S3\n",
    "            response = s3_client.get_object(Bucket=\"commoncrawl\", Key=key)\n",
    "            # Fastwarc handles the stream\n",
    "            stream = GZipStream(response[\"Body\"])\n",
    "\n",
    "            for record in ArchiveIterator(\n",
    "                stream, record_types=WarcRecordType.response, func_filter=is_http\n",
    "            ):\n",
    "                try:\n",
    "                    uri = record.headers.get(\"WARC-Target-URI\")\n",
    "                    body_bytes = record.reader.read()\n",
    "\n",
    "                    # Encoding and Text Extraction\n",
    "                    encoding = detect_encoding(body_bytes)\n",
    "                    html = bytes_to_str(body_bytes, encoding)\n",
    "                    text = extract_plain_text(html)\n",
    "\n",
    "                    # Metadata\n",
    "                    http_date = record.http_date\n",
    "                    http_last_modified = record.http_last_modified\n",
    "                    http_charset = record.http_charset\n",
    "                    surt_uri = surt(uri)\n",
    "                    host = get_surt_host(uri)\n",
    "\n",
    "                    # Language Detection\n",
    "                    r = detect_fast(text, n_results=3)\n",
    "                    main_lang = r[0][0] if r else \"unknown\"\n",
    "                    langs = [x[0] for x in r]\n",
    "                    confs = [float(x[1]) for x in r]\n",
    "\n",
    "                    # Yield a dictionary representing the row\n",
    "                    yield {\n",
    "                        \"uri\": uri,\n",
    "                        \"text\": text,  # Storing full text\n",
    "                        \"html\": html,  # Optional: Storing full HTML\n",
    "                        \"main_lang\": main_lang,\n",
    "                        \"langs\": langs,\n",
    "                        \"confs\": confs,\n",
    "                        \"http_date\": http_date,\n",
    "                        \"http_last_modified\": http_last_modified,\n",
    "                        \"http_charset\": http_charset,\n",
    "                        \"surt_uri\": surt_uri,\n",
    "                        \"host\": host,\n",
    "                        # Partition columns\n",
    "                        \"path\": filename,\n",
    "                        \"year\": year_str,\n",
    "                        \"month\": month_str,\n",
    "                        \"day\": day_str,\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    # Log internal record errors but don't stop the stream\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing WARC file {key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40ce9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize Spark with GCS connector support\n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"CommonCrawlProcessor\")\n",
    "        .config(\n",
    "            \"spark.jars\",\n",
    "            \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.16\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.hadoop.fs.gs.impl\",\n",
    "            \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\",\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.hadoop.fs.AbstractFileSystem.gs.impl\",\n",
    "            \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\",\n",
    "        )\n",
    "        .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\")\n",
    "        .config(\"google.cloud.auth.service.account.json.keyfile\", GCP_SA_KEY_PATH)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # A. Get the list of WARC paths (Driver side - lightweight)\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\", aws_access_key_id=AWS_ACCESS_KEY, aws_secret_access_key=AWS_SECRET_KEY\n",
    "    )\n",
    "\n",
    "    paths_gz_key = \"crawl-data/CC-NEWS/2025/11/warc.paths.gz\"\n",
    "    response = s3_client.get_object(Bucket=\"commoncrawl\", Key=paths_gz_key)\n",
    "    decompressed_bytes = gzip.decompress(response[\"Body\"].read())\n",
    "\n",
    "    # Read paths into a list\n",
    "    warc_paths = [\n",
    "        line.decode(\"utf-8\").strip() for line in decompressed_bytes.splitlines()\n",
    "    ]\n",
    "\n",
    "    # B. Create a simple DataFrame of paths to distribute work\n",
    "    # Repartition determines parallelism. e.g., if you have 1000 files and 100 partitions,\n",
    "    # each task processes ~10 files.\n",
    "    paths_df = spark.createDataFrame(\n",
    "        [(p,) for p in warc_paths], [\"warc_path\"]\n",
    "    ).repartition(100)\n",
    "\n",
    "    # C. Define Output Schema\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"uri\", StringType(), True),\n",
    "            StructField(\"text\", StringType(), True),\n",
    "            StructField(\"html\", StringType(), True),\n",
    "            StructField(\"main_lang\", StringType(), True),\n",
    "            StructField(\"langs\", ArrayType(StringType()), True),\n",
    "            StructField(\"confs\", ArrayType(FloatType()), True),\n",
    "            StructField(\"http_date\", StringType(), True),\n",
    "            StructField(\"http_last_modified\", StringType(), True),\n",
    "            StructField(\"http_charset\", StringType(), True),\n",
    "            StructField(\"surt_uri\", StringType(), True),\n",
    "            StructField(\"host\", StringType(), True),\n",
    "            # Partition cols\n",
    "            StructField(\"path\", StringType(), True),\n",
    "            StructField(\"year\", StringType(), True),\n",
    "            StructField(\"month\", StringType(), True),\n",
    "            StructField(\"day\", StringType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # D. Execute Processing (Map Partitions)\n",
    "    # mapPartitions is more efficient than map because we init the S3 client once per partition\n",
    "    processed_rdd = paths_df.rdd.mapPartitions(process_warc_partition)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    final_df = spark.createDataFrame(processed_rdd, schema=schema)\n",
    "\n",
    "    # E. Write to GCS\n",
    "    print(f\"Writing to {GCS_BUCKET_ROOT}...\")\n",
    "    final_df.write.mode(\"append\").partitionBy(\n",
    "        \"year\", \"month\", \"day\", \"path\", \"main_lang\"\n",
    "    ).parquet(GCS_BUCKET_ROOT)\n",
    "\n",
    "    print(\"Job Complete.\")\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe859a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5280f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed98a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # The path to your file to upload\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # The ID of your GCS object\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Optional: set a generation-match precondition to avoid potential race conditions\n",
    "    # and data corruptions. The request to upload is aborted if the object's\n",
    "    # generation number does not match your precondition. For a destination\n",
    "    # object that does not yet exist, set the if_generation_match precondition to 0.\n",
    "    # If the destination object already exists in your bucket, set instead a\n",
    "    # generation-match precondition using its generation number.\n",
    "    generation_match_precondition = 0\n",
    "\n",
    "    blob.upload_from_filename(\n",
    "        source_file_name, if_generation_match=generation_match_precondition\n",
    "    )\n",
    "\n",
    "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c657a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(\"../../../src/gen_ai_pipeline/gen_ai_pipeline/resources/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011e767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_blob(\n",
    "    \"gen-ai-tu\",\n",
    "    \"../../../src/gen_ai_pipeline/gen_ai_pipeline/resources/environment.sh\",\n",
    "    \"artifacts/environment.sh\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_blob(\n",
    "    \"gen-ai-tu\",\n",
    "    \"../../../src/gen_ai_pipeline/gen_ai_pipeline/resources/install_pixi_env.sh\",\n",
    "    \"artifacts/install_pixi_env.sh\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8fa807",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_blob(\n",
    "    \"gen-ai-tu\",\n",
    "    \"../../../src/gen_ai_pipeline/gen_ai_pipeline/external/ccnews_extract_job.py\",\n",
    "    \"artifacts/ccnews_extract_job.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ea032",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_blob(\n",
    "    \"gen-ai-tu\",\n",
    "    \"../../../src/gen_ai_pipeline/gen_ai_pipeline/external/ccnews_preprocess_job.py\",\n",
    "    \"artifacts/ccnews_preprocess_job.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8213054",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_blob(\n",
    "    \"gen-ai-tu\",\n",
    "    \"../../../src/gen_ai_pipeline/gen_ai_pipeline/resources/install_pixi_preprocess.sh\",\n",
    "    \"artifacts/install_pixi_preprocess.sh\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
