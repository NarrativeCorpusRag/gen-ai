{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d709603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import hashlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "\n",
    "INPUT_PATH = \"/data/raid5/data/picatto/ascii/news/clean/\"\n",
    "OUTPUT_PATH = \"/data/raid5/data/picatto/ascii/news/chunks_coref2/\"\n",
    "CHECKPOINT_FILE = Path(OUTPUT_PATH) / \".checkpoint.json\"\n",
    "BATCH_SIZE = 1500  # Tune based on avg doc length\n",
    "DEVICE = \"cuda:0\"\n",
    "NUM_CPU_WORKERS = 48\n",
    "\n",
    "# Compile regex once at module level\n",
    "_SENTENCE_PATTERN = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15169c",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CPU-BOUND FUNCTIONS\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c50a940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_coref_resolution(text: str, clusters: List) -> str:\n",
    "    \"\"\"Apply coreference resolution - CPU bound\"\"\"\n",
    "    if not clusters or not text:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        replacements = []\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) < 2:\n",
    "                continue\n",
    "            \n",
    "            spans_text = [text[s:e] for s, e in cluster]\n",
    "            canonical = max(spans_text, key=len)\n",
    "            \n",
    "            for (start, end), mention in zip(cluster, spans_text):\n",
    "                if mention == canonical:\n",
    "                    continue\n",
    "                if len(mention.split()) > 2:\n",
    "                    continue\n",
    "                replacements.append((start, end, canonical))\n",
    "        \n",
    "        replacements.sort(key=lambda x: x[0], reverse=True)\n",
    "        for start, end, replacement in replacements:\n",
    "            text = text[:start] + replacement + text[end:]\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 512, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"Split text into chunks - CPU bound\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    sentences = _SENTENCE_PATTERN.split(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_words = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_words = len(sentence.split())\n",
    "        \n",
    "        if current_words + sentence_words <= chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_words += sentence_words\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    'chunk_text': ' '.join(current_chunk),\n",
    "                    'chunk_word_count': current_words,\n",
    "                })\n",
    "            \n",
    "            if overlap > 0 and current_chunk:\n",
    "                overlap_text = ' '.join(current_chunk)\n",
    "                overlap_words = overlap_text.split()[-overlap:]\n",
    "                current_chunk = [' '.join(overlap_words), sentence]\n",
    "                current_words = len(overlap_words) + sentence_words\n",
    "            else:\n",
    "                current_chunk = [sentence]\n",
    "                current_words = sentence_words\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            'chunk_text': ' '.join(current_chunk),\n",
    "            'chunk_word_count': current_words,\n",
    "        })\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_index'] = i\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_single_doc(args: Tuple) -> List[Dict]:\n",
    "    \"\"\"Process one document - designed for parallel execution\"\"\"\n",
    "    doc_id, text, clusters, metadata = args\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    resolved_text = apply_coref_resolution(text, clusters)\n",
    "    chunks = chunk_text(resolved_text)\n",
    "    \n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        chunk_id = hashlib.sha256(f\"{doc_id}_{chunk['chunk_index']}\".encode()).hexdigest()[:16]\n",
    "        \n",
    "        row = {\n",
    "            'chunk_id': chunk_id,\n",
    "            'doc_id': doc_id,\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'chunk_text': chunk['chunk_text'],\n",
    "            'chunk_word_count': chunk['chunk_word_count'],\n",
    "            **metadata\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd9dd4",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CHECKPOINTING\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85b7d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint() -> set:\n",
    "    \"\"\"Load set of completed file indices\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with open(CHECKPOINT_FILE) as f:\n",
    "            data = json.load(f)\n",
    "            return set(data.get('completed_files', []))\n",
    "    return set()\n",
    "\n",
    "\n",
    "def save_checkpoint(completed_files: set, total_docs: int, total_chunks: int):\n",
    "    \"\"\"Save checkpoint\"\"\"\n",
    "    CHECKPOINT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({\n",
    "            'completed_files': list(completed_files),\n",
    "            'total_docs': total_docs,\n",
    "            'total_chunks': total_chunks,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be43d58",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# MAIN PROCESSING - Using ThreadPoolExecutor (safer with CUDA)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7904881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_threaded(\n",
    "    batch_df: pl.DataFrame, \n",
    "    coref_model,\n",
    "    thread_pool: ThreadPoolExecutor\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Process batch: GPU inference in main thread, CPU work in thread pool.\n",
    "    ThreadPoolExecutor is safe with CUDA (unlike ProcessPoolExecutor).\n",
    "    \"\"\"\n",
    "    \n",
    "    texts = batch_df['extracted_text'].to_list()\n",
    "    doc_ids = batch_df['doc_id'].to_list()\n",
    "    \n",
    "    meta_cols = ['uri', 'host', 'http_date', 'year', 'month', 'day']\n",
    "    meta_data = {col: batch_df[col].to_list() for col in meta_cols if col in batch_df.columns}\n",
    "    \n",
    "    # --- GPU BATCH INFERENCE (main thread) ---\n",
    "    valid_indices = [i for i, t in enumerate(texts) if t and len(t.strip()) > 10]\n",
    "    valid_texts = [texts[i] for i in valid_indices]\n",
    "    \n",
    "    clusters_map = {}\n",
    "    if valid_texts:\n",
    "        try:\n",
    "            preds = coref_model.predict(texts=valid_texts, is_split_into_words=False)\n",
    "            for idx, pred in zip(valid_indices, preds):\n",
    "                try:\n",
    "                    clusters_map[idx] = pred.get_clusters(as_strings=False)\n",
    "                except:\n",
    "                    clusters_map[idx] = []\n",
    "        except Exception as e:\n",
    "            print(f\"GPU inference error: {e}\")\n",
    "    \n",
    "    # --- PARALLEL CPU PROCESSING (thread pool) ---\n",
    "    process_args = []\n",
    "    for i, (doc_id, text) in enumerate(zip(doc_ids, texts)):\n",
    "        metadata = {col: meta_data[col][i] for col in meta_cols if col in meta_data}\n",
    "        clusters = clusters_map.get(i, [])\n",
    "        process_args.append((doc_id, text, clusters, metadata))\n",
    "    \n",
    "    # Thread pool for CPU-bound work (releases GIL during string ops)\n",
    "    all_rows = []\n",
    "    futures = [thread_pool.submit(process_single_doc, args) for args in process_args]\n",
    "    for future in futures:\n",
    "        rows = future.result()\n",
    "        all_rows.extend(rows)\n",
    "    \n",
    "    if not all_rows:\n",
    "        return pl.DataFrame()\n",
    "    \n",
    "    return pl.DataFrame(all_rows)\n",
    "\n",
    "\n",
    "def process_streaming_production(\n",
    "    input_path: str, \n",
    "    output_path: str, \n",
    "    batch_size: int = 1500\n",
    "):\n",
    "    \"\"\"\n",
    "    Production-ready streaming processor for 2.5M+ documents.\n",
    "    Features: checkpointing, memory-safe, progress tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading FastCoref model...\")\n",
    "    from fastcoref import FCoref\n",
    "    coref_model = FCoref(device=DEVICE)\n",
    "    print(\"Model loaded!\")\n",
    "    \n",
    "    # Get all input files\n",
    "    input_files = sorted(Path(input_path).rglob(\"*.parquet\"))\n",
    "    print(f\"Found {len(input_files)} parquet files\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    completed_files = load_checkpoint()\n",
    "    if completed_files:\n",
    "        print(f\"Resuming from checkpoint: {len(completed_files)} files already done\")\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_docs = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Thread pool for CPU work (threads are safe with CUDA)\n",
    "    with ThreadPoolExecutor(max_workers=NUM_CPU_WORKERS) as thread_pool:\n",
    "        \n",
    "        pbar = tqdm(enumerate(input_files), total=len(input_files), desc=\"Files\")\n",
    "        \n",
    "        for file_idx, parquet_file in pbar:\n",
    "            # Skip completed files\n",
    "            if file_idx in completed_files:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Lazy scan\n",
    "                lf = pl.scan_parquet(parquet_file, hive_partitioning=True)\n",
    "                file_rows = lf.select(pl.len()).collect().item()\n",
    "                \n",
    "                lf = lf.select([\n",
    "                    'doc_id', 'extracted_text', 'uri', 'host', 'http_date',\n",
    "                    'year', 'month', 'day'\n",
    "                ])\n",
    "                \n",
    "                file_chunks = []\n",
    "                file_docs = 0\n",
    "                \n",
    "                # Process in batches\n",
    "                for batch_start in range(0, file_rows, batch_size):\n",
    "                    batch_df = lf.slice(batch_start, batch_size).collect()\n",
    "                    \n",
    "                    if batch_df.is_empty():\n",
    "                        continue\n",
    "                    \n",
    "                    chunks_df = process_batch_threaded(batch_df, coref_model, thread_pool)\n",
    "                    \n",
    "                    if not chunks_df.is_empty():\n",
    "                        file_chunks.append(chunks_df)\n",
    "                    \n",
    "                    file_docs += len(batch_df)\n",
    "                    del batch_df\n",
    "                \n",
    "                # Write this file's output\n",
    "                if file_chunks:\n",
    "                    result_df = pl.concat(file_chunks)\n",
    "                    \n",
    "                    if 'year' in result_df.columns and 'month' in result_df.columns:\n",
    "                        year = result_df['year'][0]\n",
    "                        month = result_df['month'][0]\n",
    "                        out_file = Path(output_path) / f\"year={year}\" / f\"month={month}\" / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    else:\n",
    "                        out_file = Path(output_path) / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    \n",
    "                    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    result_df.write_parquet(out_file, compression='zstd', compression_level=3)\n",
    "                    \n",
    "                    total_chunks += len(result_df)\n",
    "                    del file_chunks, result_df\n",
    "                \n",
    "                total_docs += file_docs\n",
    "                \n",
    "                # Update checkpoint\n",
    "                completed_files.add(file_idx)\n",
    "                if file_idx % 5 == 0:  # Save checkpoint every 5 files\n",
    "                    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Update progress bar\n",
    "                elapsed = time.time() - start_time\n",
    "                docs_per_sec = total_docs / elapsed if elapsed > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'docs': f'{total_docs:,}',\n",
    "                    'chunks': f'{total_chunks:,}',\n",
    "                    'docs/s': f'{docs_per_sec:.1f}'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {parquet_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Final checkpoint\n",
    "    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE!\")\n",
    "    print(f\"Documents: {total_docs:,}\")\n",
    "    print(f\"Chunks: {total_chunks:,}\")\n",
    "    print(f\"Time: {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Speed: {total_docs/elapsed:.1f} docs/sec\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4a55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MULTIPROCESSING VERSION (for multi-GPU setups)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2838ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_process(\n",
    "    file_queue: mp.Queue,\n",
    "    result_queue: mp.Queue,\n",
    "    output_path: str,\n",
    "    batch_size: int,\n",
    "    device: str\n",
    "):\n",
    "    \"\"\"Worker process for multi-GPU processing\"\"\"\n",
    "    \n",
    "    # Load model in this process (fresh CUDA context)\n",
    "    from fastcoref import FCoref\n",
    "    coref_model = FCoref(device=device)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=12) as thread_pool:\n",
    "        while True:\n",
    "            item = file_queue.get()\n",
    "            if item is None:  # Poison pill\n",
    "                break\n",
    "            \n",
    "            file_idx, parquet_file = item\n",
    "            \n",
    "            try:\n",
    "                lf = pl.scan_parquet(parquet_file, hive_partitioning=True)\n",
    "                file_rows = lf.select(pl.count()).collect().item()\n",
    "                \n",
    "                lf = lf.select([\n",
    "                    'doc_id', 'extracted_text', 'uri', 'host', 'http_date',\n",
    "                    'year', 'month', 'day'\n",
    "                ])\n",
    "                \n",
    "                file_chunks = []\n",
    "                file_docs = 0\n",
    "                \n",
    "                for batch_start in range(0, file_rows, batch_size):\n",
    "                    batch_df = lf.slice(batch_start, batch_size).collect()\n",
    "                    \n",
    "                    if batch_df.is_empty():\n",
    "                        continue\n",
    "                    \n",
    "                    chunks_df = process_batch_threaded(batch_df, coref_model, thread_pool)\n",
    "                    \n",
    "                    if not chunks_df.is_empty():\n",
    "                        file_chunks.append(chunks_df)\n",
    "                    \n",
    "                    file_docs += len(batch_df)\n",
    "                    del batch_df\n",
    "                \n",
    "                if file_chunks:\n",
    "                    result_df = pl.concat(file_chunks)\n",
    "                    \n",
    "                    if 'year' in result_df.columns and 'month' in result_df.columns:\n",
    "                        year = result_df['year'][0]\n",
    "                        month = result_df['month'][0]\n",
    "                        out_file = Path(output_path) / f\"year={year}\" / f\"month={month}\" / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    else:\n",
    "                        out_file = Path(output_path) / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    \n",
    "                    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    result_df.write_parquet(out_file, compression='zstd', compression_level=3)\n",
    "                    \n",
    "                    result_queue.put((file_idx, file_docs, len(result_df)))\n",
    "                    del file_chunks, result_df\n",
    "                else:\n",
    "                    result_queue.put((file_idx, file_docs, 0))\n",
    "                \n",
    "                gc.collect()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Worker error on {parquet_file}: {e}\")\n",
    "                result_queue.put((file_idx, 0, 0))\n",
    "\n",
    "\n",
    "def process_multi_gpu(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    batch_size: int = 1500,\n",
    "    num_gpus: int = 1\n",
    "):\n",
    "    \"\"\"\n",
    "    Multi-GPU processing using separate processes per GPU.\n",
    "    Each GPU gets its own worker process with dedicated CUDA context.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    \n",
    "    mp.set_start_method('spawn', force=True)  # Required for CUDA\n",
    "    \n",
    "    num_gpus = min(num_gpus, torch.cuda.device_count())\n",
    "    print(f\"Using {num_gpus} GPUs\")\n",
    "    \n",
    "    input_files = sorted(Path(input_path).rglob(\"*.parquet\"))\n",
    "    input_files = [\n",
    "        f for f in input_files\n",
    "        if \"month=10\" in str(f) or \"month=9\" in str(f)\n",
    "    ]\n",
    "    \n",
    "    completed_files = load_checkpoint()\n",
    "    files_to_process = [\n",
    "        (i, f) for i, f in enumerate(input_files) \n",
    "        if i not in completed_files\n",
    "    ]\n",
    "    print(f\"Files to process: {len(files_to_process)} / {len(input_files)}\")\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Queues\n",
    "    file_queue = mp.Queue()\n",
    "    result_queue = mp.Queue()\n",
    "    \n",
    "    # Start workers\n",
    "    workers = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        p = mp.Process(\n",
    "            target=worker_process,\n",
    "            args=(file_queue, result_queue, output_path, batch_size, f\"cuda:{gpu_id}\")\n",
    "        )\n",
    "        p.start()\n",
    "        workers.append(p)\n",
    "    \n",
    "    # Feed files to queue\n",
    "    for item in files_to_process:\n",
    "        file_queue.put(item)\n",
    "    \n",
    "    # Poison pills\n",
    "    for _ in workers:\n",
    "        file_queue.put(None)\n",
    "    \n",
    "    # Collect results\n",
    "    total_docs = 0\n",
    "    total_chunks = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    pbar = tqdm(total=len(files_to_process), desc=\"Processing\")\n",
    "    \n",
    "    results_received = 0\n",
    "    while results_received < len(files_to_process):\n",
    "        file_idx, docs, chunks = result_queue.get()\n",
    "        completed_files.add(file_idx)\n",
    "        total_docs += docs\n",
    "        total_chunks += chunks\n",
    "        results_received += 1\n",
    "        \n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix({\n",
    "            'docs': f'{total_docs:,}',\n",
    "            'chunks': f'{total_chunks:,}'\n",
    "        })\n",
    "        \n",
    "        if results_received % 10 == 0:\n",
    "            save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Wait for workers\n",
    "    for p in workers:\n",
    "        p.join()\n",
    "    \n",
    "    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\nDone! {total_docs:,} docs → {total_chunks:,} chunks in {elapsed/3600:.2f}h\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17785863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastCoref model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2026 12:31:22 - INFO - \t missing_keys: []\n",
      "01/20/2026 12:31:22 - INFO - \t unexpected_keys: []\n",
      "01/20/2026 12:31:22 - INFO - \t mismatched_keys: []\n",
      "01/20/2026 12:31:22 - INFO - \t error_msgs: []\n",
      "01/20/2026 12:31:22 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Found 4409 parquet files\n",
      "Resuming from checkpoint: 36 files already done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/4409 [00:00<?, ?it/s]/tmp/ipykernel_3999543/2805755274.py:96: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  file_rows = lf.select(pl.count()).collect().item()\n",
      "01/20/2026 12:31:22 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:39<00:00, 37.59 examples/s]\n",
      "01/20/2026 12:32:02 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 133.78it/s]\n",
      "01/20/2026 12:32:14 - INFO - \t Tokenize 201 inputs...\n",
      "Map: 100%|██████████| 201/201 [00:04<00:00, 43.38 examples/s]\n",
      "01/20/2026 12:32:19 - INFO - \t ***** Running Inference on 201 texts *****\n",
      "Inference: 100%|██████████| 201/201 [00:01<00:00, 145.39it/s]\n",
      "Files:   1%|          | 37/4409 [00:58<1:54:25,  1.57s/it, docs=1,701, chunks=3,457, docs/s=29.3]01/20/2026 12:32:20 - INFO - \t Tokenize 1373 inputs...\n",
      "Map: 100%|██████████| 1373/1373 [00:30<00:00, 45.33 examples/s]\n",
      "01/20/2026 12:32:51 - INFO - \t ***** Running Inference on 1373 texts *****\n",
      "Inference: 100%|██████████| 1373/1373 [00:08<00:00, 160.02it/s]\n",
      "Files:   1%|          | 38/4409 [01:37<3:37:39,  2.99s/it, docs=3,074, chunks=6,088, docs/s=31.4]01/20/2026 12:33:00 - INFO - \t Tokenize 1335 inputs...\n",
      "Map: 100%|██████████| 1335/1335 [00:32<00:00, 40.67 examples/s]\n",
      "01/20/2026 12:33:33 - INFO - \t ***** Running Inference on 1335 texts *****\n",
      "Inference: 100%|██████████| 1335/1335 [00:09<00:00, 145.73it/s]\n",
      "Files:   1%|          | 39/4409 [02:20<6:04:37,  5.01s/it, docs=4,409, chunks=8,738, docs/s=31.3]01/20/2026 12:33:43 - INFO - \t Tokenize 1477 inputs...\n",
      "Map: 100%|██████████| 1477/1477 [00:28<00:00, 52.63 examples/s]\n",
      "01/20/2026 12:34:11 - INFO - \t ***** Running Inference on 1477 texts *****\n",
      "Inference: 100%|██████████| 1477/1477 [00:08<00:00, 176.70it/s]\n",
      "Files:   1%|          | 40/4409 [02:58<8:42:37,  7.18s/it, docs=5,886, chunks=11,308, docs/s=33.0]01/20/2026 12:34:21 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:35<00:00, 42.22 examples/s]\n",
      "01/20/2026 12:34:56 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 151.36it/s]\n",
      "01/20/2026 12:35:07 - INFO - \t Tokenize 34 inputs...\n",
      "Map: 100%|██████████| 34/34 [00:00<00:00, 40.46 examples/s]\n",
      "01/20/2026 12:35:08 - INFO - \t ***** Running Inference on 34 texts *****\n",
      "Inference: 100%|██████████| 34/34 [00:00<00:00, 132.51it/s]\n",
      "Files:   1%|          | 41/4409 [03:45<13:01:37, 10.74s/it, docs=7,420, chunks=14,264, docs/s=32.8]01/20/2026 12:35:08 - INFO - \t Tokenize 717 inputs...\n",
      "Map: 100%|██████████| 717/717 [00:21<00:00, 33.13 examples/s]\n",
      "01/20/2026 12:35:30 - INFO - \t ***** Running Inference on 717 texts *****\n",
      "Inference: 100%|██████████| 717/717 [00:05<00:00, 126.20it/s]\n",
      "Files:   1%|          | 42/4409 [04:13<15:19:39, 12.64s/it, docs=8,137, chunks=15,788, docs/s=32.1]01/20/2026 12:35:36 - INFO - \t Tokenize 1277 inputs...\n",
      "Map: 100%|██████████| 1277/1277 [00:36<00:00, 35.06 examples/s]\n",
      "01/20/2026 12:36:13 - INFO - \t ***** Running Inference on 1277 texts *****\n",
      "Inference: 100%|██████████| 1277/1277 [00:10<00:00, 127.44it/s]\n",
      "Files:   1%|          | 43/4409 [05:00<21:03:37, 17.37s/it, docs=9,414, chunks=18,525, docs/s=31.3]01/20/2026 12:36:23 - INFO - \t Tokenize 1494 inputs...\n",
      "Map: 100%|██████████| 1494/1494 [00:33<00:00, 44.36 examples/s]\n",
      "01/20/2026 12:36:57 - INFO - \t ***** Running Inference on 1494 texts *****\n",
      "Inference: 100%|██████████| 1494/1494 [00:09<00:00, 158.62it/s]\n",
      "Files:   1%|          | 44/4409 [05:45<26:23:34, 21.77s/it, docs=10,908, chunks=21,500, docs/s=31.6]01/20/2026 12:37:08 - INFO - \t Tokenize 944 inputs...\n",
      "Map: 100%|██████████| 944/944 [00:22<00:00, 42.41 examples/s]\n",
      "01/20/2026 12:37:30 - INFO - \t ***** Running Inference on 944 texts *****\n",
      "Inference: 100%|██████████| 944/944 [00:06<00:00, 148.87it/s]\n",
      "Files:   1%|          | 45/4409 [06:14<28:05:37, 23.18s/it, docs=11,852, chunks=23,246, docs/s=31.7]01/20/2026 12:37:37 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:44<00:00, 33.71 examples/s]\n",
      "01/20/2026 12:38:21 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:12<00:00, 119.94it/s]\n",
      "01/20/2026 12:38:35 - INFO - \t Tokenize 209 inputs...\n",
      "Map: 100%|██████████| 209/209 [00:06<00:00, 33.94 examples/s]\n",
      "01/20/2026 12:38:41 - INFO - \t ***** Running Inference on 209 texts *****\n",
      "Inference: 100%|██████████| 209/209 [00:01<00:00, 115.11it/s]\n",
      "Files:   1%|          | 46/4409 [07:21<39:21:22, 32.47s/it, docs=13,561, chunks=27,112, docs/s=30.7]01/20/2026 12:38:44 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:31<00:00, 47.36 examples/s]\n",
      "01/20/2026 12:39:15 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 160.14it/s]\n",
      "01/20/2026 12:39:25 - INFO - \t Tokenize 64 inputs...\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 71.17 examples/s]\n",
      "01/20/2026 12:39:26 - INFO - \t ***** Running Inference on 64 texts *****\n",
      "Inference: 100%|██████████| 64/64 [00:00<00:00, 139.41it/s]\n",
      "Files:   1%|          | 47/4409 [08:04<42:26:04, 35.02s/it, docs=15,125, chunks=30,038, docs/s=31.2]01/20/2026 12:39:27 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:33<00:00, 44.96 examples/s]\n",
      "01/20/2026 12:40:00 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 154.65it/s]\n",
      "01/20/2026 12:40:11 - INFO - \t Tokenize 142 inputs...\n",
      "Map: 100%|██████████| 142/142 [00:03<00:00, 36.83 examples/s]\n",
      "01/20/2026 12:40:15 - INFO - \t ***** Running Inference on 142 texts *****\n",
      "Inference: 100%|██████████| 142/142 [00:01<00:00, 136.50it/s]\n",
      "Files:   1%|          | 48/4409 [08:53<46:39:52, 38.52s/it, docs=16,767, chunks=33,205, docs/s=31.4]01/20/2026 12:40:16 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.71 examples/s]\n",
      "01/20/2026 12:40:50 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 150.08it/s]\n",
      "01/20/2026 12:41:01 - INFO - \t Tokenize 24 inputs...\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 41.64 examples/s]\n",
      "01/20/2026 12:41:02 - INFO - \t ***** Running Inference on 24 texts *****\n",
      "Inference: 100%|██████████| 24/24 [00:00<00:00, 98.78it/s] \n",
      "Files:   1%|          | 49/4409 [09:39<49:05:36, 40.54s/it, docs=18,291, chunks=36,114, docs/s=31.6]01/20/2026 12:41:02 - INFO - \t Tokenize 1207 inputs...\n",
      "Map: 100%|██████████| 1207/1207 [00:31<00:00, 38.19 examples/s]\n",
      "01/20/2026 12:41:34 - INFO - \t ***** Running Inference on 1207 texts *****\n",
      "Inference: 100%|██████████| 1207/1207 [00:09<00:00, 132.28it/s]\n",
      "Files:   1%|          | 50/4409 [10:21<49:24:08, 40.80s/it, docs=19,498, chunks=38,689, docs/s=31.4]01/20/2026 12:41:44 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 40.89 examples/s]\n",
      "01/20/2026 12:42:20 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 141.36it/s]\n",
      "01/20/2026 12:42:32 - INFO - \t Tokenize 79 inputs...\n",
      "Map: 100%|██████████| 79/79 [00:01<00:00, 39.82 examples/s]\n",
      "01/20/2026 12:42:34 - INFO - \t ***** Running Inference on 79 texts *****\n",
      "Inference: 100%|██████████| 79/79 [00:00<00:00, 138.68it/s]\n",
      "Files:   1%|          | 51/4409 [11:12<53:01:43, 43.81s/it, docs=21,077, chunks=41,739, docs/s=31.3]01/20/2026 12:42:35 - INFO - \t Tokenize 1012 inputs...\n",
      "Map: 100%|██████████| 1012/1012 [00:23<00:00, 42.23 examples/s]\n",
      "01/20/2026 12:42:59 - INFO - \t ***** Running Inference on 1012 texts *****\n",
      "Inference: 100%|██████████| 1012/1012 [00:06<00:00, 147.10it/s]\n",
      "Files:   1%|          | 52/4409 [11:44<48:45:25, 40.29s/it, docs=22,089, chunks=43,707, docs/s=31.4]01/20/2026 12:43:07 - INFO - \t Tokenize 1285 inputs...\n",
      "Map: 100%|██████████| 1285/1285 [00:27<00:00, 46.11 examples/s]\n",
      "01/20/2026 12:43:35 - INFO - \t ***** Running Inference on 1285 texts *****\n",
      "Inference: 100%|██████████| 1285/1285 [00:08<00:00, 160.07it/s]\n",
      "Files:   1%|          | 53/4409 [12:20<47:27:45, 39.23s/it, docs=23,374, chunks=46,091, docs/s=31.5]01/20/2026 12:43:43 - INFO - \t Tokenize 1281 inputs...\n",
      "Map: 100%|██████████| 1281/1281 [00:31<00:00, 40.98 examples/s]\n",
      "01/20/2026 12:44:15 - INFO - \t ***** Running Inference on 1281 texts *****\n",
      "Inference: 100%|██████████| 1281/1281 [00:09<00:00, 140.35it/s]\n",
      "Files:   1%|          | 54/4409 [13:02<48:08:59, 39.80s/it, docs=24,655, chunks=48,644, docs/s=31.5]01/20/2026 12:44:24 - INFO - \t Tokenize 1127 inputs...\n",
      "Map: 100%|██████████| 1127/1127 [00:32<00:00, 34.67 examples/s]\n",
      "01/20/2026 12:44:57 - INFO - \t ***** Running Inference on 1127 texts *****\n",
      "Inference: 100%|██████████| 1127/1127 [00:08<00:00, 129.96it/s]\n",
      "Files:   1%|          | 55/4409 [13:44<48:55:17, 40.45s/it, docs=25,782, chunks=51,023, docs/s=31.3]01/20/2026 12:45:06 - INFO - \t Tokenize 878 inputs...\n",
      "Map: 100%|██████████| 878/878 [00:28<00:00, 30.50 examples/s]\n",
      "01/20/2026 12:45:35 - INFO - \t ***** Running Inference on 878 texts *****\n",
      "Inference: 100%|██████████| 878/878 [00:07<00:00, 117.31it/s]\n",
      "Files:   1%|▏         | 56/4409 [14:21<47:45:14, 39.49s/it, docs=26,660, chunks=52,994, docs/s=31.0]01/20/2026 12:45:44 - INFO - \t Tokenize 1300 inputs...\n",
      "Map: 100%|██████████| 1300/1300 [00:28<00:00, 45.62 examples/s]\n",
      "01/20/2026 12:46:12 - INFO - \t ***** Running Inference on 1300 texts *****\n",
      "Inference: 100%|██████████| 1300/1300 [00:08<00:00, 156.31it/s]\n",
      "Files:   1%|▏         | 57/4409 [14:58<47:06:20, 38.97s/it, docs=27,960, chunks=55,437, docs/s=31.1]01/20/2026 12:46:21 - INFO - \t Tokenize 1308 inputs...\n",
      "Map: 100%|██████████| 1308/1308 [00:44<00:00, 29.69 examples/s]\n",
      "01/20/2026 12:47:06 - INFO - \t ***** Running Inference on 1308 texts *****\n",
      "Inference: 100%|██████████| 1308/1308 [00:11<00:00, 110.39it/s]\n",
      "Files:   1%|▏         | 58/4409 [15:55<53:31:42, 44.29s/it, docs=29,268, chunks=58,225, docs/s=30.6]01/20/2026 12:47:18 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:39<00:00, 38.01 examples/s]\n",
      "01/20/2026 12:47:58 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 128.70it/s]\n",
      "01/20/2026 12:48:10 - INFO - \t Tokenize 62 inputs...\n",
      "Map: 100%|██████████| 62/62 [00:01<00:00, 38.53 examples/s]\n",
      "01/20/2026 12:48:12 - INFO - \t ***** Running Inference on 62 texts *****\n",
      "Inference: 100%|██████████| 62/62 [00:00<00:00, 88.62it/s]\n",
      "Files:   1%|▏         | 59/4409 [16:50<57:18:06, 47.42s/it, docs=30,830, chunks=61,485, docs/s=30.5]01/20/2026 12:48:13 - INFO - \t Tokenize 1256 inputs...\n",
      "Map: 100%|██████████| 1256/1256 [00:27<00:00, 45.20 examples/s]\n",
      "01/20/2026 12:48:41 - INFO - \t ***** Running Inference on 1256 texts *****\n",
      "Inference: 100%|██████████| 1256/1256 [00:08<00:00, 153.90it/s]\n",
      "Files:   1%|▏         | 60/4409 [17:27<53:26:44, 44.24s/it, docs=32,086, chunks=63,851, docs/s=30.6]01/20/2026 12:48:50 - INFO - \t Tokenize 944 inputs...\n",
      "Map: 100%|██████████| 944/944 [00:24<00:00, 39.00 examples/s]\n",
      "01/20/2026 12:49:14 - INFO - \t ***** Running Inference on 944 texts *****\n",
      "Inference: 100%|██████████| 944/944 [00:06<00:00, 137.32it/s]\n",
      "Files:   1%|▏         | 61/4409 [17:59<49:00:46, 40.58s/it, docs=33,030, chunks=65,858, docs/s=30.6]01/20/2026 12:49:22 - INFO - \t Tokenize 1079 inputs...\n",
      "Map: 100%|██████████| 1079/1079 [00:26<00:00, 40.89 examples/s]\n",
      "01/20/2026 12:49:48 - INFO - \t ***** Running Inference on 1079 texts *****\n",
      "Inference: 100%|██████████| 1079/1079 [00:07<00:00, 145.52it/s]\n",
      "Files:   1%|▏         | 62/4409 [18:34<46:53:21, 38.83s/it, docs=34,109, chunks=67,996, docs/s=30.6]01/20/2026 12:49:57 - INFO - \t Tokenize 946 inputs...\n",
      "Map: 100%|██████████| 946/946 [00:15<00:00, 62.05 examples/s]\n",
      "01/20/2026 12:50:12 - INFO - \t ***** Running Inference on 946 texts *****\n",
      "Inference: 100%|██████████| 946/946 [00:04<00:00, 211.49it/s]\n",
      "Files:   1%|▏         | 63/4409 [18:54<40:11:02, 33.29s/it, docs=35,055, chunks=69,486, docs/s=30.9]01/20/2026 12:50:17 - INFO - \t Tokenize 1378 inputs...\n",
      "Map: 100%|██████████| 1378/1378 [00:38<00:00, 35.48 examples/s]\n",
      "01/20/2026 12:50:56 - INFO - \t ***** Running Inference on 1378 texts *****\n",
      "Inference: 100%|██████████| 1378/1378 [00:11<00:00, 124.76it/s]\n",
      "Files:   1%|▏         | 64/4409 [19:45<46:37:22, 38.63s/it, docs=36,433, chunks=72,573, docs/s=30.7]01/20/2026 12:51:08 - INFO - \t Tokenize 682 inputs...\n",
      "Map: 100%|██████████| 682/682 [00:15<00:00, 43.19 examples/s]\n",
      "01/20/2026 12:51:24 - INFO - \t ***** Running Inference on 682 texts *****\n",
      "Inference: 100%|██████████| 682/682 [00:04<00:00, 147.09it/s]\n",
      "Files:   1%|▏         | 65/4409 [20:06<40:15:30, 33.36s/it, docs=37,115, chunks=73,911, docs/s=30.8]01/20/2026 12:51:29 - INFO - \t Tokenize 1162 inputs...\n",
      "Map: 100%|██████████| 1162/1162 [00:28<00:00, 40.42 examples/s]\n",
      "01/20/2026 12:51:58 - INFO - \t ***** Running Inference on 1162 texts *****\n",
      "Inference: 100%|██████████| 1162/1162 [00:08<00:00, 140.80it/s]\n",
      "Files:   1%|▏         | 66/4409 [20:44<41:59:34, 34.81s/it, docs=38,277, chunks=76,283, docs/s=30.7]01/20/2026 12:52:07 - INFO - \t Tokenize 1145 inputs...\n",
      "Map: 100%|██████████| 1145/1145 [00:33<00:00, 33.85 examples/s]\n",
      "01/20/2026 12:52:41 - INFO - \t ***** Running Inference on 1145 texts *****\n",
      "Inference: 100%|██████████| 1145/1145 [00:08<00:00, 127.68it/s]\n",
      "Files:   2%|▏         | 67/4409 [21:28<45:14:53, 37.52s/it, docs=39,422, chunks=78,839, docs/s=30.6]01/20/2026 12:52:51 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 40.54 examples/s]\n",
      "01/20/2026 12:53:28 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 137.81it/s]\n",
      "01/20/2026 12:53:40 - INFO - \t Tokenize 225 inputs...\n",
      "Map: 100%|██████████| 225/225 [00:05<00:00, 37.94 examples/s]\n",
      "01/20/2026 12:53:46 - INFO - \t ***** Running Inference on 225 texts *****\n",
      "Inference: 100%|██████████| 225/225 [00:01<00:00, 128.39it/s]\n",
      "Files:   2%|▏         | 68/4409 [22:25<52:19:49, 43.40s/it, docs=41,147, chunks=82,330, docs/s=30.6]01/20/2026 12:53:48 - INFO - \t Tokenize 937 inputs...\n",
      "Map: 100%|██████████| 937/937 [00:21<00:00, 44.19 examples/s]\n",
      "01/20/2026 12:54:10 - INFO - \t ***** Running Inference on 937 texts *****\n",
      "Inference: 100%|██████████| 937/937 [00:06<00:00, 151.13it/s]\n",
      "Files:   2%|▏         | 69/4409 [22:54<46:50:17, 38.85s/it, docs=42,084, chunks=84,039, docs/s=30.6]01/20/2026 12:54:16 - INFO - \t Tokenize 1001 inputs...\n",
      "Map: 100%|██████████| 1001/1001 [00:25<00:00, 39.67 examples/s]\n",
      "01/20/2026 12:54:42 - INFO - \t ***** Running Inference on 1001 texts *****\n",
      "Inference: 100%|██████████| 1001/1001 [00:07<00:00, 137.46it/s]\n",
      "Files:   2%|▏         | 70/4409 [23:27<44:56:04, 37.28s/it, docs=43,085, chunks=86,138, docs/s=30.6]01/20/2026 12:54:50 - INFO - \t Tokenize 1200 inputs...\n",
      "Map: 100%|██████████| 1200/1200 [00:27<00:00, 43.92 examples/s]\n",
      "01/20/2026 12:55:18 - INFO - \t ***** Running Inference on 1200 texts *****\n",
      "Inference: 100%|██████████| 1200/1200 [00:07<00:00, 155.74it/s]\n",
      "Files:   2%|▏         | 71/4409 [24:04<44:36:03, 37.01s/it, docs=44,285, chunks=88,401, docs/s=30.7]01/20/2026 12:55:26 - INFO - \t Tokenize 1080 inputs...\n",
      "Map: 100%|██████████| 1080/1080 [00:35<00:00, 30.60 examples/s]\n",
      "01/20/2026 12:56:02 - INFO - \t ***** Running Inference on 1080 texts *****\n",
      "Inference: 100%|██████████| 1080/1080 [00:08<00:00, 124.04it/s]\n",
      "Files:   2%|▏         | 72/4409 [24:49<47:29:48, 39.43s/it, docs=45,365, chunks=90,755, docs/s=30.5]01/20/2026 12:56:11 - INFO - \t Tokenize 675 inputs...\n",
      "Map: 100%|██████████| 675/675 [00:16<00:00, 40.20 examples/s]\n",
      "01/20/2026 12:56:29 - INFO - \t ***** Running Inference on 675 texts *****\n",
      "Inference: 100%|██████████| 675/675 [00:04<00:00, 140.28it/s]\n",
      "Files:   2%|▏         | 73/4409 [25:11<41:16:33, 34.27s/it, docs=46,040, chunks=92,137, docs/s=30.5]01/20/2026 12:56:34 - INFO - \t Tokenize 1354 inputs...\n",
      "Map: 100%|██████████| 1354/1354 [00:33<00:00, 40.01 examples/s]\n",
      "01/20/2026 12:57:08 - INFO - \t ***** Running Inference on 1354 texts *****\n",
      "Inference: 100%|██████████| 1354/1354 [00:09<00:00, 137.74it/s]\n",
      "Files:   2%|▏         | 74/4409 [25:55<45:00:57, 37.38s/it, docs=47,394, chunks=94,979, docs/s=30.5]01/20/2026 12:57:18 - INFO - \t Tokenize 902 inputs...\n",
      "Map: 100%|██████████| 902/902 [00:21<00:00, 42.14 examples/s]\n",
      "01/20/2026 12:57:40 - INFO - \t ***** Running Inference on 902 texts *****\n",
      "Inference: 100%|██████████| 902/902 [00:06<00:00, 146.21it/s]\n",
      "Files:   2%|▏         | 75/4409 [26:24<41:47:02, 34.71s/it, docs=48,296, chunks=96,763, docs/s=30.5]01/20/2026 12:57:47 - INFO - \t Tokenize 1445 inputs...\n",
      "Map: 100%|██████████| 1445/1445 [00:53<00:00, 26.89 examples/s]\n",
      "01/20/2026 12:58:41 - INFO - \t ***** Running Inference on 1445 texts *****\n",
      "Inference: 100%|██████████| 1445/1445 [00:12<00:00, 117.14it/s]\n",
      "Files:   2%|▏         | 76/4409 [27:31<53:38:19, 44.56s/it, docs=49,741, chunks=99,978, docs/s=30.1]01/20/2026 12:58:54 - INFO - \t Tokenize 660 inputs...\n",
      "Map: 100%|██████████| 660/660 [00:16<00:00, 40.88 examples/s]\n",
      "01/20/2026 12:59:11 - INFO - \t ***** Running Inference on 660 texts *****\n",
      "Inference: 100%|██████████| 660/660 [00:04<00:00, 145.72it/s]\n",
      "Files:   2%|▏         | 77/4409 [27:53<45:16:36, 37.63s/it, docs=50,401, chunks=101,292, docs/s=30.1]01/20/2026 12:59:16 - INFO - \t Tokenize 1012 inputs...\n",
      "Map: 100%|██████████| 1012/1012 [00:23<00:00, 43.38 examples/s]\n",
      "01/20/2026 12:59:39 - INFO - \t ***** Running Inference on 1012 texts *****\n",
      "Inference: 100%|██████████| 1012/1012 [00:06<00:00, 151.93it/s]\n",
      "Files:   2%|▏         | 78/4409 [28:24<42:48:56, 35.59s/it, docs=51,413, chunks=103,197, docs/s=30.2]01/20/2026 12:59:47 - INFO - \t Tokenize 1116 inputs...\n",
      "Map: 100%|██████████| 1116/1116 [00:26<00:00, 42.58 examples/s]\n",
      "01/20/2026 13:00:13 - INFO - \t ***** Running Inference on 1116 texts *****\n",
      "Inference: 100%|██████████| 1116/1116 [00:07<00:00, 149.56it/s]\n",
      "Files:   2%|▏         | 79/4409 [28:58<42:28:14, 35.31s/it, docs=52,529, chunks=105,276, docs/s=30.2]01/20/2026 13:00:21 - INFO - \t Tokenize 1343 inputs...\n",
      "Map: 100%|██████████| 1343/1343 [00:43<00:00, 30.56 examples/s]\n",
      "01/20/2026 13:01:06 - INFO - \t ***** Running Inference on 1343 texts *****\n",
      "Inference: 100%|██████████| 1343/1343 [00:11<00:00, 114.51it/s]\n",
      "Files:   2%|▏         | 80/4409 [29:55<50:18:31, 41.84s/it, docs=53,872, chunks=107,899, docs/s=30.0]01/20/2026 13:01:18 - INFO - \t Tokenize 743 inputs...\n",
      "Map: 100%|██████████| 743/743 [00:17<00:00, 43.55 examples/s]\n",
      "01/20/2026 13:01:36 - INFO - \t ***** Running Inference on 743 texts *****\n",
      "Inference: 100%|██████████| 743/743 [00:05<00:00, 142.54it/s]\n",
      "Files:   2%|▏         | 81/4409 [30:19<43:33:57, 36.24s/it, docs=54,615, chunks=109,306, docs/s=30.0]01/20/2026 13:01:42 - INFO - \t Tokenize 760 inputs...\n",
      "Map: 100%|██████████| 760/760 [00:16<00:00, 47.47 examples/s]\n",
      "01/20/2026 13:01:58 - INFO - \t ***** Running Inference on 760 texts *****\n",
      "Inference: 100%|██████████| 760/760 [00:04<00:00, 173.85it/s]\n",
      "Files:   2%|▏         | 82/4409 [30:40<38:04:55, 31.68s/it, docs=55,375, chunks=110,651, docs/s=30.1]01/20/2026 13:02:03 - INFO - \t Tokenize 1123 inputs...\n",
      "Map: 100%|██████████| 1123/1123 [00:24<00:00, 45.66 examples/s]\n",
      "01/20/2026 13:02:28 - INFO - \t ***** Running Inference on 1123 texts *****\n",
      "Inference: 100%|██████████| 1123/1123 [00:07<00:00, 159.80it/s]\n",
      "Files:   2%|▏         | 83/4409 [31:12<38:24:50, 31.97s/it, docs=56,498, chunks=112,722, docs/s=30.2]01/20/2026 13:02:35 - INFO - \t Tokenize 1242 inputs...\n",
      "Map: 100%|██████████| 1242/1242 [00:28<00:00, 43.94 examples/s]\n",
      "01/20/2026 13:03:04 - INFO - \t ***** Running Inference on 1242 texts *****\n",
      "Inference: 100%|██████████| 1242/1242 [00:08<00:00, 152.97it/s]\n",
      "Files:   2%|▏         | 84/4409 [31:50<40:21:07, 33.59s/it, docs=57,740, chunks=115,035, docs/s=30.2]01/20/2026 13:03:13 - INFO - \t Tokenize 995 inputs...\n",
      "Map: 100%|██████████| 995/995 [00:27<00:00, 36.69 examples/s]\n",
      "01/20/2026 13:03:40 - INFO - \t ***** Running Inference on 995 texts *****\n",
      "Inference: 100%|██████████| 995/995 [00:07<00:00, 126.73it/s]\n",
      "Files:   2%|▏         | 85/4409 [32:26<41:11:57, 34.30s/it, docs=58,735, chunks=117,083, docs/s=30.2]01/20/2026 13:03:49 - INFO - \t Tokenize 1139 inputs...\n",
      "Map: 100%|██████████| 1139/1139 [00:25<00:00, 44.50 examples/s]\n",
      "01/20/2026 13:04:15 - INFO - \t ***** Running Inference on 1139 texts *****\n",
      "Inference: 100%|██████████| 1139/1139 [00:07<00:00, 152.80it/s]\n",
      "Files:   2%|▏         | 86/4409 [33:00<41:08:48, 34.27s/it, docs=59,874, chunks=119,238, docs/s=30.2]01/20/2026 13:04:23 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:32<00:00, 46.83 examples/s]\n",
      "01/20/2026 13:04:55 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 151.04it/s]\n",
      "01/20/2026 13:05:06 - INFO - \t Tokenize 24 inputs...\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 57.09 examples/s]\n",
      "01/20/2026 13:05:07 - INFO - \t ***** Running Inference on 24 texts *****\n",
      "Inference: 100%|██████████| 24/24 [00:00<00:00, 68.51it/s]\n",
      "Files:   2%|▏         | 87/4409 [33:44<44:47:03, 37.30s/it, docs=61,398, chunks=121,965, docs/s=30.3]01/20/2026 13:05:07 - INFO - \t Tokenize 1243 inputs...\n",
      "Map: 100%|██████████| 1243/1243 [00:31<00:00, 40.00 examples/s]\n",
      "01/20/2026 13:05:39 - INFO - \t ***** Running Inference on 1243 texts *****\n",
      "Inference: 100%|██████████| 1243/1243 [00:08<00:00, 144.09it/s]\n",
      "Files:   2%|▏         | 88/4409 [34:25<46:01:10, 38.34s/it, docs=62,641, chunks=124,413, docs/s=30.3]01/20/2026 13:05:48 - INFO - \t Tokenize 1138 inputs...\n",
      "Map: 100%|██████████| 1138/1138 [00:31<00:00, 36.46 examples/s]\n",
      "01/20/2026 13:06:20 - INFO - \t ***** Running Inference on 1138 texts *****\n",
      "Inference: 100%|██████████| 1138/1138 [00:08<00:00, 132.27it/s]\n",
      "Files:   2%|▏         | 89/4409 [35:06<46:54:55, 39.10s/it, docs=63,779, chunks=126,792, docs/s=30.3]01/20/2026 13:06:29 - INFO - \t Tokenize 1208 inputs...\n",
      "Map: 100%|██████████| 1208/1208 [00:35<00:00, 34.33 examples/s]\n",
      "01/20/2026 13:07:04 - INFO - \t ***** Running Inference on 1208 texts *****\n",
      "Inference: 100%|██████████| 1208/1208 [00:09<00:00, 129.98it/s]\n",
      "Files:   2%|▏         | 90/4409 [35:52<49:20:26, 41.13s/it, docs=64,987, chunks=129,134, docs/s=30.2]01/20/2026 13:07:15 - INFO - \t Tokenize 1436 inputs...\n",
      "Map: 100%|██████████| 1436/1436 [00:31<00:00, 45.27 examples/s]\n",
      "01/20/2026 13:07:47 - INFO - \t ***** Running Inference on 1436 texts *****\n",
      "Inference: 100%|██████████| 1436/1436 [00:09<00:00, 157.12it/s]\n",
      "Files:   2%|▏         | 91/4409 [36:34<49:44:21, 41.47s/it, docs=66,423, chunks=131,990, docs/s=30.3]01/20/2026 13:07:57 - INFO - \t Tokenize 966 inputs...\n",
      "Map: 100%|██████████| 966/966 [00:25<00:00, 38.07 examples/s]\n",
      "01/20/2026 13:08:23 - INFO - \t ***** Running Inference on 966 texts *****\n",
      "Inference: 100%|██████████| 966/966 [00:07<00:00, 121.46it/s]\n",
      "Files:   2%|▏         | 92/4409 [37:08<47:11:26, 39.35s/it, docs=67,389, chunks=134,018, docs/s=30.2]01/20/2026 13:08:31 - INFO - \t Tokenize 1152 inputs...\n",
      "Map: 100%|██████████| 1152/1152 [00:25<00:00, 45.51 examples/s]\n",
      "01/20/2026 13:08:57 - INFO - \t ***** Running Inference on 1152 texts *****\n",
      "Inference: 100%|██████████| 1152/1152 [00:07<00:00, 153.69it/s]\n",
      "Files:   2%|▏         | 93/4409 [37:42<45:10:31, 37.68s/it, docs=68,541, chunks=136,110, docs/s=30.3]01/20/2026 13:09:05 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:38<00:00, 38.91 examples/s]\n",
      "01/20/2026 13:09:44 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 135.97it/s]\n",
      "01/20/2026 13:09:56 - INFO - \t Tokenize 244 inputs...\n",
      "Map: 100%|██████████| 244/244 [00:05<00:00, 41.63 examples/s]\n",
      "01/20/2026 13:10:02 - INFO - \t ***** Running Inference on 244 texts *****\n",
      "Inference: 100%|██████████| 244/244 [00:01<00:00, 139.32it/s]\n",
      "Files:   2%|▏         | 94/4409 [38:41<52:53:55, 44.13s/it, docs=70,285, chunks=139,797, docs/s=30.3]01/20/2026 13:10:04 - INFO - \t Tokenize 986 inputs...\n",
      "Map: 100%|██████████| 986/986 [00:31<00:00, 31.30 examples/s]\n",
      "01/20/2026 13:10:36 - INFO - \t ***** Running Inference on 986 texts *****\n",
      "Inference: 100%|██████████| 986/986 [00:08<00:00, 113.92it/s]\n",
      "Files:   2%|▏         | 95/4409 [39:23<51:54:20, 43.31s/it, docs=71,271, chunks=141,926, docs/s=30.2]01/20/2026 13:10:46 - INFO - \t Tokenize 1361 inputs...\n",
      "Map: 100%|██████████| 1361/1361 [00:32<00:00, 41.40 examples/s]\n",
      "01/20/2026 13:11:19 - INFO - \t ***** Running Inference on 1361 texts *****\n",
      "Inference: 100%|██████████| 1361/1361 [00:09<00:00, 139.57it/s]\n",
      "Files:   2%|▏         | 96/4409 [40:07<52:11:35, 43.56s/it, docs=72,632, chunks=144,604, docs/s=30.2]01/20/2026 13:11:30 - INFO - \t Tokenize 852 inputs...\n",
      "Map: 100%|██████████| 852/852 [00:19<00:00, 44.70 examples/s]\n",
      "01/20/2026 13:11:49 - INFO - \t ***** Running Inference on 852 texts *****\n",
      "Inference: 100%|██████████| 852/852 [00:05<00:00, 155.56it/s]\n",
      "Files:   2%|▏         | 97/4409 [40:32<45:40:00, 38.13s/it, docs=73,484, chunks=146,204, docs/s=30.2]01/20/2026 13:11:55 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.31 examples/s]\n",
      "01/20/2026 13:12:30 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 149.65it/s]\n",
      "01/20/2026 13:12:41 - INFO - \t Tokenize 110 inputs...\n",
      "Map: 100%|██████████| 110/110 [00:03<00:00, 35.97 examples/s]\n",
      "01/20/2026 13:12:44 - INFO - \t ***** Running Inference on 110 texts *****\n",
      "Inference: 100%|██████████| 110/110 [00:01<00:00, 96.74it/s]\n",
      "Files:   2%|▏         | 98/4409 [41:23<50:05:39, 41.83s/it, docs=75,094, chunks=149,334, docs/s=30.2]01/20/2026 13:12:46 - INFO - \t Tokenize 1116 inputs...\n",
      "Map: 100%|██████████| 1116/1116 [00:26<00:00, 42.66 examples/s]\n",
      "01/20/2026 13:13:12 - INFO - \t ***** Running Inference on 1116 texts *****\n",
      "Inference: 100%|██████████| 1116/1116 [00:07<00:00, 144.81it/s]\n",
      "Files:   2%|▏         | 99/4409 [41:58<47:38:04, 39.79s/it, docs=76,210, chunks=151,469, docs/s=30.3]01/20/2026 13:13:21 - INFO - \t Tokenize 1189 inputs...\n",
      "Map: 100%|██████████| 1189/1189 [00:26<00:00, 45.55 examples/s]\n",
      "01/20/2026 13:13:47 - INFO - \t ***** Running Inference on 1189 texts *****\n",
      "Inference: 100%|██████████| 1189/1189 [00:07<00:00, 158.49it/s]\n",
      "Files:   2%|▏         | 100/4409 [42:33<45:54:24, 38.35s/it, docs=77,399, chunks=153,693, docs/s=30.3]01/20/2026 13:13:56 - INFO - \t Tokenize 815 inputs...\n",
      "Map: 100%|██████████| 815/815 [00:21<00:00, 38.70 examples/s]\n",
      "01/20/2026 13:14:17 - INFO - \t ***** Running Inference on 815 texts *****\n",
      "Inference: 100%|██████████| 815/815 [00:06<00:00, 127.85it/s]\n",
      "Files:   2%|▏         | 101/4409 [43:02<42:24:32, 35.44s/it, docs=78,214, chunks=155,402, docs/s=30.3]01/20/2026 13:14:24 - INFO - \t Tokenize 1075 inputs...\n",
      "Map: 100%|██████████| 1075/1075 [00:24<00:00, 44.02 examples/s]\n",
      "01/20/2026 13:14:49 - INFO - \t ***** Running Inference on 1075 texts *****\n",
      "Inference: 100%|██████████| 1075/1075 [00:07<00:00, 153.20it/s]\n",
      "Files:   2%|▏         | 102/4409 [43:34<41:21:50, 34.57s/it, docs=79,289, chunks=157,452, docs/s=30.3]01/20/2026 13:14:57 - INFO - \t Tokenize 1115 inputs...\n",
      "Map: 100%|██████████| 1115/1115 [00:23<00:00, 47.33 examples/s]\n",
      "01/20/2026 13:15:21 - INFO - \t ***** Running Inference on 1115 texts *****\n",
      "Inference: 100%|██████████| 1115/1115 [00:06<00:00, 167.84it/s]\n",
      "Files:   2%|▏         | 103/4409 [44:05<40:07:48, 33.55s/it, docs=80,404, chunks=159,487, docs/s=30.4]01/20/2026 13:15:28 - INFO - \t Tokenize 1287 inputs...\n",
      "Map: 100%|██████████| 1287/1287 [00:30<00:00, 42.52 examples/s]\n",
      "01/20/2026 13:15:59 - INFO - \t ***** Running Inference on 1287 texts *****\n",
      "Inference: 100%|██████████| 1287/1287 [00:09<00:00, 143.00it/s]\n",
      "Files:   2%|▏         | 104/4409 [44:46<42:37:18, 35.64s/it, docs=81,691, chunks=162,023, docs/s=30.4]01/20/2026 13:16:09 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:35<00:00, 42.32 examples/s]\n",
      "01/20/2026 13:16:45 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 146.37it/s]\n",
      "01/20/2026 13:16:56 - INFO - \t Tokenize 101 inputs...\n",
      "Map: 100%|██████████| 101/101 [00:02<00:00, 43.46 examples/s]\n",
      "01/20/2026 13:16:59 - INFO - \t ***** Running Inference on 101 texts *****\n",
      "Inference: 100%|██████████| 101/101 [00:00<00:00, 150.68it/s]\n",
      "Files:   2%|▏         | 105/4409 [45:37<48:03:12, 40.19s/it, docs=83,292, chunks=165,107, docs/s=30.4]01/20/2026 13:16:59 - INFO - \t Tokenize 1240 inputs...\n",
      "Map: 100%|██████████| 1240/1240 [00:26<00:00, 46.62 examples/s]\n",
      "01/20/2026 13:17:27 - INFO - \t ***** Running Inference on 1240 texts *****\n",
      "Inference: 100%|██████████| 1240/1240 [00:07<00:00, 160.15it/s]\n",
      "Files:   2%|▏         | 106/4409 [46:12<46:22:52, 38.80s/it, docs=84,532, chunks=167,334, docs/s=30.5]01/20/2026 13:17:35 - INFO - \t Tokenize 985 inputs...\n",
      "Map: 100%|██████████| 985/985 [00:21<00:00, 45.93 examples/s]\n",
      "01/20/2026 13:17:57 - INFO - \t ***** Running Inference on 985 texts *****\n",
      "Inference: 100%|██████████| 985/985 [00:05<00:00, 165.20it/s]\n",
      "Files:   2%|▏         | 107/4409 [46:41<42:38:27, 35.68s/it, docs=85,517, chunks=169,221, docs/s=30.5]01/20/2026 13:18:03 - INFO - \t Tokenize 1244 inputs...\n",
      "Map: 100%|██████████| 1244/1244 [00:28<00:00, 43.72 examples/s]\n",
      "01/20/2026 13:18:32 - INFO - \t ***** Running Inference on 1244 texts *****\n",
      "Inference: 100%|██████████| 1244/1244 [00:08<00:00, 149.69it/s]\n",
      "Files:   2%|▏         | 108/4409 [47:19<43:31:41, 36.43s/it, docs=86,761, chunks=171,662, docs/s=30.6]01/20/2026 13:18:42 - INFO - \t Tokenize 1136 inputs...\n",
      "Map: 100%|██████████| 1136/1136 [00:26<00:00, 42.24 examples/s]\n",
      "01/20/2026 13:19:09 - INFO - \t ***** Running Inference on 1136 texts *****\n",
      "Inference: 100%|██████████| 1136/1136 [00:07<00:00, 143.75it/s]\n",
      "Files:   2%|▏         | 109/4409 [47:55<43:22:36, 36.32s/it, docs=87,897, chunks=173,862, docs/s=30.6]01/20/2026 13:19:18 - INFO - \t Tokenize 1313 inputs...\n",
      "Map: 100%|██████████| 1313/1313 [00:33<00:00, 38.63 examples/s]\n",
      "01/20/2026 13:19:52 - INFO - \t ***** Running Inference on 1313 texts *****\n",
      "Inference: 100%|██████████| 1313/1313 [00:10<00:00, 130.69it/s]\n",
      "Files:   2%|▏         | 110/4409 [48:40<46:36:16, 39.03s/it, docs=89,210, chunks=176,598, docs/s=30.5]01/20/2026 13:20:03 - INFO - \t Tokenize 796 inputs...\n",
      "Map: 100%|██████████| 796/796 [00:21<00:00, 36.33 examples/s]\n",
      "01/20/2026 13:20:25 - INFO - \t ***** Running Inference on 796 texts *****\n",
      "Inference: 100%|██████████| 796/796 [00:06<00:00, 122.43it/s]\n",
      "Files:   3%|▎         | 111/4409 [49:10<43:12:43, 36.19s/it, docs=90,006, chunks=178,263, docs/s=30.5]01/20/2026 13:20:33 - INFO - \t Tokenize 1369 inputs...\n",
      "Map: 100%|██████████| 1369/1369 [00:37<00:00, 37.00 examples/s]\n",
      "01/20/2026 13:21:10 - INFO - \t ***** Running Inference on 1369 texts *****\n",
      "Inference: 100%|██████████| 1369/1369 [00:10<00:00, 127.86it/s]\n",
      "Files:   3%|▎         | 112/4409 [49:59<47:47:06, 40.03s/it, docs=91,375, chunks=181,221, docs/s=30.5]01/20/2026 13:21:22 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:38<00:00, 38.72 examples/s]\n",
      "01/20/2026 13:22:01 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 132.03it/s]\n",
      "01/20/2026 13:22:13 - INFO - \t Tokenize 23 inputs...\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 47.09 examples/s]\n",
      "01/20/2026 13:22:14 - INFO - \t ***** Running Inference on 23 texts *****\n",
      "Inference: 100%|██████████| 23/23 [00:00<00:00, 106.09it/s]\n",
      "Files:   3%|▎         | 113/4409 [50:51<52:15:49, 43.80s/it, docs=92,898, chunks=184,385, docs/s=30.4]01/20/2026 13:22:14 - INFO - \t Tokenize 1111 inputs...\n",
      "Map: 100%|██████████| 1111/1111 [00:28<00:00, 39.22 examples/s]\n",
      "01/20/2026 13:22:43 - INFO - \t ***** Running Inference on 1111 texts *****\n",
      "Inference: 100%|██████████| 1111/1111 [00:07<00:00, 139.19it/s]\n",
      "Files:   3%|▎         | 114/4409 [51:29<50:05:45, 41.99s/it, docs=94,009, chunks=186,656, docs/s=30.4]01/20/2026 13:22:52 - INFO - \t Tokenize 1026 inputs...\n",
      "Map: 100%|██████████| 1026/1026 [00:23<00:00, 43.69 examples/s]\n",
      "01/20/2026 13:23:16 - INFO - \t ***** Running Inference on 1026 texts *****\n",
      "Inference: 100%|██████████| 1026/1026 [00:07<00:00, 133.82it/s]\n",
      "Files:   3%|▎         | 115/4409 [52:01<46:36:30, 39.08s/it, docs=95,035, chunks=188,595, docs/s=30.4]01/20/2026 13:23:24 - INFO - \t Tokenize 1013 inputs...\n",
      "Map: 100%|██████████| 1013/1013 [00:30<00:00, 32.82 examples/s]\n",
      "01/20/2026 13:23:56 - INFO - \t ***** Running Inference on 1013 texts *****\n",
      "Inference: 100%|██████████| 1013/1013 [00:08<00:00, 112.94it/s]\n",
      "Files:   3%|▎         | 116/4409 [52:43<47:24:39, 39.76s/it, docs=96,048, chunks=190,927, docs/s=30.4]01/20/2026 13:24:06 - INFO - \t Tokenize 1113 inputs...\n",
      "Map: 100%|██████████| 1113/1113 [00:25<00:00, 42.86 examples/s]\n",
      "01/20/2026 13:24:32 - INFO - \t ***** Running Inference on 1113 texts *****\n",
      "Inference: 100%|██████████| 1113/1113 [00:08<00:00, 133.99it/s]\n",
      "Files:   3%|▎         | 117/4409 [53:18<45:51:46, 38.47s/it, docs=97,161, chunks=193,077, docs/s=30.4]01/20/2026 13:24:41 - INFO - \t Tokenize 1178 inputs...\n",
      "Map: 100%|██████████| 1178/1178 [00:41<00:00, 28.73 examples/s]\n",
      "01/20/2026 13:25:23 - INFO - \t ***** Running Inference on 1178 texts *****\n",
      "Inference: 100%|██████████| 1178/1178 [00:10<00:00, 110.81it/s]\n",
      "Files:   3%|▎         | 118/4409 [54:11<51:02:29, 42.82s/it, docs=98,339, chunks=195,865, docs/s=30.2]01/20/2026 13:25:34 - INFO - \t Tokenize 1257 inputs...\n",
      "Map: 100%|██████████| 1257/1257 [00:29<00:00, 42.33 examples/s]\n",
      "01/20/2026 13:26:04 - INFO - \t ***** Running Inference on 1257 texts *****\n",
      "Inference: 100%|██████████| 1257/1257 [00:08<00:00, 147.47it/s]\n",
      "Files:   3%|▎         | 119/4409 [54:51<49:52:27, 41.85s/it, docs=99,596, chunks=198,327, docs/s=30.3]01/20/2026 13:26:14 - INFO - \t Tokenize 1196 inputs...\n",
      "Map: 100%|██████████| 1196/1196 [00:31<00:00, 38.28 examples/s]\n",
      "01/20/2026 13:26:45 - INFO - \t ***** Running Inference on 1196 texts *****\n",
      "Inference: 100%|██████████| 1196/1196 [00:09<00:00, 126.28it/s]\n",
      "Files:   3%|▎         | 120/4409 [55:33<49:54:11, 41.89s/it, docs=100,792, chunks=200,735, docs/s=30.2]01/20/2026 13:26:56 - INFO - \t Tokenize 1215 inputs...\n",
      "Map: 100%|██████████| 1215/1215 [00:34<00:00, 34.84 examples/s]\n",
      "01/20/2026 13:27:31 - INFO - \t ***** Running Inference on 1215 texts *****\n",
      "Inference: 100%|██████████| 1215/1215 [00:09<00:00, 124.35it/s]\n",
      "Files:   3%|▎         | 121/4409 [56:19<51:25:10, 43.17s/it, docs=102,007, chunks=203,348, docs/s=30.2]01/20/2026 13:27:42 - INFO - \t Tokenize 827 inputs...\n",
      "Map: 100%|██████████| 827/827 [00:19<00:00, 42.46 examples/s]\n",
      "01/20/2026 13:28:02 - INFO - \t ***** Running Inference on 827 texts *****\n",
      "Inference: 100%|██████████| 827/827 [00:05<00:00, 146.04it/s]\n",
      "Files:   3%|▎         | 122/4409 [56:45<45:22:16, 38.10s/it, docs=102,834, chunks=204,916, docs/s=30.2]01/20/2026 13:28:08 - INFO - \t Tokenize 900 inputs...\n",
      "Map: 100%|██████████| 900/900 [00:18<00:00, 48.96 examples/s]\n",
      "01/20/2026 13:28:27 - INFO - \t ***** Running Inference on 900 texts *****\n",
      "Inference: 100%|██████████| 900/900 [00:05<00:00, 171.04it/s]\n",
      "Files:   3%|▎         | 123/4409 [57:10<40:33:12, 34.06s/it, docs=103,734, chunks=206,536, docs/s=30.2]01/20/2026 13:28:33 - INFO - \t Tokenize 1071 inputs...\n",
      "Map: 100%|██████████| 1071/1071 [00:24<00:00, 43.80 examples/s]\n",
      "01/20/2026 13:28:58 - INFO - \t ***** Running Inference on 1071 texts *****\n",
      "Inference: 100%|██████████| 1071/1071 [00:07<00:00, 152.72it/s]\n",
      "Files:   3%|▎         | 124/4409 [57:43<40:05:09, 33.68s/it, docs=104,805, chunks=208,522, docs/s=30.3]01/20/2026 13:29:05 - INFO - \t Tokenize 673 inputs...\n",
      "Map: 100%|██████████| 673/673 [00:16<00:00, 41.78 examples/s]\n",
      "01/20/2026 13:29:22 - INFO - \t ***** Running Inference on 673 texts *****\n",
      "Inference: 100%|██████████| 673/673 [00:04<00:00, 143.91it/s]\n",
      "Files:   3%|▎         | 125/4409 [58:04<35:49:40, 30.11s/it, docs=105,478, chunks=209,820, docs/s=30.3]01/20/2026 13:29:27 - INFO - \t Tokenize 916 inputs...\n"
     ]
    }
   ],
   "source": [
    "process_streaming_production(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131e853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebe17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_streaming(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Option 2: Polars streaming engine (for single large files)\n",
    "#process_with_streaming_engine(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
