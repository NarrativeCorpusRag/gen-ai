{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d709603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import hashlib\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "\n",
    "INPUT_PATH = \"/data/raid5/data/picatto/ascii/news/clean/\"\n",
    "OUTPUT_PATH = \"/data/raid5/data/picatto/ascii/news/chunks_coref2/\"\n",
    "CHECKPOINT_FILE = Path(OUTPUT_PATH) / \".checkpoint.json\"\n",
    "BATCH_SIZE = 1500  # Tune based on avg doc length\n",
    "DEVICE = \"cuda:0\"\n",
    "NUM_CPU_WORKERS = 48\n",
    "\n",
    "# Compile regex once at module level\n",
    "_SENTENCE_PATTERN = re.compile(r'(?<=[.!?])\\s+(?=[A-Z])')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15169c",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CPU-BOUND FUNCTIONS\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50a940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_coref_resolution(text: str, clusters: List) -> str:\n",
    "    \"\"\"Apply coreference resolution - CPU bound\"\"\"\n",
    "    if not clusters or not text:\n",
    "        return text\n",
    "    \n",
    "    try:\n",
    "        replacements = []\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) < 2:\n",
    "                continue\n",
    "            \n",
    "            spans_text = [text[s:e] for s, e in cluster]\n",
    "            canonical = max(spans_text, key=len)\n",
    "            \n",
    "            for (start, end), mention in zip(cluster, spans_text):\n",
    "                if mention == canonical:\n",
    "                    continue\n",
    "                if len(mention.split()) > 2:\n",
    "                    continue\n",
    "                replacements.append((start, end, canonical))\n",
    "        \n",
    "        replacements.sort(key=lambda x: x[0], reverse=True)\n",
    "        for start, end, replacement in replacements:\n",
    "            text = text[:start] + replacement + text[end:]\n",
    "        \n",
    "        return text\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 512, overlap: int = 50) -> List[Dict]:\n",
    "    \"\"\"Split text into chunks - CPU bound\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    sentences = _SENTENCE_PATTERN.split(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_words = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_words = len(sentence.split())\n",
    "        \n",
    "        if current_words + sentence_words <= chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_words += sentence_words\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append({\n",
    "                    'chunk_text': ' '.join(current_chunk),\n",
    "                    'chunk_word_count': current_words,\n",
    "                })\n",
    "            \n",
    "            if overlap > 0 and current_chunk:\n",
    "                overlap_text = ' '.join(current_chunk)\n",
    "                overlap_words = overlap_text.split()[-overlap:]\n",
    "                current_chunk = [' '.join(overlap_words), sentence]\n",
    "                current_words = len(overlap_words) + sentence_words\n",
    "            else:\n",
    "                current_chunk = [sentence]\n",
    "                current_words = sentence_words\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            'chunk_text': ' '.join(current_chunk),\n",
    "            'chunk_word_count': current_words,\n",
    "        })\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['chunk_index'] = i\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def process_single_doc(args: Tuple) -> List[Dict]:\n",
    "    \"\"\"Process one document - designed for parallel execution\"\"\"\n",
    "    doc_id, text, clusters, metadata = args\n",
    "    \n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    resolved_text = apply_coref_resolution(text, clusters)\n",
    "    chunks = chunk_text(resolved_text)\n",
    "    \n",
    "    rows = []\n",
    "    for chunk in chunks:\n",
    "        chunk_id = hashlib.sha256(f\"{doc_id}_{chunk['chunk_index']}\".encode()).hexdigest()[:16]\n",
    "        \n",
    "        row = {\n",
    "            'chunk_id': chunk_id,\n",
    "            'doc_id': doc_id,\n",
    "            'chunk_index': chunk['chunk_index'],\n",
    "            'chunk_text': chunk['chunk_text'],\n",
    "            'chunk_word_count': chunk['chunk_word_count'],\n",
    "            **metadata\n",
    "        }\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd9dd4",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# CHECKPOINTING\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85b7d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint() -> set:\n",
    "    \"\"\"Load set of completed file indices\"\"\"\n",
    "    if CHECKPOINT_FILE.exists():\n",
    "        with open(CHECKPOINT_FILE) as f:\n",
    "            data = json.load(f)\n",
    "            return set(data.get('completed_files', []))\n",
    "    return set()\n",
    "\n",
    "\n",
    "def save_checkpoint(completed_files: set, total_docs: int, total_chunks: int):\n",
    "    \"\"\"Save checkpoint\"\"\"\n",
    "    CHECKPOINT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump({\n",
    "            'completed_files': list(completed_files),\n",
    "            'total_docs': total_docs,\n",
    "            'total_chunks': total_chunks,\n",
    "            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77358bf2",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# PIPELINED PROCESSING - Tokenize next batch while GPU runs\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d7ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PipelinedCorefProcessor:\n",
    "    \"\"\"\n",
    "    Pipeline tokenization and inference:\n",
    "    - Thread 1: Tokenizes batch N+1\n",
    "    - Main: GPU inference on batch N\n",
    "    - Thread pool: Post-processes batch N-1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, device: str = \"cuda:0\", num_workers: int = 16):\n",
    "        from fastcoref import FCoref\n",
    "        \n",
    "        print(\"Loading FastCoref model...\")\n",
    "        self.model = FCoref(device=device)\n",
    "        print(\"Model loaded!\")\n",
    "        \n",
    "        # Access internal components for manual tokenization\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        self.nlp_model = self.model.model\n",
    "        \n",
    "        self.num_workers = num_workers\n",
    "        self.device = device\n",
    "        \n",
    "    def _tokenize_batch(self, texts: List[str]) -> dict:\n",
    "        \"\"\"Tokenize a batch of texts (CPU-bound)\"\"\"\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        # Filter empty texts\n",
    "        valid_texts = [t if t and len(t.strip()) > 10 else \"\" for t in texts]\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = Dataset.from_dict({\"text\": valid_texts})\n",
    "        \n",
    "        # Tokenize with multiple workers\n",
    "        def tokenize_fn(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.model.max_tokens,\n",
    "                return_tensors=None  # Return lists, not tensors\n",
    "            )\n",
    "        \n",
    "        tokenized = dataset.map(\n",
    "            tokenize_fn,\n",
    "            batched=True,\n",
    "            batch_size=256,\n",
    "            num_proc=self.num_workers,  # Parallel tokenization!\n",
    "            remove_columns=[\"text\"],\n",
    "            desc=\"Tokenizing\"\n",
    "        )\n",
    "        \n",
    "        return tokenized\n",
    "    \n",
    "    def _run_inference(self, tokenized_data) -> List:\n",
    "        \"\"\"Run GPU inference on tokenized data\"\"\"\n",
    "        import torch\n",
    "        \n",
    "        # Convert to tensors and move to GPU\n",
    "        input_ids = torch.tensor(tokenized_data[\"input_ids\"]).to(self.device)\n",
    "        attention_mask = torch.tensor(tokenized_data[\"attention_mask\"]).to(self.device)\n",
    "        \n",
    "        # Run model\n",
    "        with torch.no_grad():\n",
    "            outputs = self.nlp_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict_pipelined(\n",
    "        self,\n",
    "        batches: List[List[str]],\n",
    "        thread_pool: ThreadPoolExecutor\n",
    "    ) -> List[List]:\n",
    "        \"\"\"\n",
    "        Process multiple batches with pipelined tokenization.\n",
    "        Returns list of cluster predictions per batch.\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        \n",
    "        all_predictions = []\n",
    "        tokenize_future: Optional[Future] = None\n",
    "        next_tokenized = None\n",
    "        \n",
    "        for batch_idx, texts in enumerate(batches):\n",
    "            # Start tokenizing NEXT batch in background\n",
    "            if batch_idx + 1 < len(batches):\n",
    "                next_texts = batches[batch_idx + 1]\n",
    "                tokenize_future = thread_pool.submit(self._tokenize_batch, next_texts)\n",
    "            \n",
    "            # Get tokenized data for current batch\n",
    "            if batch_idx == 0:\n",
    "                # First batch - must tokenize synchronously\n",
    "                tokenized = self._tokenize_batch(texts)\n",
    "            else:\n",
    "                # Use pre-tokenized data from previous iteration\n",
    "                tokenized = next_tokenized\n",
    "            \n",
    "            # GPU inference (while next batch tokenizes in background)\n",
    "            preds = self.model.predict(texts=texts, is_split_into_words=False)\n",
    "            \n",
    "            # Extract clusters\n",
    "            batch_clusters = []\n",
    "            for pred in preds:\n",
    "                try:\n",
    "                    clusters = pred.get_clusters(as_strings=False)\n",
    "                    batch_clusters.append(clusters)\n",
    "                except:\n",
    "                    batch_clusters.append([])\n",
    "            \n",
    "            all_predictions.append(batch_clusters)\n",
    "            \n",
    "            # Wait for next batch's tokenization\n",
    "            if tokenize_future is not None:\n",
    "                next_tokenized = tokenize_future.result()\n",
    "                tokenize_future = None\n",
    "        \n",
    "        return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c43bed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_pipelined(\n",
    "    parquet_file: Path,\n",
    "    file_idx: int,\n",
    "    output_path: str,\n",
    "    processor: PipelinedCorefProcessor,\n",
    "    thread_pool: ThreadPoolExecutor,\n",
    "    batch_size: int = 1500\n",
    ") -> Tuple[int, int]:\n",
    "    \"\"\"Process a single file with pipelined tokenization\"\"\"\n",
    "    \n",
    "    lf = pl.scan_parquet(parquet_file, hive_partitioning=True)\n",
    "    file_rows = lf.select(pl.len()).collect().item()\n",
    "    \n",
    "    lf = lf.select([\n",
    "        'doc_id', 'extracted_text', 'uri', 'host', 'http_date',\n",
    "        'year', 'month', 'day'\n",
    "    ])\n",
    "    \n",
    "    # Load ALL batches for this file (they're small enough)\n",
    "    all_batch_data = []\n",
    "    for batch_start in range(0, file_rows, batch_size):\n",
    "        batch_df = lf.slice(batch_start, batch_size).collect()\n",
    "        if not batch_df.is_empty():\n",
    "            all_batch_data.append(batch_df)\n",
    "    \n",
    "    if not all_batch_data:\n",
    "        return 0, 0\n",
    "    \n",
    "    # Extract texts for pipelined prediction\n",
    "    all_texts = [batch['extracted_text'].to_list() for batch in all_batch_data]\n",
    "    \n",
    "    # Run pipelined prediction - tokenization overlaps with GPU\n",
    "    # Note: For simplicity, we'll use the model's predict but with parallel tokenization\n",
    "    all_chunks = []\n",
    "    total_docs = 0\n",
    "    \n",
    "    meta_cols = ['uri', 'host', 'http_date', 'year', 'month', 'day']\n",
    "    \n",
    "    for batch_df, texts in zip(all_batch_data, all_texts):\n",
    "        doc_ids = batch_df['doc_id'].to_list()\n",
    "        meta_data = {col: batch_df[col].to_list() for col in meta_cols if col in batch_df.columns}\n",
    "        \n",
    "        # Get predictions\n",
    "        valid_indices = [i for i, t in enumerate(texts) if t and len(str(t).strip()) > 10]\n",
    "        valid_texts = [texts[i] for i in valid_indices]\n",
    "        \n",
    "        clusters_map = {}\n",
    "        if valid_texts:\n",
    "            try:\n",
    "                preds = processor.model.predict(texts=valid_texts, is_split_into_words=False)\n",
    "                for idx, pred in zip(valid_indices, preds):\n",
    "                    try:\n",
    "                        clusters_map[idx] = pred.get_clusters(as_strings=False)\n",
    "                    except:\n",
    "                        clusters_map[idx] = []\n",
    "            except Exception as e:\n",
    "                print(f\"Inference error: {e}\")\n",
    "        \n",
    "        # Parallel post-processing\n",
    "        process_args = []\n",
    "        for i, (doc_id, text) in enumerate(zip(doc_ids, texts)):\n",
    "            metadata = {col: meta_data[col][i] for col in meta_cols if col in meta_data}\n",
    "            clusters = clusters_map.get(i, [])\n",
    "            process_args.append((doc_id, text, clusters, metadata))\n",
    "        \n",
    "        # Process docs in parallel\n",
    "        futures = [thread_pool.submit(process_single_doc, args) for args in process_args]\n",
    "        for future in futures:\n",
    "            rows = future.result()\n",
    "            if rows:\n",
    "                all_chunks.extend(rows)\n",
    "        \n",
    "        total_docs += len(batch_df)\n",
    "        del batch_df\n",
    "    \n",
    "    # Write output\n",
    "    if all_chunks:\n",
    "        result_df = pl.DataFrame(all_chunks)\n",
    "        \n",
    "        if 'year' in result_df.columns and 'month' in result_df.columns:\n",
    "            year = result_df['year'][0]\n",
    "            month = result_df['month'][0]\n",
    "            out_file = Path(output_path) / f\"year={year}\" / f\"month={month}\" / f\"chunks_{file_idx:05d}.parquet\"\n",
    "        else:\n",
    "            out_file = Path(output_path) / f\"chunks_{file_idx:05d}.parquet\"\n",
    "        \n",
    "        out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        result_df.write_parquet(out_file, compression='zstd', compression_level=3)\n",
    "        \n",
    "        return total_docs, len(result_df)\n",
    "    \n",
    "    return total_docs, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be43d58",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# MAIN PROCESSING - Using ThreadPoolExecutor (safer with CUDA)\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7904881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_threaded(\n",
    "    batch_df: pl.DataFrame, \n",
    "    coref_model,\n",
    "    thread_pool: ThreadPoolExecutor\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Process batch: GPU inference in main thread, CPU work in thread pool.\n",
    "    ThreadPoolExecutor is safe with CUDA (unlike ProcessPoolExecutor).\n",
    "    \"\"\"\n",
    "    \n",
    "    texts = batch_df['extracted_text'].to_list()\n",
    "    doc_ids = batch_df['doc_id'].to_list()\n",
    "    \n",
    "    meta_cols = ['uri', 'host', 'http_date', 'year', 'month', 'day']\n",
    "    meta_data = {col: batch_df[col].to_list() for col in meta_cols if col in batch_df.columns}\n",
    "    \n",
    "    # --- GPU BATCH INFERENCE (main thread) ---\n",
    "    valid_indices = [i for i, t in enumerate(texts) if t and len(t.strip()) > 10]\n",
    "    valid_texts = [texts[i] for i in valid_indices]\n",
    "    \n",
    "    clusters_map = {}\n",
    "    if valid_texts:\n",
    "        try:\n",
    "            preds = coref_model.predict(texts=valid_texts, is_split_into_words=False)\n",
    "            for idx, pred in zip(valid_indices, preds):\n",
    "                try:\n",
    "                    clusters_map[idx] = pred.get_clusters(as_strings=False)\n",
    "                except:\n",
    "                    clusters_map[idx] = []\n",
    "        except Exception as e:\n",
    "            print(f\"GPU inference error: {e}\")\n",
    "    \n",
    "    # --- PARALLEL CPU PROCESSING (thread pool) ---\n",
    "    process_args = []\n",
    "    for i, (doc_id, text) in enumerate(zip(doc_ids, texts)):\n",
    "        metadata = {col: meta_data[col][i] for col in meta_cols if col in meta_data}\n",
    "        clusters = clusters_map.get(i, [])\n",
    "        process_args.append((doc_id, text, clusters, metadata))\n",
    "    \n",
    "    # Thread pool for CPU-bound work (releases GIL during string ops)\n",
    "    all_rows = []\n",
    "    futures = [thread_pool.submit(process_single_doc, args) for args in process_args]\n",
    "    for future in futures:\n",
    "        rows = future.result()\n",
    "        all_rows.extend(rows)\n",
    "    \n",
    "    if not all_rows:\n",
    "        return pl.DataFrame()\n",
    "    \n",
    "    return pl.DataFrame(all_rows)\n",
    "\n",
    "\n",
    "def process_streaming_production(\n",
    "    input_path: str, \n",
    "    output_path: str, \n",
    "    batch_size: int = 1500\n",
    "):\n",
    "    \"\"\"\n",
    "    Production-ready streaming processor for 2.5M+ documents.\n",
    "    Features: checkpointing, memory-safe, progress tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading FastCoref model...\")\n",
    "    from fastcoref import FCoref\n",
    "    coref_model = FCoref(device=DEVICE)\n",
    "    print(\"Model loaded!\")\n",
    "    \n",
    "    # Get all input files\n",
    "    input_files = sorted(Path(input_path).rglob(\"*.parquet\"))\n",
    "    print(f\"Found {len(input_files)} parquet files\")\n",
    "    \n",
    "    # Load checkpoint\n",
    "    completed_files = load_checkpoint()\n",
    "    if completed_files:\n",
    "        print(f\"Resuming from checkpoint: {len(completed_files)} files already done\")\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_docs = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Thread pool for CPU work (threads are safe with CUDA)\n",
    "    with ThreadPoolExecutor(max_workers=NUM_CPU_WORKERS) as thread_pool:\n",
    "        \n",
    "        pbar = tqdm(enumerate(input_files), total=len(input_files), desc=\"Files\")\n",
    "        \n",
    "        for file_idx, parquet_file in pbar:\n",
    "            # Skip completed files\n",
    "            if file_idx in completed_files:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Lazy scan\n",
    "                lf = pl.scan_parquet(parquet_file, hive_partitioning=True)\n",
    "                file_rows = lf.select(pl.len()).collect().item()\n",
    "                \n",
    "                lf = lf.select([\n",
    "                    'doc_id', 'extracted_text', 'uri', 'host', 'http_date',\n",
    "                    'year', 'month', 'day'\n",
    "                ])\n",
    "                \n",
    "                file_chunks = []\n",
    "                file_docs = 0\n",
    "                \n",
    "                # Process in batches\n",
    "                for batch_start in range(0, file_rows, batch_size):\n",
    "                    batch_df = lf.slice(batch_start, batch_size).collect()\n",
    "                    \n",
    "                    if batch_df.is_empty():\n",
    "                        continue\n",
    "                    \n",
    "                    chunks_df = process_batch_threaded(batch_df, coref_model, thread_pool)\n",
    "                    \n",
    "                    if not chunks_df.is_empty():\n",
    "                        file_chunks.append(chunks_df)\n",
    "                    \n",
    "                    file_docs += len(batch_df)\n",
    "                    del batch_df\n",
    "                \n",
    "                # Write this file's output\n",
    "                if file_chunks:\n",
    "                    result_df = pl.concat(file_chunks)\n",
    "                    \n",
    "                    if 'year' in result_df.columns and 'month' in result_df.columns:\n",
    "                        year = result_df['year'][0]\n",
    "                        month = result_df['month'][0]\n",
    "                        out_file = Path(output_path) / f\"year={year}\" / f\"month={month}\" / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    else:\n",
    "                        out_file = Path(output_path) / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    \n",
    "                    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    result_df.write_parquet(out_file, compression='zstd', compression_level=3)\n",
    "                    \n",
    "                    total_chunks += len(result_df)\n",
    "                    del file_chunks, result_df\n",
    "                \n",
    "                total_docs += file_docs\n",
    "                \n",
    "                # Update checkpoint\n",
    "                completed_files.add(file_idx)\n",
    "                if file_idx % 5 == 0:  # Save checkpoint every 5 files\n",
    "                    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Update progress bar\n",
    "                elapsed = time.time() - start_time\n",
    "                docs_per_sec = total_docs / elapsed if elapsed > 0 else 0\n",
    "                pbar.set_postfix({\n",
    "                    'docs': f'{total_docs:,}',\n",
    "                    'chunks': f'{total_chunks:,}',\n",
    "                    'docs/s': f'{docs_per_sec:.1f}'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing {parquet_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # Final checkpoint\n",
    "    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE!\")\n",
    "    print(f\"Documents: {total_docs:,}\")\n",
    "    print(f\"Chunks: {total_chunks:,}\")\n",
    "    print(f\"Time: {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Speed: {total_docs/elapsed:.1f} docs/sec\")\n",
    "    print(f\"Output: {output_path}\")\n",
    "    print(f\"{'='*60}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a55e3",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# MAIN - Using parallel tokenization\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838ee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_streaming_fast(\n",
    "    input_path: str,\n",
    "    output_path: str,\n",
    "    batch_size: int = 1500\n",
    "):\n",
    "    \"\"\"\n",
    "    Fast streaming with parallel tokenization.\n",
    "    Key optimization: Use num_proc in HuggingFace tokenization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Patch fastcoref to use parallel tokenization\n",
    "    import os\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    \n",
    "    from fastcoref import FCoref\n",
    "    \n",
    "    print(\"Loading FastCoref model...\")\n",
    "    coref_model = FCoref(device=DEVICE)\n",
    "    print(\"Model loaded!\")\n",
    "    \n",
    "    # Monkey-patch the model's predict to use more workers\n",
    "    original_predict = coref_model.predict\n",
    "    \n",
    "    def patched_predict(texts, **kwargs):\n",
    "        # Set environment for parallel tokenization\n",
    "        import datasets\n",
    "        datasets.disable_caching()  # Avoid disk I/O\n",
    "        return original_predict(texts, **kwargs)\n",
    "    \n",
    "    coref_model.predict = patched_predict\n",
    "    \n",
    "    input_files = sorted(Path(input_path).rglob(\"*.parquet\"))\n",
    "    print(f\"Found {len(input_files)} parquet files\")\n",
    "    \n",
    "    completed_files = load_checkpoint()\n",
    "    if completed_files:\n",
    "        print(f\"Resuming: {len(completed_files)} files done\")\n",
    "    \n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    total_chunks = 0\n",
    "    total_docs = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pre-load next file while processing current (I/O overlap)\n",
    "    prefetch_executor = ThreadPoolExecutor(max_workers=2)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=NUM_CPU_WORKERS) as thread_pool:\n",
    "        pbar = tqdm(enumerate(input_files), total=len(input_files), desc=\"Files\")\n",
    "        \n",
    "        # Prefetch first file\n",
    "        prefetch_future = None\n",
    "        \n",
    "        for file_idx, parquet_file in pbar:\n",
    "            if file_idx in completed_files:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Start prefetching next file\n",
    "                if file_idx + 1 < len(input_files) and file_idx + 1 not in completed_files:\n",
    "                    next_file = input_files[file_idx + 1]\n",
    "                    prefetch_future = prefetch_executor.submit(\n",
    "                        lambda f: pl.scan_parquet(f, hive_partitioning=True).collect(),\n",
    "                        next_file\n",
    "                    )\n",
    "                \n",
    "                # Process current file\n",
    "                lf = pl.scan_parquet(parquet_file, hive_partitioning=True)\n",
    "                file_rows = lf.select(pl.len()).collect().item()\n",
    "                \n",
    "                lf = lf.select([\n",
    "                    'doc_id', 'extracted_text', 'uri', 'host', 'http_date',\n",
    "                    'year', 'month', 'day'\n",
    "                ])\n",
    "                \n",
    "                file_chunks = []\n",
    "                file_docs = 0\n",
    "                meta_cols = ['uri', 'host', 'http_date', 'year', 'month', 'day']\n",
    "                \n",
    "                for batch_start in range(0, file_rows, batch_size):\n",
    "                    batch_df = lf.slice(batch_start, batch_size).collect()\n",
    "                    \n",
    "                    if batch_df.is_empty():\n",
    "                        continue\n",
    "                    \n",
    "                    texts = batch_df['extracted_text'].to_list()\n",
    "                    doc_ids = batch_df['doc_id'].to_list()\n",
    "                    meta_data = {col: batch_df[col].to_list() for col in meta_cols if col in batch_df.columns}\n",
    "                    \n",
    "                    # GPU inference\n",
    "                    valid_indices = [i for i, t in enumerate(texts) if t and len(str(t).strip()) > 10]\n",
    "                    valid_texts = [texts[i] for i in valid_indices]\n",
    "                    \n",
    "                    clusters_map = {}\n",
    "                    if valid_texts:\n",
    "                        try:\n",
    "                            preds = coref_model.predict(texts=valid_texts, is_split_into_words=False)\n",
    "                            for idx, pred in zip(valid_indices, preds):\n",
    "                                try:\n",
    "                                    clusters_map[idx] = pred.get_clusters(as_strings=False)\n",
    "                                except:\n",
    "                                    clusters_map[idx] = []\n",
    "                        except Exception as e:\n",
    "                            print(f\"GPU error: {e}\")\n",
    "                    \n",
    "                    # Parallel CPU post-processing\n",
    "                    process_args = []\n",
    "                    for i, (doc_id, text) in enumerate(zip(doc_ids, texts)):\n",
    "                        metadata = {col: meta_data[col][i] for col in meta_cols if col in meta_data}\n",
    "                        clusters = clusters_map.get(i, [])\n",
    "                        process_args.append((doc_id, text, clusters, metadata))\n",
    "                    \n",
    "                    batch_rows = []\n",
    "                    futures = [thread_pool.submit(process_single_doc, args) for args in process_args]\n",
    "                    for future in futures:\n",
    "                        batch_rows.extend(future.result())\n",
    "                    \n",
    "                    if batch_rows:\n",
    "                        file_chunks.append(pl.DataFrame(batch_rows))\n",
    "                    \n",
    "                    file_docs += len(batch_df)\n",
    "                    del batch_df\n",
    "                \n",
    "                # Write output\n",
    "                if file_chunks:\n",
    "                    result_df = pl.concat(file_chunks)\n",
    "                    \n",
    "                    if 'year' in result_df.columns and 'month' in result_df.columns:\n",
    "                        year = result_df['year'][0]\n",
    "                        month = result_df['month'][0]\n",
    "                        out_file = Path(output_path) / f\"year={year}\" / f\"month={month}\" / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    else:\n",
    "                        out_file = Path(output_path) / f\"chunks_{file_idx:05d}.parquet\"\n",
    "                    \n",
    "                    out_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    result_df.write_parquet(out_file, compression='zstd', compression_level=3)\n",
    "                    \n",
    "                    total_chunks += len(result_df)\n",
    "                    del file_chunks, result_df\n",
    "                \n",
    "                total_docs += file_docs\n",
    "                completed_files.add(file_idx)\n",
    "                \n",
    "                if file_idx % 5 == 0:\n",
    "                    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "                    gc.collect()\n",
    "                \n",
    "                elapsed = time.time() - start_time\n",
    "                docs_per_sec = total_docs / elapsed if elapsed > 0 else 0\n",
    "                eta_hours = (len(input_files) - file_idx) * (elapsed / (file_idx + 1)) / 3600\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'docs': f'{total_docs:,}',\n",
    "                    'chunks': f'{total_chunks:,}',\n",
    "                    'docs/s': f'{docs_per_sec:.1f}',\n",
    "                    'ETA': f'{eta_hours:.1f}h'\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError on {parquet_file}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    prefetch_executor.shutdown(wait=False)\n",
    "    save_checkpoint(completed_files, total_docs, total_chunks)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"COMPLETE!\")\n",
    "    print(f\"Documents: {total_docs:,}\")\n",
    "    print(f\"Chunks: {total_chunks:,}\")\n",
    "    print(f\"Time: {elapsed/3600:.2f} hours\")\n",
    "    print(f\"Speed: {total_docs/elapsed:.1f} docs/sec\")\n",
    "    print(f\"{'='*60}\")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17785863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastCoref model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2026 12:31:22 - INFO - \t missing_keys: []\n",
      "01/20/2026 12:31:22 - INFO - \t unexpected_keys: []\n",
      "01/20/2026 12:31:22 - INFO - \t mismatched_keys: []\n",
      "01/20/2026 12:31:22 - INFO - \t error_msgs: []\n",
      "01/20/2026 12:31:22 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Found 4409 parquet files\n",
      "Resuming from checkpoint: 36 files already done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/4409 [00:00<?, ?it/s]/tmp/ipykernel_3999543/2805755274.py:96: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  file_rows = lf.select(pl.count()).collect().item()\n",
      "01/20/2026 12:31:22 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:39<00:00, 37.59 examples/s]\n",
      "01/20/2026 12:32:02 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 133.78it/s]\n",
      "01/20/2026 12:32:14 - INFO - \t Tokenize 201 inputs...\n",
      "Map: 100%|██████████| 201/201 [00:04<00:00, 43.38 examples/s]\n",
      "01/20/2026 12:32:19 - INFO - \t ***** Running Inference on 201 texts *****\n",
      "Inference: 100%|██████████| 201/201 [00:01<00:00, 145.39it/s]\n",
      "Files:   1%|          | 37/4409 [00:58<1:54:25,  1.57s/it, docs=1,701, chunks=3,457, docs/s=29.3]01/20/2026 12:32:20 - INFO - \t Tokenize 1373 inputs...\n",
      "Map: 100%|██████████| 1373/1373 [00:30<00:00, 45.33 examples/s]\n",
      "01/20/2026 12:32:51 - INFO - \t ***** Running Inference on 1373 texts *****\n",
      "Inference: 100%|██████████| 1373/1373 [00:08<00:00, 160.02it/s]\n",
      "Files:   1%|          | 38/4409 [01:37<3:37:39,  2.99s/it, docs=3,074, chunks=6,088, docs/s=31.4]01/20/2026 12:33:00 - INFO - \t Tokenize 1335 inputs...\n",
      "Map: 100%|██████████| 1335/1335 [00:32<00:00, 40.67 examples/s]\n",
      "01/20/2026 12:33:33 - INFO - \t ***** Running Inference on 1335 texts *****\n",
      "Inference: 100%|██████████| 1335/1335 [00:09<00:00, 145.73it/s]\n",
      "Files:   1%|          | 39/4409 [02:20<6:04:37,  5.01s/it, docs=4,409, chunks=8,738, docs/s=31.3]01/20/2026 12:33:43 - INFO - \t Tokenize 1477 inputs...\n",
      "Map: 100%|██████████| 1477/1477 [00:28<00:00, 52.63 examples/s]\n",
      "01/20/2026 12:34:11 - INFO - \t ***** Running Inference on 1477 texts *****\n",
      "Inference: 100%|██████████| 1477/1477 [00:08<00:00, 176.70it/s]\n",
      "Files:   1%|          | 40/4409 [02:58<8:42:37,  7.18s/it, docs=5,886, chunks=11,308, docs/s=33.0]01/20/2026 12:34:21 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:35<00:00, 42.22 examples/s]\n",
      "01/20/2026 12:34:56 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 151.36it/s]\n",
      "01/20/2026 12:35:07 - INFO - \t Tokenize 34 inputs...\n",
      "Map: 100%|██████████| 34/34 [00:00<00:00, 40.46 examples/s]\n",
      "01/20/2026 12:35:08 - INFO - \t ***** Running Inference on 34 texts *****\n",
      "Inference: 100%|██████████| 34/34 [00:00<00:00, 132.51it/s]\n",
      "Files:   1%|          | 41/4409 [03:45<13:01:37, 10.74s/it, docs=7,420, chunks=14,264, docs/s=32.8]01/20/2026 12:35:08 - INFO - \t Tokenize 717 inputs...\n",
      "Map: 100%|██████████| 717/717 [00:21<00:00, 33.13 examples/s]\n",
      "01/20/2026 12:35:30 - INFO - \t ***** Running Inference on 717 texts *****\n",
      "Inference: 100%|██████████| 717/717 [00:05<00:00, 126.20it/s]\n",
      "Files:   1%|          | 42/4409 [04:13<15:19:39, 12.64s/it, docs=8,137, chunks=15,788, docs/s=32.1]01/20/2026 12:35:36 - INFO - \t Tokenize 1277 inputs...\n",
      "Map: 100%|██████████| 1277/1277 [00:36<00:00, 35.06 examples/s]\n",
      "01/20/2026 12:36:13 - INFO - \t ***** Running Inference on 1277 texts *****\n",
      "Inference: 100%|██████████| 1277/1277 [00:10<00:00, 127.44it/s]\n",
      "Files:   1%|          | 43/4409 [05:00<21:03:37, 17.37s/it, docs=9,414, chunks=18,525, docs/s=31.3]01/20/2026 12:36:23 - INFO - \t Tokenize 1494 inputs...\n",
      "Map: 100%|██████████| 1494/1494 [00:33<00:00, 44.36 examples/s]\n",
      "01/20/2026 12:36:57 - INFO - \t ***** Running Inference on 1494 texts *****\n",
      "Inference: 100%|██████████| 1494/1494 [00:09<00:00, 158.62it/s]\n",
      "Files:   1%|          | 44/4409 [05:45<26:23:34, 21.77s/it, docs=10,908, chunks=21,500, docs/s=31.6]01/20/2026 12:37:08 - INFO - \t Tokenize 944 inputs...\n",
      "Map: 100%|██████████| 944/944 [00:22<00:00, 42.41 examples/s]\n",
      "01/20/2026 12:37:30 - INFO - \t ***** Running Inference on 944 texts *****\n",
      "Inference: 100%|██████████| 944/944 [00:06<00:00, 148.87it/s]\n",
      "Files:   1%|          | 45/4409 [06:14<28:05:37, 23.18s/it, docs=11,852, chunks=23,246, docs/s=31.7]01/20/2026 12:37:37 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:44<00:00, 33.71 examples/s]\n",
      "01/20/2026 12:38:21 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:12<00:00, 119.94it/s]\n",
      "01/20/2026 12:38:35 - INFO - \t Tokenize 209 inputs...\n",
      "Map: 100%|██████████| 209/209 [00:06<00:00, 33.94 examples/s]\n",
      "01/20/2026 12:38:41 - INFO - \t ***** Running Inference on 209 texts *****\n",
      "Inference: 100%|██████████| 209/209 [00:01<00:00, 115.11it/s]\n",
      "Files:   1%|          | 46/4409 [07:21<39:21:22, 32.47s/it, docs=13,561, chunks=27,112, docs/s=30.7]01/20/2026 12:38:44 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:31<00:00, 47.36 examples/s]\n",
      "01/20/2026 12:39:15 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 160.14it/s]\n",
      "01/20/2026 12:39:25 - INFO - \t Tokenize 64 inputs...\n",
      "Map: 100%|██████████| 64/64 [00:00<00:00, 71.17 examples/s]\n",
      "01/20/2026 12:39:26 - INFO - \t ***** Running Inference on 64 texts *****\n",
      "Inference: 100%|██████████| 64/64 [00:00<00:00, 139.41it/s]\n",
      "Files:   1%|          | 47/4409 [08:04<42:26:04, 35.02s/it, docs=15,125, chunks=30,038, docs/s=31.2]01/20/2026 12:39:27 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:33<00:00, 44.96 examples/s]\n",
      "01/20/2026 12:40:00 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 154.65it/s]\n",
      "01/20/2026 12:40:11 - INFO - \t Tokenize 142 inputs...\n",
      "Map: 100%|██████████| 142/142 [00:03<00:00, 36.83 examples/s]\n",
      "01/20/2026 12:40:15 - INFO - \t ***** Running Inference on 142 texts *****\n",
      "Inference: 100%|██████████| 142/142 [00:01<00:00, 136.50it/s]\n",
      "Files:   1%|          | 48/4409 [08:53<46:39:52, 38.52s/it, docs=16,767, chunks=33,205, docs/s=31.4]01/20/2026 12:40:16 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.71 examples/s]\n",
      "01/20/2026 12:40:50 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 150.08it/s]\n",
      "01/20/2026 12:41:01 - INFO - \t Tokenize 24 inputs...\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 41.64 examples/s]\n",
      "01/20/2026 12:41:02 - INFO - \t ***** Running Inference on 24 texts *****\n",
      "Inference: 100%|██████████| 24/24 [00:00<00:00, 98.78it/s] \n",
      "Files:   1%|          | 49/4409 [09:39<49:05:36, 40.54s/it, docs=18,291, chunks=36,114, docs/s=31.6]01/20/2026 12:41:02 - INFO - \t Tokenize 1207 inputs...\n",
      "Map: 100%|██████████| 1207/1207 [00:31<00:00, 38.19 examples/s]\n",
      "01/20/2026 12:41:34 - INFO - \t ***** Running Inference on 1207 texts *****\n",
      "Inference: 100%|██████████| 1207/1207 [00:09<00:00, 132.28it/s]\n",
      "Files:   1%|          | 50/4409 [10:21<49:24:08, 40.80s/it, docs=19,498, chunks=38,689, docs/s=31.4]01/20/2026 12:41:44 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 40.89 examples/s]\n",
      "01/20/2026 12:42:20 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 141.36it/s]\n",
      "01/20/2026 12:42:32 - INFO - \t Tokenize 79 inputs...\n",
      "Map: 100%|██████████| 79/79 [00:01<00:00, 39.82 examples/s]\n",
      "01/20/2026 12:42:34 - INFO - \t ***** Running Inference on 79 texts *****\n",
      "Inference: 100%|██████████| 79/79 [00:00<00:00, 138.68it/s]\n",
      "Files:   1%|          | 51/4409 [11:12<53:01:43, 43.81s/it, docs=21,077, chunks=41,739, docs/s=31.3]01/20/2026 12:42:35 - INFO - \t Tokenize 1012 inputs...\n",
      "Map: 100%|██████████| 1012/1012 [00:23<00:00, 42.23 examples/s]\n",
      "01/20/2026 12:42:59 - INFO - \t ***** Running Inference on 1012 texts *****\n",
      "Inference: 100%|██████████| 1012/1012 [00:06<00:00, 147.10it/s]\n",
      "Files:   1%|          | 52/4409 [11:44<48:45:25, 40.29s/it, docs=22,089, chunks=43,707, docs/s=31.4]01/20/2026 12:43:07 - INFO - \t Tokenize 1285 inputs...\n",
      "Map: 100%|██████████| 1285/1285 [00:27<00:00, 46.11 examples/s]\n",
      "01/20/2026 12:43:35 - INFO - \t ***** Running Inference on 1285 texts *****\n",
      "Inference: 100%|██████████| 1285/1285 [00:08<00:00, 160.07it/s]\n",
      "Files:   1%|          | 53/4409 [12:20<47:27:45, 39.23s/it, docs=23,374, chunks=46,091, docs/s=31.5]01/20/2026 12:43:43 - INFO - \t Tokenize 1281 inputs...\n",
      "Map: 100%|██████████| 1281/1281 [00:31<00:00, 40.98 examples/s]\n",
      "01/20/2026 12:44:15 - INFO - \t ***** Running Inference on 1281 texts *****\n",
      "Inference: 100%|██████████| 1281/1281 [00:09<00:00, 140.35it/s]\n",
      "Files:   1%|          | 54/4409 [13:02<48:08:59, 39.80s/it, docs=24,655, chunks=48,644, docs/s=31.5]01/20/2026 12:44:24 - INFO - \t Tokenize 1127 inputs...\n",
      "Map: 100%|██████████| 1127/1127 [00:32<00:00, 34.67 examples/s]\n",
      "01/20/2026 12:44:57 - INFO - \t ***** Running Inference on 1127 texts *****\n",
      "Inference: 100%|██████████| 1127/1127 [00:08<00:00, 129.96it/s]\n",
      "Files:   1%|          | 55/4409 [13:44<48:55:17, 40.45s/it, docs=25,782, chunks=51,023, docs/s=31.3]01/20/2026 12:45:06 - INFO - \t Tokenize 878 inputs...\n",
      "Map: 100%|██████████| 878/878 [00:28<00:00, 30.50 examples/s]\n",
      "01/20/2026 12:45:35 - INFO - \t ***** Running Inference on 878 texts *****\n",
      "Inference: 100%|██████████| 878/878 [00:07<00:00, 117.31it/s]\n",
      "Files:   1%|▏         | 56/4409 [14:21<47:45:14, 39.49s/it, docs=26,660, chunks=52,994, docs/s=31.0]01/20/2026 12:45:44 - INFO - \t Tokenize 1300 inputs...\n",
      "Map: 100%|██████████| 1300/1300 [00:28<00:00, 45.62 examples/s]\n",
      "01/20/2026 12:46:12 - INFO - \t ***** Running Inference on 1300 texts *****\n",
      "Inference: 100%|██████████| 1300/1300 [00:08<00:00, 156.31it/s]\n",
      "Files:   1%|▏         | 57/4409 [14:58<47:06:20, 38.97s/it, docs=27,960, chunks=55,437, docs/s=31.1]01/20/2026 12:46:21 - INFO - \t Tokenize 1308 inputs...\n",
      "Map: 100%|██████████| 1308/1308 [00:44<00:00, 29.69 examples/s]\n",
      "01/20/2026 12:47:06 - INFO - \t ***** Running Inference on 1308 texts *****\n",
      "Inference: 100%|██████████| 1308/1308 [00:11<00:00, 110.39it/s]\n",
      "Files:   1%|▏         | 58/4409 [15:55<53:31:42, 44.29s/it, docs=29,268, chunks=58,225, docs/s=30.6]01/20/2026 12:47:18 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:39<00:00, 38.01 examples/s]\n",
      "01/20/2026 12:47:58 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 128.70it/s]\n",
      "01/20/2026 12:48:10 - INFO - \t Tokenize 62 inputs...\n",
      "Map: 100%|██████████| 62/62 [00:01<00:00, 38.53 examples/s]\n",
      "01/20/2026 12:48:12 - INFO - \t ***** Running Inference on 62 texts *****\n",
      "Inference: 100%|██████████| 62/62 [00:00<00:00, 88.62it/s]\n",
      "Files:   1%|▏         | 59/4409 [16:50<57:18:06, 47.42s/it, docs=30,830, chunks=61,485, docs/s=30.5]01/20/2026 12:48:13 - INFO - \t Tokenize 1256 inputs...\n",
      "Map: 100%|██████████| 1256/1256 [00:27<00:00, 45.20 examples/s]\n",
      "01/20/2026 12:48:41 - INFO - \t ***** Running Inference on 1256 texts *****\n",
      "Inference: 100%|██████████| 1256/1256 [00:08<00:00, 153.90it/s]\n",
      "Files:   1%|▏         | 60/4409 [17:27<53:26:44, 44.24s/it, docs=32,086, chunks=63,851, docs/s=30.6]01/20/2026 12:48:50 - INFO - \t Tokenize 944 inputs...\n",
      "Map: 100%|██████████| 944/944 [00:24<00:00, 39.00 examples/s]\n",
      "01/20/2026 12:49:14 - INFO - \t ***** Running Inference on 944 texts *****\n",
      "Inference: 100%|██████████| 944/944 [00:06<00:00, 137.32it/s]\n",
      "Files:   1%|▏         | 61/4409 [17:59<49:00:46, 40.58s/it, docs=33,030, chunks=65,858, docs/s=30.6]01/20/2026 12:49:22 - INFO - \t Tokenize 1079 inputs...\n",
      "Map: 100%|██████████| 1079/1079 [00:26<00:00, 40.89 examples/s]\n",
      "01/20/2026 12:49:48 - INFO - \t ***** Running Inference on 1079 texts *****\n",
      "Inference: 100%|██████████| 1079/1079 [00:07<00:00, 145.52it/s]\n",
      "Files:   1%|▏         | 62/4409 [18:34<46:53:21, 38.83s/it, docs=34,109, chunks=67,996, docs/s=30.6]01/20/2026 12:49:57 - INFO - \t Tokenize 946 inputs...\n",
      "Map: 100%|██████████| 946/946 [00:15<00:00, 62.05 examples/s]\n",
      "01/20/2026 12:50:12 - INFO - \t ***** Running Inference on 946 texts *****\n",
      "Inference: 100%|██████████| 946/946 [00:04<00:00, 211.49it/s]\n",
      "Files:   1%|▏         | 63/4409 [18:54<40:11:02, 33.29s/it, docs=35,055, chunks=69,486, docs/s=30.9]01/20/2026 12:50:17 - INFO - \t Tokenize 1378 inputs...\n",
      "Map: 100%|██████████| 1378/1378 [00:38<00:00, 35.48 examples/s]\n",
      "01/20/2026 12:50:56 - INFO - \t ***** Running Inference on 1378 texts *****\n",
      "Inference: 100%|██████████| 1378/1378 [00:11<00:00, 124.76it/s]\n",
      "Files:   1%|▏         | 64/4409 [19:45<46:37:22, 38.63s/it, docs=36,433, chunks=72,573, docs/s=30.7]01/20/2026 12:51:08 - INFO - \t Tokenize 682 inputs...\n",
      "Map: 100%|██████████| 682/682 [00:15<00:00, 43.19 examples/s]\n",
      "01/20/2026 12:51:24 - INFO - \t ***** Running Inference on 682 texts *****\n",
      "Inference: 100%|██████████| 682/682 [00:04<00:00, 147.09it/s]\n",
      "Files:   1%|▏         | 65/4409 [20:06<40:15:30, 33.36s/it, docs=37,115, chunks=73,911, docs/s=30.8]01/20/2026 12:51:29 - INFO - \t Tokenize 1162 inputs...\n",
      "Map: 100%|██████████| 1162/1162 [00:28<00:00, 40.42 examples/s]\n",
      "01/20/2026 12:51:58 - INFO - \t ***** Running Inference on 1162 texts *****\n",
      "Inference: 100%|██████████| 1162/1162 [00:08<00:00, 140.80it/s]\n",
      "Files:   1%|▏         | 66/4409 [20:44<41:59:34, 34.81s/it, docs=38,277, chunks=76,283, docs/s=30.7]01/20/2026 12:52:07 - INFO - \t Tokenize 1145 inputs...\n",
      "Map: 100%|██████████| 1145/1145 [00:33<00:00, 33.85 examples/s]\n",
      "01/20/2026 12:52:41 - INFO - \t ***** Running Inference on 1145 texts *****\n",
      "Inference: 100%|██████████| 1145/1145 [00:08<00:00, 127.68it/s]\n",
      "Files:   2%|▏         | 67/4409 [21:28<45:14:53, 37.52s/it, docs=39,422, chunks=78,839, docs/s=30.6]01/20/2026 12:52:51 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 40.54 examples/s]\n",
      "01/20/2026 12:53:28 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 137.81it/s]\n",
      "01/20/2026 12:53:40 - INFO - \t Tokenize 225 inputs...\n",
      "Map: 100%|██████████| 225/225 [00:05<00:00, 37.94 examples/s]\n",
      "01/20/2026 12:53:46 - INFO - \t ***** Running Inference on 225 texts *****\n",
      "Inference: 100%|██████████| 225/225 [00:01<00:00, 128.39it/s]\n",
      "Files:   2%|▏         | 68/4409 [22:25<52:19:49, 43.40s/it, docs=41,147, chunks=82,330, docs/s=30.6]01/20/2026 12:53:48 - INFO - \t Tokenize 937 inputs...\n",
      "Map: 100%|██████████| 937/937 [00:21<00:00, 44.19 examples/s]\n",
      "01/20/2026 12:54:10 - INFO - \t ***** Running Inference on 937 texts *****\n",
      "Inference: 100%|██████████| 937/937 [00:06<00:00, 151.13it/s]\n",
      "Files:   2%|▏         | 69/4409 [22:54<46:50:17, 38.85s/it, docs=42,084, chunks=84,039, docs/s=30.6]01/20/2026 12:54:16 - INFO - \t Tokenize 1001 inputs...\n",
      "Map: 100%|██████████| 1001/1001 [00:25<00:00, 39.67 examples/s]\n",
      "01/20/2026 12:54:42 - INFO - \t ***** Running Inference on 1001 texts *****\n",
      "Inference: 100%|██████████| 1001/1001 [00:07<00:00, 137.46it/s]\n",
      "Files:   2%|▏         | 70/4409 [23:27<44:56:04, 37.28s/it, docs=43,085, chunks=86,138, docs/s=30.6]01/20/2026 12:54:50 - INFO - \t Tokenize 1200 inputs...\n",
      "Map: 100%|██████████| 1200/1200 [00:27<00:00, 43.92 examples/s]\n",
      "01/20/2026 12:55:18 - INFO - \t ***** Running Inference on 1200 texts *****\n",
      "Inference: 100%|██████████| 1200/1200 [00:07<00:00, 155.74it/s]\n",
      "Files:   2%|▏         | 71/4409 [24:04<44:36:03, 37.01s/it, docs=44,285, chunks=88,401, docs/s=30.7]01/20/2026 12:55:26 - INFO - \t Tokenize 1080 inputs...\n",
      "Map: 100%|██████████| 1080/1080 [00:35<00:00, 30.60 examples/s]\n",
      "01/20/2026 12:56:02 - INFO - \t ***** Running Inference on 1080 texts *****\n",
      "Inference: 100%|██████████| 1080/1080 [00:08<00:00, 124.04it/s]\n",
      "Files:   2%|▏         | 72/4409 [24:49<47:29:48, 39.43s/it, docs=45,365, chunks=90,755, docs/s=30.5]01/20/2026 12:56:11 - INFO - \t Tokenize 675 inputs...\n",
      "Map: 100%|██████████| 675/675 [00:16<00:00, 40.20 examples/s]\n",
      "01/20/2026 12:56:29 - INFO - \t ***** Running Inference on 675 texts *****\n",
      "Inference: 100%|██████████| 675/675 [00:04<00:00, 140.28it/s]\n",
      "Files:   2%|▏         | 73/4409 [25:11<41:16:33, 34.27s/it, docs=46,040, chunks=92,137, docs/s=30.5]01/20/2026 12:56:34 - INFO - \t Tokenize 1354 inputs...\n",
      "Map: 100%|██████████| 1354/1354 [00:33<00:00, 40.01 examples/s]\n",
      "01/20/2026 12:57:08 - INFO - \t ***** Running Inference on 1354 texts *****\n",
      "Inference: 100%|██████████| 1354/1354 [00:09<00:00, 137.74it/s]\n",
      "Files:   2%|▏         | 74/4409 [25:55<45:00:57, 37.38s/it, docs=47,394, chunks=94,979, docs/s=30.5]01/20/2026 12:57:18 - INFO - \t Tokenize 902 inputs...\n",
      "Map: 100%|██████████| 902/902 [00:21<00:00, 42.14 examples/s]\n",
      "01/20/2026 12:57:40 - INFO - \t ***** Running Inference on 902 texts *****\n",
      "Inference: 100%|██████████| 902/902 [00:06<00:00, 146.21it/s]\n",
      "Files:   2%|▏         | 75/4409 [26:24<41:47:02, 34.71s/it, docs=48,296, chunks=96,763, docs/s=30.5]01/20/2026 12:57:47 - INFO - \t Tokenize 1445 inputs...\n",
      "Map: 100%|██████████| 1445/1445 [00:53<00:00, 26.89 examples/s]\n",
      "01/20/2026 12:58:41 - INFO - \t ***** Running Inference on 1445 texts *****\n",
      "Inference: 100%|██████████| 1445/1445 [00:12<00:00, 117.14it/s]\n",
      "Files:   2%|▏         | 76/4409 [27:31<53:38:19, 44.56s/it, docs=49,741, chunks=99,978, docs/s=30.1]01/20/2026 12:58:54 - INFO - \t Tokenize 660 inputs...\n",
      "Map: 100%|██████████| 660/660 [00:16<00:00, 40.88 examples/s]\n",
      "01/20/2026 12:59:11 - INFO - \t ***** Running Inference on 660 texts *****\n",
      "Inference: 100%|██████████| 660/660 [00:04<00:00, 145.72it/s]\n",
      "Files:   2%|▏         | 77/4409 [27:53<45:16:36, 37.63s/it, docs=50,401, chunks=101,292, docs/s=30.1]01/20/2026 12:59:16 - INFO - \t Tokenize 1012 inputs...\n",
      "Map: 100%|██████████| 1012/1012 [00:23<00:00, 43.38 examples/s]\n",
      "01/20/2026 12:59:39 - INFO - \t ***** Running Inference on 1012 texts *****\n",
      "Inference: 100%|██████████| 1012/1012 [00:06<00:00, 151.93it/s]\n",
      "Files:   2%|▏         | 78/4409 [28:24<42:48:56, 35.59s/it, docs=51,413, chunks=103,197, docs/s=30.2]01/20/2026 12:59:47 - INFO - \t Tokenize 1116 inputs...\n",
      "Map: 100%|██████████| 1116/1116 [00:26<00:00, 42.58 examples/s]\n",
      "01/20/2026 13:00:13 - INFO - \t ***** Running Inference on 1116 texts *****\n",
      "Inference: 100%|██████████| 1116/1116 [00:07<00:00, 149.56it/s]\n",
      "Files:   2%|▏         | 79/4409 [28:58<42:28:14, 35.31s/it, docs=52,529, chunks=105,276, docs/s=30.2]01/20/2026 13:00:21 - INFO - \t Tokenize 1343 inputs...\n",
      "Map: 100%|██████████| 1343/1343 [00:43<00:00, 30.56 examples/s]\n",
      "01/20/2026 13:01:06 - INFO - \t ***** Running Inference on 1343 texts *****\n",
      "Inference: 100%|██████████| 1343/1343 [00:11<00:00, 114.51it/s]\n",
      "Files:   2%|▏         | 80/4409 [29:55<50:18:31, 41.84s/it, docs=53,872, chunks=107,899, docs/s=30.0]01/20/2026 13:01:18 - INFO - \t Tokenize 743 inputs...\n",
      "Map: 100%|██████████| 743/743 [00:17<00:00, 43.55 examples/s]\n",
      "01/20/2026 13:01:36 - INFO - \t ***** Running Inference on 743 texts *****\n",
      "Inference: 100%|██████████| 743/743 [00:05<00:00, 142.54it/s]\n",
      "Files:   2%|▏         | 81/4409 [30:19<43:33:57, 36.24s/it, docs=54,615, chunks=109,306, docs/s=30.0]01/20/2026 13:01:42 - INFO - \t Tokenize 760 inputs...\n",
      "Map: 100%|██████████| 760/760 [00:16<00:00, 47.47 examples/s]\n",
      "01/20/2026 13:01:58 - INFO - \t ***** Running Inference on 760 texts *****\n",
      "Inference: 100%|██████████| 760/760 [00:04<00:00, 173.85it/s]\n",
      "Files:   2%|▏         | 82/4409 [30:40<38:04:55, 31.68s/it, docs=55,375, chunks=110,651, docs/s=30.1]01/20/2026 13:02:03 - INFO - \t Tokenize 1123 inputs...\n",
      "Map: 100%|██████████| 1123/1123 [00:24<00:00, 45.66 examples/s]\n",
      "01/20/2026 13:02:28 - INFO - \t ***** Running Inference on 1123 texts *****\n",
      "Inference: 100%|██████████| 1123/1123 [00:07<00:00, 159.80it/s]\n",
      "Files:   2%|▏         | 83/4409 [31:12<38:24:50, 31.97s/it, docs=56,498, chunks=112,722, docs/s=30.2]01/20/2026 13:02:35 - INFO - \t Tokenize 1242 inputs...\n",
      "Map: 100%|██████████| 1242/1242 [00:28<00:00, 43.94 examples/s]\n",
      "01/20/2026 13:03:04 - INFO - \t ***** Running Inference on 1242 texts *****\n",
      "Inference: 100%|██████████| 1242/1242 [00:08<00:00, 152.97it/s]\n",
      "Files:   2%|▏         | 84/4409 [31:50<40:21:07, 33.59s/it, docs=57,740, chunks=115,035, docs/s=30.2]01/20/2026 13:03:13 - INFO - \t Tokenize 995 inputs...\n",
      "Map: 100%|██████████| 995/995 [00:27<00:00, 36.69 examples/s]\n",
      "01/20/2026 13:03:40 - INFO - \t ***** Running Inference on 995 texts *****\n",
      "Inference: 100%|██████████| 995/995 [00:07<00:00, 126.73it/s]\n",
      "Files:   2%|▏         | 85/4409 [32:26<41:11:57, 34.30s/it, docs=58,735, chunks=117,083, docs/s=30.2]01/20/2026 13:03:49 - INFO - \t Tokenize 1139 inputs...\n",
      "Map: 100%|██████████| 1139/1139 [00:25<00:00, 44.50 examples/s]\n",
      "01/20/2026 13:04:15 - INFO - \t ***** Running Inference on 1139 texts *****\n",
      "Inference: 100%|██████████| 1139/1139 [00:07<00:00, 152.80it/s]\n",
      "Files:   2%|▏         | 86/4409 [33:00<41:08:48, 34.27s/it, docs=59,874, chunks=119,238, docs/s=30.2]01/20/2026 13:04:23 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:32<00:00, 46.83 examples/s]\n",
      "01/20/2026 13:04:55 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 151.04it/s]\n",
      "01/20/2026 13:05:06 - INFO - \t Tokenize 24 inputs...\n",
      "Map: 100%|██████████| 24/24 [00:00<00:00, 57.09 examples/s]\n",
      "01/20/2026 13:05:07 - INFO - \t ***** Running Inference on 24 texts *****\n",
      "Inference: 100%|██████████| 24/24 [00:00<00:00, 68.51it/s]\n",
      "Files:   2%|▏         | 87/4409 [33:44<44:47:03, 37.30s/it, docs=61,398, chunks=121,965, docs/s=30.3]01/20/2026 13:05:07 - INFO - \t Tokenize 1243 inputs...\n",
      "Map: 100%|██████████| 1243/1243 [00:31<00:00, 40.00 examples/s]\n",
      "01/20/2026 13:05:39 - INFO - \t ***** Running Inference on 1243 texts *****\n",
      "Inference: 100%|██████████| 1243/1243 [00:08<00:00, 144.09it/s]\n",
      "Files:   2%|▏         | 88/4409 [34:25<46:01:10, 38.34s/it, docs=62,641, chunks=124,413, docs/s=30.3]01/20/2026 13:05:48 - INFO - \t Tokenize 1138 inputs...\n",
      "Map: 100%|██████████| 1138/1138 [00:31<00:00, 36.46 examples/s]\n",
      "01/20/2026 13:06:20 - INFO - \t ***** Running Inference on 1138 texts *****\n",
      "Inference: 100%|██████████| 1138/1138 [00:08<00:00, 132.27it/s]\n",
      "Files:   2%|▏         | 89/4409 [35:06<46:54:55, 39.10s/it, docs=63,779, chunks=126,792, docs/s=30.3]01/20/2026 13:06:29 - INFO - \t Tokenize 1208 inputs...\n",
      "Map: 100%|██████████| 1208/1208 [00:35<00:00, 34.33 examples/s]\n",
      "01/20/2026 13:07:04 - INFO - \t ***** Running Inference on 1208 texts *****\n",
      "Inference: 100%|██████████| 1208/1208 [00:09<00:00, 129.98it/s]\n",
      "Files:   2%|▏         | 90/4409 [35:52<49:20:26, 41.13s/it, docs=64,987, chunks=129,134, docs/s=30.2]01/20/2026 13:07:15 - INFO - \t Tokenize 1436 inputs...\n",
      "Map: 100%|██████████| 1436/1436 [00:31<00:00, 45.27 examples/s]\n",
      "01/20/2026 13:07:47 - INFO - \t ***** Running Inference on 1436 texts *****\n",
      "Inference: 100%|██████████| 1436/1436 [00:09<00:00, 157.12it/s]\n",
      "Files:   2%|▏         | 91/4409 [36:34<49:44:21, 41.47s/it, docs=66,423, chunks=131,990, docs/s=30.3]01/20/2026 13:07:57 - INFO - \t Tokenize 966 inputs...\n",
      "Map: 100%|██████████| 966/966 [00:25<00:00, 38.07 examples/s]\n",
      "01/20/2026 13:08:23 - INFO - \t ***** Running Inference on 966 texts *****\n",
      "Inference: 100%|██████████| 966/966 [00:07<00:00, 121.46it/s]\n",
      "Files:   2%|▏         | 92/4409 [37:08<47:11:26, 39.35s/it, docs=67,389, chunks=134,018, docs/s=30.2]01/20/2026 13:08:31 - INFO - \t Tokenize 1152 inputs...\n",
      "Map: 100%|██████████| 1152/1152 [00:25<00:00, 45.51 examples/s]\n",
      "01/20/2026 13:08:57 - INFO - \t ***** Running Inference on 1152 texts *****\n",
      "Inference: 100%|██████████| 1152/1152 [00:07<00:00, 153.69it/s]\n",
      "Files:   2%|▏         | 93/4409 [37:42<45:10:31, 37.68s/it, docs=68,541, chunks=136,110, docs/s=30.3]01/20/2026 13:09:05 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:38<00:00, 38.91 examples/s]\n",
      "01/20/2026 13:09:44 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 135.97it/s]\n",
      "01/20/2026 13:09:56 - INFO - \t Tokenize 244 inputs...\n",
      "Map: 100%|██████████| 244/244 [00:05<00:00, 41.63 examples/s]\n",
      "01/20/2026 13:10:02 - INFO - \t ***** Running Inference on 244 texts *****\n",
      "Inference: 100%|██████████| 244/244 [00:01<00:00, 139.32it/s]\n",
      "Files:   2%|▏         | 94/4409 [38:41<52:53:55, 44.13s/it, docs=70,285, chunks=139,797, docs/s=30.3]01/20/2026 13:10:04 - INFO - \t Tokenize 986 inputs...\n",
      "Map: 100%|██████████| 986/986 [00:31<00:00, 31.30 examples/s]\n",
      "01/20/2026 13:10:36 - INFO - \t ***** Running Inference on 986 texts *****\n",
      "Inference: 100%|██████████| 986/986 [00:08<00:00, 113.92it/s]\n",
      "Files:   2%|▏         | 95/4409 [39:23<51:54:20, 43.31s/it, docs=71,271, chunks=141,926, docs/s=30.2]01/20/2026 13:10:46 - INFO - \t Tokenize 1361 inputs...\n",
      "Map: 100%|██████████| 1361/1361 [00:32<00:00, 41.40 examples/s]\n",
      "01/20/2026 13:11:19 - INFO - \t ***** Running Inference on 1361 texts *****\n",
      "Inference: 100%|██████████| 1361/1361 [00:09<00:00, 139.57it/s]\n",
      "Files:   2%|▏         | 96/4409 [40:07<52:11:35, 43.56s/it, docs=72,632, chunks=144,604, docs/s=30.2]01/20/2026 13:11:30 - INFO - \t Tokenize 852 inputs...\n",
      "Map: 100%|██████████| 852/852 [00:19<00:00, 44.70 examples/s]\n",
      "01/20/2026 13:11:49 - INFO - \t ***** Running Inference on 852 texts *****\n",
      "Inference: 100%|██████████| 852/852 [00:05<00:00, 155.56it/s]\n",
      "Files:   2%|▏         | 97/4409 [40:32<45:40:00, 38.13s/it, docs=73,484, chunks=146,204, docs/s=30.2]01/20/2026 13:11:55 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.31 examples/s]\n",
      "01/20/2026 13:12:30 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 149.65it/s]\n",
      "01/20/2026 13:12:41 - INFO - \t Tokenize 110 inputs...\n",
      "Map: 100%|██████████| 110/110 [00:03<00:00, 35.97 examples/s]\n",
      "01/20/2026 13:12:44 - INFO - \t ***** Running Inference on 110 texts *****\n",
      "Inference: 100%|██████████| 110/110 [00:01<00:00, 96.74it/s]\n",
      "Files:   2%|▏         | 98/4409 [41:23<50:05:39, 41.83s/it, docs=75,094, chunks=149,334, docs/s=30.2]01/20/2026 13:12:46 - INFO - \t Tokenize 1116 inputs...\n",
      "Map: 100%|██████████| 1116/1116 [00:26<00:00, 42.66 examples/s]\n",
      "01/20/2026 13:13:12 - INFO - \t ***** Running Inference on 1116 texts *****\n",
      "Inference: 100%|██████████| 1116/1116 [00:07<00:00, 144.81it/s]\n",
      "Files:   2%|▏         | 99/4409 [41:58<47:38:04, 39.79s/it, docs=76,210, chunks=151,469, docs/s=30.3]01/20/2026 13:13:21 - INFO - \t Tokenize 1189 inputs...\n",
      "Map: 100%|██████████| 1189/1189 [00:26<00:00, 45.55 examples/s]\n",
      "01/20/2026 13:13:47 - INFO - \t ***** Running Inference on 1189 texts *****\n",
      "Inference: 100%|██████████| 1189/1189 [00:07<00:00, 158.49it/s]\n",
      "Files:   2%|▏         | 100/4409 [42:33<45:54:24, 38.35s/it, docs=77,399, chunks=153,693, docs/s=30.3]01/20/2026 13:13:56 - INFO - \t Tokenize 815 inputs...\n",
      "Map: 100%|██████████| 815/815 [00:21<00:00, 38.70 examples/s]\n",
      "01/20/2026 13:14:17 - INFO - \t ***** Running Inference on 815 texts *****\n",
      "Inference: 100%|██████████| 815/815 [00:06<00:00, 127.85it/s]\n",
      "Files:   2%|▏         | 101/4409 [43:02<42:24:32, 35.44s/it, docs=78,214, chunks=155,402, docs/s=30.3]01/20/2026 13:14:24 - INFO - \t Tokenize 1075 inputs...\n",
      "Map: 100%|██████████| 1075/1075 [00:24<00:00, 44.02 examples/s]\n",
      "01/20/2026 13:14:49 - INFO - \t ***** Running Inference on 1075 texts *****\n",
      "Inference: 100%|██████████| 1075/1075 [00:07<00:00, 153.20it/s]\n",
      "Files:   2%|▏         | 102/4409 [43:34<41:21:50, 34.57s/it, docs=79,289, chunks=157,452, docs/s=30.3]01/20/2026 13:14:57 - INFO - \t Tokenize 1115 inputs...\n",
      "Map: 100%|██████████| 1115/1115 [00:23<00:00, 47.33 examples/s]\n",
      "01/20/2026 13:15:21 - INFO - \t ***** Running Inference on 1115 texts *****\n",
      "Inference: 100%|██████████| 1115/1115 [00:06<00:00, 167.84it/s]\n",
      "Files:   2%|▏         | 103/4409 [44:05<40:07:48, 33.55s/it, docs=80,404, chunks=159,487, docs/s=30.4]01/20/2026 13:15:28 - INFO - \t Tokenize 1287 inputs...\n",
      "Map: 100%|██████████| 1287/1287 [00:30<00:00, 42.52 examples/s]\n",
      "01/20/2026 13:15:59 - INFO - \t ***** Running Inference on 1287 texts *****\n",
      "Inference: 100%|██████████| 1287/1287 [00:09<00:00, 143.00it/s]\n",
      "Files:   2%|▏         | 104/4409 [44:46<42:37:18, 35.64s/it, docs=81,691, chunks=162,023, docs/s=30.4]01/20/2026 13:16:09 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:35<00:00, 42.32 examples/s]\n",
      "01/20/2026 13:16:45 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 146.37it/s]\n",
      "01/20/2026 13:16:56 - INFO - \t Tokenize 101 inputs...\n",
      "Map: 100%|██████████| 101/101 [00:02<00:00, 43.46 examples/s]\n",
      "01/20/2026 13:16:59 - INFO - \t ***** Running Inference on 101 texts *****\n",
      "Inference: 100%|██████████| 101/101 [00:00<00:00, 150.68it/s]\n",
      "Files:   2%|▏         | 105/4409 [45:37<48:03:12, 40.19s/it, docs=83,292, chunks=165,107, docs/s=30.4]01/20/2026 13:16:59 - INFO - \t Tokenize 1240 inputs...\n",
      "Map: 100%|██████████| 1240/1240 [00:26<00:00, 46.62 examples/s]\n",
      "01/20/2026 13:17:27 - INFO - \t ***** Running Inference on 1240 texts *****\n",
      "Inference: 100%|██████████| 1240/1240 [00:07<00:00, 160.15it/s]\n",
      "Files:   2%|▏         | 106/4409 [46:12<46:22:52, 38.80s/it, docs=84,532, chunks=167,334, docs/s=30.5]01/20/2026 13:17:35 - INFO - \t Tokenize 985 inputs...\n",
      "Map: 100%|██████████| 985/985 [00:21<00:00, 45.93 examples/s]\n",
      "01/20/2026 13:17:57 - INFO - \t ***** Running Inference on 985 texts *****\n",
      "Inference: 100%|██████████| 985/985 [00:05<00:00, 165.20it/s]\n",
      "Files:   2%|▏         | 107/4409 [46:41<42:38:27, 35.68s/it, docs=85,517, chunks=169,221, docs/s=30.5]01/20/2026 13:18:03 - INFO - \t Tokenize 1244 inputs...\n",
      "Map: 100%|██████████| 1244/1244 [00:28<00:00, 43.72 examples/s]\n",
      "01/20/2026 13:18:32 - INFO - \t ***** Running Inference on 1244 texts *****\n",
      "Inference: 100%|██████████| 1244/1244 [00:08<00:00, 149.69it/s]\n",
      "Files:   2%|▏         | 108/4409 [47:19<43:31:41, 36.43s/it, docs=86,761, chunks=171,662, docs/s=30.6]01/20/2026 13:18:42 - INFO - \t Tokenize 1136 inputs...\n",
      "Map: 100%|██████████| 1136/1136 [00:26<00:00, 42.24 examples/s]\n",
      "01/20/2026 13:19:09 - INFO - \t ***** Running Inference on 1136 texts *****\n",
      "Inference: 100%|██████████| 1136/1136 [00:07<00:00, 143.75it/s]\n",
      "Files:   2%|▏         | 109/4409 [47:55<43:22:36, 36.32s/it, docs=87,897, chunks=173,862, docs/s=30.6]01/20/2026 13:19:18 - INFO - \t Tokenize 1313 inputs...\n",
      "Map: 100%|██████████| 1313/1313 [00:33<00:00, 38.63 examples/s]\n",
      "01/20/2026 13:19:52 - INFO - \t ***** Running Inference on 1313 texts *****\n",
      "Inference: 100%|██████████| 1313/1313 [00:10<00:00, 130.69it/s]\n",
      "Files:   2%|▏         | 110/4409 [48:40<46:36:16, 39.03s/it, docs=89,210, chunks=176,598, docs/s=30.5]01/20/2026 13:20:03 - INFO - \t Tokenize 796 inputs...\n",
      "Map: 100%|██████████| 796/796 [00:21<00:00, 36.33 examples/s]\n",
      "01/20/2026 13:20:25 - INFO - \t ***** Running Inference on 796 texts *****\n",
      "Inference: 100%|██████████| 796/796 [00:06<00:00, 122.43it/s]\n",
      "Files:   3%|▎         | 111/4409 [49:10<43:12:43, 36.19s/it, docs=90,006, chunks=178,263, docs/s=30.5]01/20/2026 13:20:33 - INFO - \t Tokenize 1369 inputs...\n",
      "Map: 100%|██████████| 1369/1369 [00:37<00:00, 37.00 examples/s]\n",
      "01/20/2026 13:21:10 - INFO - \t ***** Running Inference on 1369 texts *****\n",
      "Inference: 100%|██████████| 1369/1369 [00:10<00:00, 127.86it/s]\n",
      "Files:   3%|▎         | 112/4409 [49:59<47:47:06, 40.03s/it, docs=91,375, chunks=181,221, docs/s=30.5]01/20/2026 13:21:22 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:38<00:00, 38.72 examples/s]\n",
      "01/20/2026 13:22:01 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 132.03it/s]\n",
      "01/20/2026 13:22:13 - INFO - \t Tokenize 23 inputs...\n",
      "Map: 100%|██████████| 23/23 [00:00<00:00, 47.09 examples/s]\n",
      "01/20/2026 13:22:14 - INFO - \t ***** Running Inference on 23 texts *****\n",
      "Inference: 100%|██████████| 23/23 [00:00<00:00, 106.09it/s]\n",
      "Files:   3%|▎         | 113/4409 [50:51<52:15:49, 43.80s/it, docs=92,898, chunks=184,385, docs/s=30.4]01/20/2026 13:22:14 - INFO - \t Tokenize 1111 inputs...\n",
      "Map: 100%|██████████| 1111/1111 [00:28<00:00, 39.22 examples/s]\n",
      "01/20/2026 13:22:43 - INFO - \t ***** Running Inference on 1111 texts *****\n",
      "Inference: 100%|██████████| 1111/1111 [00:07<00:00, 139.19it/s]\n",
      "Files:   3%|▎         | 114/4409 [51:29<50:05:45, 41.99s/it, docs=94,009, chunks=186,656, docs/s=30.4]01/20/2026 13:22:52 - INFO - \t Tokenize 1026 inputs...\n",
      "Map: 100%|██████████| 1026/1026 [00:23<00:00, 43.69 examples/s]\n",
      "01/20/2026 13:23:16 - INFO - \t ***** Running Inference on 1026 texts *****\n",
      "Inference: 100%|██████████| 1026/1026 [00:07<00:00, 133.82it/s]\n",
      "Files:   3%|▎         | 115/4409 [52:01<46:36:30, 39.08s/it, docs=95,035, chunks=188,595, docs/s=30.4]01/20/2026 13:23:24 - INFO - \t Tokenize 1013 inputs...\n",
      "Map: 100%|██████████| 1013/1013 [00:30<00:00, 32.82 examples/s]\n",
      "01/20/2026 13:23:56 - INFO - \t ***** Running Inference on 1013 texts *****\n",
      "Inference: 100%|██████████| 1013/1013 [00:08<00:00, 112.94it/s]\n",
      "Files:   3%|▎         | 116/4409 [52:43<47:24:39, 39.76s/it, docs=96,048, chunks=190,927, docs/s=30.4]01/20/2026 13:24:06 - INFO - \t Tokenize 1113 inputs...\n",
      "Map: 100%|██████████| 1113/1113 [00:25<00:00, 42.86 examples/s]\n",
      "01/20/2026 13:24:32 - INFO - \t ***** Running Inference on 1113 texts *****\n",
      "Inference: 100%|██████████| 1113/1113 [00:08<00:00, 133.99it/s]\n",
      "Files:   3%|▎         | 117/4409 [53:18<45:51:46, 38.47s/it, docs=97,161, chunks=193,077, docs/s=30.4]01/20/2026 13:24:41 - INFO - \t Tokenize 1178 inputs...\n",
      "Map: 100%|██████████| 1178/1178 [00:41<00:00, 28.73 examples/s]\n",
      "01/20/2026 13:25:23 - INFO - \t ***** Running Inference on 1178 texts *****\n",
      "Inference: 100%|██████████| 1178/1178 [00:10<00:00, 110.81it/s]\n",
      "Files:   3%|▎         | 118/4409 [54:11<51:02:29, 42.82s/it, docs=98,339, chunks=195,865, docs/s=30.2]01/20/2026 13:25:34 - INFO - \t Tokenize 1257 inputs...\n",
      "Map: 100%|██████████| 1257/1257 [00:29<00:00, 42.33 examples/s]\n",
      "01/20/2026 13:26:04 - INFO - \t ***** Running Inference on 1257 texts *****\n",
      "Inference: 100%|██████████| 1257/1257 [00:08<00:00, 147.47it/s]\n",
      "Files:   3%|▎         | 119/4409 [54:51<49:52:27, 41.85s/it, docs=99,596, chunks=198,327, docs/s=30.3]01/20/2026 13:26:14 - INFO - \t Tokenize 1196 inputs...\n",
      "Map: 100%|██████████| 1196/1196 [00:31<00:00, 38.28 examples/s]\n",
      "01/20/2026 13:26:45 - INFO - \t ***** Running Inference on 1196 texts *****\n",
      "Inference: 100%|██████████| 1196/1196 [00:09<00:00, 126.28it/s]\n",
      "Files:   3%|▎         | 120/4409 [55:33<49:54:11, 41.89s/it, docs=100,792, chunks=200,735, docs/s=30.2]01/20/2026 13:26:56 - INFO - \t Tokenize 1215 inputs...\n",
      "Map: 100%|██████████| 1215/1215 [00:34<00:00, 34.84 examples/s]\n",
      "01/20/2026 13:27:31 - INFO - \t ***** Running Inference on 1215 texts *****\n",
      "Inference: 100%|██████████| 1215/1215 [00:09<00:00, 124.35it/s]\n",
      "Files:   3%|▎         | 121/4409 [56:19<51:25:10, 43.17s/it, docs=102,007, chunks=203,348, docs/s=30.2]01/20/2026 13:27:42 - INFO - \t Tokenize 827 inputs...\n",
      "Map: 100%|██████████| 827/827 [00:19<00:00, 42.46 examples/s]\n",
      "01/20/2026 13:28:02 - INFO - \t ***** Running Inference on 827 texts *****\n",
      "Inference: 100%|██████████| 827/827 [00:05<00:00, 146.04it/s]\n",
      "Files:   3%|▎         | 122/4409 [56:45<45:22:16, 38.10s/it, docs=102,834, chunks=204,916, docs/s=30.2]01/20/2026 13:28:08 - INFO - \t Tokenize 900 inputs...\n",
      "Map: 100%|██████████| 900/900 [00:18<00:00, 48.96 examples/s]\n",
      "01/20/2026 13:28:27 - INFO - \t ***** Running Inference on 900 texts *****\n",
      "Inference: 100%|██████████| 900/900 [00:05<00:00, 171.04it/s]\n",
      "Files:   3%|▎         | 123/4409 [57:10<40:33:12, 34.06s/it, docs=103,734, chunks=206,536, docs/s=30.2]01/20/2026 13:28:33 - INFO - \t Tokenize 1071 inputs...\n",
      "Map: 100%|██████████| 1071/1071 [00:24<00:00, 43.80 examples/s]\n",
      "01/20/2026 13:28:58 - INFO - \t ***** Running Inference on 1071 texts *****\n",
      "Inference: 100%|██████████| 1071/1071 [00:07<00:00, 152.72it/s]\n",
      "Files:   3%|▎         | 124/4409 [57:43<40:05:09, 33.68s/it, docs=104,805, chunks=208,522, docs/s=30.3]01/20/2026 13:29:05 - INFO - \t Tokenize 673 inputs...\n",
      "Map: 100%|██████████| 673/673 [00:16<00:00, 41.78 examples/s]\n",
      "01/20/2026 13:29:22 - INFO - \t ***** Running Inference on 673 texts *****\n",
      "Inference: 100%|██████████| 673/673 [00:04<00:00, 143.91it/s]\n",
      "Files:   3%|▎         | 125/4409 [58:04<35:49:40, 30.11s/it, docs=105,478, chunks=209,820, docs/s=30.3]01/20/2026 13:29:27 - INFO - \t Tokenize 916 inputs...\n",
      "Map: 100%|██████████| 916/916 [00:22<00:00, 40.53 examples/s]\n",
      "01/20/2026 13:29:50 - INFO - \t ***** Running Inference on 916 texts *****\n",
      "Inference: 100%|██████████| 916/916 [00:06<00:00, 139.51it/s]\n",
      "Files:   3%|▎         | 126/4409 [58:35<35:59:53, 30.26s/it, docs=106,394, chunks=211,669, docs/s=30.3]01/20/2026 13:29:58 - INFO - \t Tokenize 974 inputs...\n",
      "Map: 100%|██████████| 974/974 [00:23<00:00, 41.37 examples/s]\n",
      "01/20/2026 13:30:22 - INFO - \t ***** Running Inference on 974 texts *****\n",
      "Inference: 100%|██████████| 974/974 [00:06<00:00, 144.24it/s]\n",
      "Files:   3%|▎         | 127/4409 [59:06<36:25:05, 30.62s/it, docs=107,368, chunks=213,582, docs/s=30.3]01/20/2026 13:30:29 - INFO - \t Tokenize 1268 inputs...\n",
      "Map: 100%|██████████| 1268/1268 [00:28<00:00, 44.33 examples/s]\n",
      "01/20/2026 13:30:58 - INFO - \t ***** Running Inference on 1268 texts *****\n",
      "Inference: 100%|██████████| 1268/1268 [00:07<00:00, 158.64it/s]\n",
      "Files:   3%|▎         | 128/4409 [59:44<39:01:11, 32.81s/it, docs=108,636, chunks=215,955, docs/s=30.3]01/20/2026 13:31:07 - INFO - \t Tokenize 1149 inputs...\n",
      "Map: 100%|██████████| 1149/1149 [00:32<00:00, 35.58 examples/s]\n",
      "01/20/2026 13:31:40 - INFO - \t ***** Running Inference on 1149 texts *****\n",
      "Inference: 100%|██████████| 1149/1149 [00:10<00:00, 105.25it/s]\n",
      "Files:   3%|▎         | 129/4409 [1:00:29<43:11:17, 36.33s/it, docs=109,785, chunks=218,348, docs/s=30.2]01/20/2026 13:31:52 - INFO - \t Tokenize 1405 inputs...\n",
      "Map: 100%|██████████| 1405/1405 [00:34<00:00, 40.44 examples/s]\n",
      "01/20/2026 13:32:27 - INFO - \t ***** Running Inference on 1405 texts *****\n",
      "Inference: 100%|██████████| 1405/1405 [00:10<00:00, 139.13it/s]\n",
      "Files:   3%|▎         | 130/4409 [1:01:15<46:41:50, 39.29s/it, docs=111,190, chunks=221,166, docs/s=30.3]01/20/2026 13:32:38 - INFO - \t Tokenize 968 inputs...\n",
      "Map: 100%|██████████| 968/968 [00:23<00:00, 40.59 examples/s]\n",
      "01/20/2026 13:33:02 - INFO - \t ***** Running Inference on 968 texts *****\n",
      "Inference: 100%|██████████| 968/968 [00:07<00:00, 135.44it/s]\n",
      "Files:   3%|▎         | 131/4409 [1:01:47<44:13:45, 37.22s/it, docs=112,158, chunks=223,052, docs/s=30.2]01/20/2026 13:33:10 - INFO - \t Tokenize 974 inputs...\n",
      "Map: 100%|██████████| 974/974 [00:24<00:00, 39.81 examples/s]\n",
      "01/20/2026 13:33:35 - INFO - \t ***** Running Inference on 974 texts *****\n",
      "Inference: 100%|██████████| 974/974 [00:07<00:00, 125.81it/s]\n",
      "Files:   3%|▎         | 132/4409 [1:02:21<42:52:42, 36.09s/it, docs=113,132, chunks=224,942, docs/s=30.2]01/20/2026 13:33:44 - INFO - \t Tokenize 1149 inputs...\n",
      "Map: 100%|██████████| 1149/1149 [00:25<00:00, 44.27 examples/s]\n",
      "01/20/2026 13:34:10 - INFO - \t ***** Running Inference on 1149 texts *****\n",
      "Inference: 100%|██████████| 1149/1149 [00:07<00:00, 152.43it/s]\n",
      "Files:   3%|▎         | 133/4409 [1:02:56<42:22:29, 35.68s/it, docs=114,281, chunks=227,107, docs/s=30.3]01/20/2026 13:34:18 - INFO - \t Tokenize 996 inputs...\n",
      "Map: 100%|██████████| 996/996 [00:21<00:00, 46.33 examples/s]\n",
      "01/20/2026 13:34:41 - INFO - \t ***** Running Inference on 996 texts *****\n",
      "Inference: 100%|██████████| 996/996 [00:06<00:00, 155.78it/s]\n",
      "Files:   3%|▎         | 134/4409 [1:03:25<40:01:49, 33.71s/it, docs=115,277, chunks=228,928, docs/s=30.3]01/20/2026 13:34:48 - INFO - \t Tokenize 1320 inputs...\n",
      "Map: 100%|██████████| 1320/1320 [00:31<00:00, 41.72 examples/s]\n",
      "01/20/2026 13:35:20 - INFO - \t ***** Running Inference on 1320 texts *****\n",
      "Inference: 100%|██████████| 1320/1320 [00:08<00:00, 146.71it/s]\n",
      "Files:   3%|▎         | 135/4409 [1:04:07<43:04:30, 36.28s/it, docs=116,597, chunks=231,551, docs/s=30.3]01/20/2026 13:35:30 - INFO - \t Tokenize 1277 inputs...\n",
      "Map: 100%|██████████| 1277/1277 [00:32<00:00, 38.95 examples/s]\n",
      "01/20/2026 13:36:03 - INFO - \t ***** Running Inference on 1277 texts *****\n",
      "Inference: 100%|██████████| 1277/1277 [00:09<00:00, 132.46it/s]\n",
      "Files:   3%|▎         | 136/4409 [1:04:51<45:54:22, 38.68s/it, docs=117,874, chunks=234,221, docs/s=30.3]01/20/2026 13:36:14 - INFO - \t Tokenize 1055 inputs...\n",
      "Map: 100%|██████████| 1055/1055 [00:22<00:00, 45.98 examples/s]\n",
      "01/20/2026 13:36:38 - INFO - \t ***** Running Inference on 1055 texts *****\n",
      "Inference: 100%|██████████| 1055/1055 [00:06<00:00, 162.52it/s]\n",
      "Files:   3%|▎         | 137/4409 [1:05:22<43:03:15, 36.28s/it, docs=118,929, chunks=236,142, docs/s=30.3]01/20/2026 13:36:45 - INFO - \t Tokenize 944 inputs...\n",
      "Map: 100%|██████████| 944/944 [00:24<00:00, 39.12 examples/s]\n",
      "01/20/2026 13:37:10 - INFO - \t ***** Running Inference on 944 texts *****\n",
      "Inference: 100%|██████████| 944/944 [00:06<00:00, 145.50it/s]\n",
      "Files:   3%|▎         | 138/4409 [1:05:54<41:23:42, 34.89s/it, docs=119,873, chunks=238,061, docs/s=30.3]01/20/2026 13:37:16 - INFO - \t Tokenize 1003 inputs...\n",
      "Map: 100%|██████████| 1003/1003 [00:23<00:00, 42.71 examples/s]\n",
      "01/20/2026 13:37:41 - INFO - \t ***** Running Inference on 1003 texts *****\n",
      "Inference: 100%|██████████| 1003/1003 [00:06<00:00, 145.44it/s]\n",
      "Files:   3%|▎         | 139/4409 [1:06:25<40:13:46, 33.92s/it, docs=120,876, chunks=240,006, docs/s=30.3]01/20/2026 13:37:48 - INFO - \t Tokenize 1015 inputs...\n",
      "Map: 100%|██████████| 1015/1015 [00:23<00:00, 44.04 examples/s]\n",
      "01/20/2026 13:38:12 - INFO - \t ***** Running Inference on 1015 texts *****\n",
      "Inference: 100%|██████████| 1015/1015 [00:06<00:00, 150.83it/s]\n",
      "Files:   3%|▎         | 140/4409 [1:06:56<39:10:20, 33.03s/it, docs=121,891, chunks=241,924, docs/s=30.3]01/20/2026 13:38:19 - INFO - \t Tokenize 1471 inputs...\n",
      "Map: 100%|██████████| 1471/1471 [00:37<00:00, 39.15 examples/s]\n",
      "01/20/2026 13:38:57 - INFO - \t ***** Running Inference on 1471 texts *****\n",
      "Inference: 100%|██████████| 1471/1471 [00:10<00:00, 138.40it/s]\n",
      "Files:   3%|▎         | 141/4409 [1:07:46<45:09:46, 38.09s/it, docs=123,362, chunks=244,775, docs/s=30.3]01/20/2026 13:39:09 - INFO - \t Tokenize 1095 inputs...\n",
      "Map: 100%|██████████| 1095/1095 [00:24<00:00, 44.61 examples/s]\n",
      "01/20/2026 13:39:34 - INFO - \t ***** Running Inference on 1095 texts *****\n",
      "Inference: 100%|██████████| 1095/1095 [00:07<00:00, 147.15it/s]\n",
      "Files:   3%|▎         | 142/4409 [1:08:19<43:24:11, 36.62s/it, docs=124,457, chunks=246,819, docs/s=30.4]01/20/2026 13:39:42 - INFO - \t Tokenize 1270 inputs...\n",
      "Map: 100%|██████████| 1270/1270 [00:43<00:00, 29.11 examples/s]\n",
      "01/20/2026 13:40:26 - INFO - \t ***** Running Inference on 1270 texts *****\n",
      "Inference: 100%|██████████| 1270/1270 [00:10<00:00, 123.82it/s]\n",
      "Files:   3%|▎         | 143/4409 [1:09:15<50:01:39, 42.22s/it, docs=125,727, chunks=249,556, docs/s=30.3]01/20/2026 13:40:37 - INFO - \t Tokenize 1157 inputs...\n",
      "Map: 100%|██████████| 1157/1157 [00:39<00:00, 29.10 examples/s]\n",
      "01/20/2026 13:41:18 - INFO - \t ***** Running Inference on 1157 texts *****\n",
      "Inference: 100%|██████████| 1157/1157 [00:10<00:00, 106.41it/s]\n",
      "Files:   3%|▎         | 144/4409 [1:10:07<53:31:41, 45.18s/it, docs=126,884, chunks=252,208, docs/s=30.2]01/20/2026 13:41:30 - INFO - \t Tokenize 1306 inputs...\n",
      "Map: 100%|██████████| 1306/1306 [00:28<00:00, 46.19 examples/s]\n",
      "01/20/2026 13:41:59 - INFO - \t ***** Running Inference on 1306 texts *****\n",
      "Inference: 100%|██████████| 1306/1306 [00:08<00:00, 163.14it/s]\n",
      "Files:   3%|▎         | 145/4409 [1:10:44<50:49:52, 42.92s/it, docs=128,190, chunks=254,607, docs/s=30.2]01/20/2026 13:42:07 - INFO - \t Tokenize 1346 inputs...\n",
      "Map: 100%|██████████| 1346/1346 [00:27<00:00, 48.26 examples/s]\n",
      "01/20/2026 13:42:36 - INFO - \t ***** Running Inference on 1346 texts *****\n",
      "Inference: 100%|██████████| 1346/1346 [00:08<00:00, 161.21it/s]\n",
      "Files:   3%|▎         | 146/4409 [1:11:22<49:02:06, 41.41s/it, docs=129,536, chunks=256,996, docs/s=30.2]01/20/2026 13:42:45 - INFO - \t Tokenize 1346 inputs...\n",
      "Map: 100%|██████████| 1346/1346 [00:35<00:00, 38.41 examples/s]\n",
      "01/20/2026 13:43:21 - INFO - \t ***** Running Inference on 1346 texts *****\n",
      "Inference: 100%|██████████| 1346/1346 [00:10<00:00, 132.49it/s]\n",
      "Files:   3%|▎         | 147/4409 [1:12:09<50:51:40, 42.96s/it, docs=130,882, chunks=259,822, docs/s=30.2]01/20/2026 13:43:32 - INFO - \t Tokenize 1087 inputs...\n",
      "Map: 100%|██████████| 1087/1087 [00:24<00:00, 44.29 examples/s]\n",
      "01/20/2026 13:43:57 - INFO - \t ***** Running Inference on 1087 texts *****\n",
      "Inference: 100%|██████████| 1087/1087 [00:07<00:00, 150.22it/s]\n",
      "Files:   3%|▎         | 148/4409 [1:12:42<47:16:57, 39.95s/it, docs=131,969, chunks=261,790, docs/s=30.3]01/20/2026 13:44:05 - INFO - \t Tokenize 939 inputs...\n",
      "Map: 100%|██████████| 939/939 [00:21<00:00, 44.27 examples/s]\n",
      "01/20/2026 13:44:26 - INFO - \t ***** Running Inference on 939 texts *****\n",
      "Inference: 100%|██████████| 939/939 [00:05<00:00, 162.30it/s]\n",
      "Files:   3%|▎         | 149/4409 [1:13:10<43:05:35, 36.42s/it, docs=132,908, chunks=263,593, docs/s=30.3]01/20/2026 13:44:33 - INFO - \t Tokenize 1100 inputs...\n",
      "Map: 100%|██████████| 1100/1100 [00:25<00:00, 43.67 examples/s]\n",
      "01/20/2026 13:44:59 - INFO - \t ***** Running Inference on 1100 texts *****\n",
      "Inference: 100%|██████████| 1100/1100 [00:07<00:00, 148.66it/s]\n",
      "Files:   3%|▎         | 150/4409 [1:13:44<42:11:10, 35.66s/it, docs=134,008, chunks=265,700, docs/s=30.3]01/20/2026 13:45:07 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:41<00:00, 36.27 examples/s]\n",
      "01/20/2026 13:45:49 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:12<00:00, 123.02it/s]\n",
      "01/20/2026 13:46:02 - INFO - \t Tokenize 826 inputs...\n",
      "Map: 100%|██████████| 826/826 [00:22<00:00, 36.30 examples/s]\n",
      "01/20/2026 13:46:25 - INFO - \t ***** Running Inference on 826 texts *****\n",
      "Inference: 100%|██████████| 826/826 [00:07<00:00, 114.70it/s]\n",
      "Files:   3%|▎         | 151/4409 [1:15:10<60:14:42, 50.94s/it, docs=136,334, chunks=270,755, docs/s=30.2]01/20/2026 13:46:33 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:39<00:00, 38.35 examples/s]\n",
      "01/20/2026 13:47:13 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 130.98it/s]\n",
      "01/20/2026 13:47:25 - INFO - \t Tokenize 1003 inputs...\n",
      "Map: 100%|██████████| 1003/1003 [00:20<00:00, 48.63 examples/s]\n",
      "01/20/2026 13:47:47 - INFO - \t ***** Running Inference on 1003 texts *****\n",
      "Inference: 100%|██████████| 1003/1003 [00:05<00:00, 171.36it/s]\n",
      "Files:   3%|▎         | 152/4409 [1:16:31<70:36:01, 59.70s/it, docs=138,837, chunks=275,571, docs/s=30.2]01/20/2026 13:47:53 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.61 examples/s]\n",
      "01/20/2026 13:48:29 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 144.07it/s]\n",
      "01/20/2026 13:48:40 - INFO - \t Tokenize 983 inputs...\n",
      "Map: 100%|██████████| 983/983 [00:19<00:00, 50.89 examples/s]\n",
      "01/20/2026 13:49:00 - INFO - \t ***** Running Inference on 983 texts *****\n",
      "Inference: 100%|██████████| 983/983 [00:05<00:00, 183.49it/s]\n",
      "Files:   3%|▎         | 153/4409 [1:17:43<75:02:06, 63.47s/it, docs=141,320, chunks=280,159, docs/s=30.3]01/20/2026 13:49:06 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.30 examples/s]\n",
      "01/20/2026 13:49:41 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 146.36it/s]\n",
      "01/20/2026 13:49:52 - INFO - \t Tokenize 628 inputs...\n",
      "Map: 100%|██████████| 628/628 [00:19<00:00, 32.82 examples/s]\n",
      "01/20/2026 13:50:12 - INFO - \t ***** Running Inference on 628 texts *****\n",
      "Inference: 100%|██████████| 628/628 [00:04<00:00, 148.48it/s]\n",
      "Files:   3%|▎         | 154/4409 [1:18:54<77:39:26, 65.70s/it, docs=143,448, chunks=284,427, docs/s=30.3]01/20/2026 13:50:17 - INFO - \t Tokenize 801 inputs...\n",
      "Map: 100%|██████████| 801/801 [00:18<00:00, 43.05 examples/s]\n",
      "01/20/2026 13:50:36 - INFO - \t ***** Running Inference on 801 texts *****\n",
      "Inference: 100%|██████████| 801/801 [00:05<00:00, 142.67it/s]\n",
      "Files:   4%|▎         | 155/4409 [1:19:19<63:20:13, 53.60s/it, docs=144,249, chunks=285,925, docs/s=30.3]01/20/2026 13:50:42 - INFO - \t Tokenize 1199 inputs...\n",
      "Map: 100%|██████████| 1199/1199 [00:29<00:00, 40.60 examples/s]\n",
      "01/20/2026 13:51:12 - INFO - \t ***** Running Inference on 1199 texts *****\n",
      "Inference: 100%|██████████| 1199/1199 [00:08<00:00, 148.56it/s]\n",
      "Files:   4%|▎         | 156/4409 [1:19:58<58:11:10, 49.25s/it, docs=145,448, chunks=288,322, docs/s=30.3]01/20/2026 13:51:21 - INFO - \t Tokenize 1344 inputs...\n",
      "Map: 100%|██████████| 1344/1344 [00:30<00:00, 43.85 examples/s]\n",
      "01/20/2026 13:51:52 - INFO - \t ***** Running Inference on 1344 texts *****\n",
      "Inference: 100%|██████████| 1344/1344 [00:09<00:00, 146.77it/s]\n",
      "Files:   4%|▎         | 157/4409 [1:20:39<55:17:01, 46.81s/it, docs=146,792, chunks=290,816, docs/s=30.3]01/20/2026 13:52:02 - INFO - \t Tokenize 1205 inputs...\n",
      "Map: 100%|██████████| 1205/1205 [00:31<00:00, 38.28 examples/s]\n",
      "01/20/2026 13:52:34 - INFO - \t ***** Running Inference on 1205 texts *****\n",
      "Inference: 100%|██████████| 1205/1205 [00:09<00:00, 132.49it/s]\n",
      "Files:   4%|▎         | 158/4409 [1:21:22<53:39:41, 45.44s/it, docs=147,997, chunks=293,286, docs/s=30.3]01/20/2026 13:52:44 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:33<00:00, 44.60 examples/s]\n",
      "01/20/2026 13:53:19 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 156.85it/s]\n",
      "01/20/2026 13:53:29 - INFO - \t Tokenize 988 inputs...\n",
      "Map: 100%|██████████| 988/988 [00:20<00:00, 48.08 examples/s]\n",
      "01/20/2026 13:53:51 - INFO - \t ***** Running Inference on 988 texts *****\n",
      "Inference: 100%|██████████| 988/988 [00:06<00:00, 161.09it/s]\n",
      "Files:   4%|▎         | 159/4409 [1:22:34<63:18:25, 53.62s/it, docs=150,485, chunks=297,824, docs/s=30.4]01/20/2026 13:53:57 - INFO - \t Tokenize 967 inputs...\n",
      "Map: 100%|██████████| 967/967 [00:31<00:00, 30.65 examples/s]\n",
      "01/20/2026 13:54:29 - INFO - \t ***** Running Inference on 967 texts *****\n",
      "Inference: 100%|██████████| 967/967 [00:08<00:00, 118.07it/s]\n",
      "Files:   4%|▎         | 160/4409 [1:23:15<58:54:27, 49.91s/it, docs=151,452, chunks=300,043, docs/s=30.3]01/20/2026 13:54:38 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:40<00:00, 37.08 examples/s]\n",
      "01/20/2026 13:55:20 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 136.23it/s]\n",
      "01/20/2026 13:55:32 - INFO - \t Tokenize 694 inputs...\n",
      "Map: 100%|██████████| 694/694 [00:19<00:00, 35.64 examples/s]\n",
      "01/20/2026 13:55:52 - INFO - \t ***** Running Inference on 694 texts *****\n",
      "Inference: 100%|██████████| 694/694 [00:05<00:00, 127.03it/s]\n",
      "Files:   4%|▎         | 161/4409 [1:24:35<69:22:21, 58.79s/it, docs=153,646, chunks=304,607, docs/s=30.3]01/20/2026 13:55:58 - INFO - \t Tokenize 1097 inputs...\n",
      "Map: 100%|██████████| 1097/1097 [00:29<00:00, 37.68 examples/s]\n",
      "01/20/2026 13:56:28 - INFO - \t ***** Running Inference on 1097 texts *****\n",
      "Inference: 100%|██████████| 1097/1097 [00:09<00:00, 117.94it/s]\n",
      "Files:   4%|▎         | 162/4409 [1:25:15<62:38:18, 53.10s/it, docs=154,743, chunks=306,751, docs/s=30.3]01/20/2026 13:56:38 - INFO - \t Tokenize 1034 inputs...\n",
      "Map: 100%|██████████| 1034/1034 [00:24<00:00, 41.99 examples/s]\n",
      "01/20/2026 13:57:03 - INFO - \t ***** Running Inference on 1034 texts *****\n",
      "Inference: 100%|██████████| 1034/1034 [00:06<00:00, 148.06it/s]\n",
      "Files:   4%|▎         | 163/4409 [1:25:48<55:31:15, 47.07s/it, docs=155,777, chunks=308,760, docs/s=30.3]01/20/2026 13:57:11 - INFO - \t Tokenize 1020 inputs...\n",
      "Map: 100%|██████████| 1020/1020 [00:29<00:00, 35.00 examples/s]\n",
      "01/20/2026 13:57:41 - INFO - \t ***** Running Inference on 1020 texts *****\n",
      "Inference: 100%|██████████| 1020/1020 [00:08<00:00, 123.19it/s]\n",
      "Files:   4%|▎         | 164/4409 [1:26:27<52:36:42, 44.62s/it, docs=156,797, chunks=310,957, docs/s=30.2]01/20/2026 13:57:50 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:30<00:00, 48.91 examples/s]\n",
      "01/20/2026 13:58:21 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:08<00:00, 169.96it/s]\n",
      "01/20/2026 13:58:31 - INFO - \t Tokenize 1050 inputs...\n",
      "Map: 100%|██████████| 1050/1050 [00:27<00:00, 38.19 examples/s]\n",
      "01/20/2026 13:58:59 - INFO - \t ***** Running Inference on 1050 texts *****\n",
      "Inference: 100%|██████████| 1050/1050 [00:08<00:00, 129.18it/s]\n",
      "Files:   4%|▎         | 165/4409 [1:27:45<64:33:13, 54.76s/it, docs=159,347, chunks=315,906, docs/s=30.3]01/20/2026 13:59:08 - INFO - \t Tokenize 1058 inputs...\n",
      "Map: 100%|██████████| 1058/1058 [00:23<00:00, 44.11 examples/s]\n",
      "01/20/2026 13:59:33 - INFO - \t ***** Running Inference on 1058 texts *****\n",
      "Inference: 100%|██████████| 1058/1058 [00:06<00:00, 152.42it/s]\n",
      "Files:   4%|▍         | 166/4409 [1:28:18<56:41:05, 48.09s/it, docs=160,405, chunks=317,978, docs/s=30.3]01/20/2026 13:59:41 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 41.19 examples/s]\n",
      "01/20/2026 14:00:18 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 140.26it/s]\n",
      "01/20/2026 14:00:29 - INFO - \t Tokenize 708 inputs...\n",
      "Map: 100%|██████████| 708/708 [00:17<00:00, 39.78 examples/s]\n",
      "01/20/2026 14:00:48 - INFO - \t ***** Running Inference on 708 texts *****\n",
      "Inference: 100%|██████████| 708/708 [00:05<00:00, 136.29it/s]\n",
      "Files:   4%|▍         | 167/4409 [1:29:30<65:24:52, 55.51s/it, docs=162,613, chunks=322,484, docs/s=30.3]01/20/2026 14:00:53 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 41.41 examples/s]\n",
      "01/20/2026 14:01:30 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 144.21it/s]\n",
      "01/20/2026 14:01:42 - INFO - \t Tokenize 939 inputs...\n",
      "Map: 100%|██████████| 939/939 [00:22<00:00, 41.86 examples/s]\n",
      "01/20/2026 14:02:05 - INFO - \t ***** Running Inference on 939 texts *****\n",
      "Inference: 100%|██████████| 939/939 [00:06<00:00, 142.55it/s]\n",
      "Files:   4%|▍         | 168/4409 [1:30:49<73:34:15, 62.45s/it, docs=165,052, chunks=327,242, docs/s=30.3]01/20/2026 14:02:12 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 41.32 examples/s]\n",
      "01/20/2026 14:02:49 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 142.56it/s]\n",
      "01/20/2026 14:03:00 - INFO - \t Tokenize 1239 inputs...\n",
      "Map: 100%|██████████| 1239/1239 [00:28<00:00, 43.13 examples/s]\n",
      "01/20/2026 14:03:30 - INFO - \t ***** Running Inference on 1239 texts *****\n",
      "Inference: 100%|██████████| 1239/1239 [00:08<00:00, 149.26it/s]\n",
      "Files:   4%|▍         | 169/4409 [1:32:16<82:07:18, 69.73s/it, docs=167,791, chunks=332,528, docs/s=30.3]01/20/2026 14:03:39 - INFO - \t Tokenize 1024 inputs...\n",
      "Map: 100%|██████████| 1024/1024 [00:20<00:00, 49.29 examples/s]\n",
      "01/20/2026 14:04:00 - INFO - \t ***** Running Inference on 1024 texts *****\n",
      "Inference: 100%|██████████| 1024/1024 [00:05<00:00, 172.47it/s]\n",
      "Files:   4%|▍         | 170/4409 [1:32:44<67:19:56, 57.18s/it, docs=168,815, chunks=334,383, docs/s=30.3]01/20/2026 14:04:07 - INFO - \t Tokenize 1214 inputs...\n",
      "Map: 100%|██████████| 1214/1214 [00:25<00:00, 48.20 examples/s]\n",
      "01/20/2026 14:04:33 - INFO - \t ***** Running Inference on 1214 texts *****\n",
      "Inference: 100%|██████████| 1214/1214 [00:07<00:00, 160.65it/s]\n",
      "Files:   4%|▍         | 171/4409 [1:33:18<59:15:47, 50.34s/it, docs=170,029, chunks=336,591, docs/s=30.4]01/20/2026 14:04:41 - INFO - \t Tokenize 762 inputs...\n",
      "Map: 100%|██████████| 762/762 [00:18<00:00, 41.06 examples/s]\n",
      "01/20/2026 14:05:00 - INFO - \t ***** Running Inference on 762 texts *****\n",
      "Inference: 100%|██████████| 762/762 [00:05<00:00, 131.92it/s]\n",
      "Files:   4%|▍         | 172/4409 [1:33:44<50:32:05, 42.94s/it, docs=170,791, chunks=338,090, docs/s=30.4]01/20/2026 14:05:07 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 40.84 examples/s]\n",
      "01/20/2026 14:05:44 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 129.60it/s]\n",
      "01/20/2026 14:05:57 - INFO - \t Tokenize 1020 inputs...\n",
      "Map: 100%|██████████| 1020/1020 [00:23<00:00, 43.09 examples/s]\n",
      "01/20/2026 14:06:21 - INFO - \t ***** Running Inference on 1020 texts *****\n",
      "Inference: 100%|██████████| 1020/1020 [00:06<00:00, 146.32it/s]\n",
      "Files:   4%|▍         | 173/4409 [1:35:06<64:21:01, 54.69s/it, docs=173,311, chunks=343,009, docs/s=30.4]01/20/2026 14:06:29 - INFO - \t Tokenize 1349 inputs...\n",
      "Map: 100%|██████████| 1349/1349 [00:35<00:00, 38.43 examples/s]\n",
      "01/20/2026 14:07:05 - INFO - \t ***** Running Inference on 1349 texts *****\n",
      "Inference: 100%|██████████| 1349/1349 [00:10<00:00, 132.28it/s]\n",
      "Files:   4%|▍         | 174/4409 [1:35:53<61:36:07, 52.37s/it, docs=174,660, chunks=345,779, docs/s=30.4]01/20/2026 14:07:16 - INFO - \t Tokenize 1055 inputs...\n",
      "Map: 100%|██████████| 1055/1055 [00:27<00:00, 38.14 examples/s]\n",
      "01/20/2026 14:07:44 - INFO - \t ***** Running Inference on 1055 texts *****\n",
      "Inference: 100%|██████████| 1055/1055 [00:08<00:00, 130.13it/s]\n",
      "Files:   4%|▍         | 175/4409 [1:36:30<56:14:02, 47.81s/it, docs=175,715, chunks=347,945, docs/s=30.3]01/20/2026 14:07:53 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:36<00:00, 41.44 examples/s]\n",
      "01/20/2026 14:08:30 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 144.44it/s]\n",
      "01/20/2026 14:08:41 - INFO - \t Tokenize 60 inputs...\n",
      "Map: 100%|██████████| 60/60 [00:01<00:00, 38.30 examples/s]\n",
      "01/20/2026 14:08:44 - INFO - \t ***** Running Inference on 60 texts *****\n",
      "Inference: 100%|██████████| 60/60 [00:00<00:00, 133.36it/s]\n",
      "Files:   4%|▍         | 176/4409 [1:37:21<57:28:39, 48.88s/it, docs=177,275, chunks=350,974, docs/s=30.3]01/20/2026 14:08:44 - INFO - \t Tokenize 1298 inputs...\n",
      "Map: 100%|██████████| 1298/1298 [00:32<00:00, 39.64 examples/s]\n",
      "01/20/2026 14:09:18 - INFO - \t ***** Running Inference on 1298 texts *****\n",
      "Inference: 100%|██████████| 1298/1298 [00:09<00:00, 133.23it/s]\n",
      "Files:   4%|▍         | 177/4409 [1:38:06<55:50:41, 47.50s/it, docs=178,573, chunks=353,664, docs/s=30.3]01/20/2026 14:09:29 - INFO - \t Tokenize 980 inputs...\n",
      "Map: 100%|██████████| 980/980 [00:22<00:00, 42.82 examples/s]\n",
      "01/20/2026 14:09:52 - INFO - \t ***** Running Inference on 980 texts *****\n",
      "Inference: 100%|██████████| 980/980 [00:06<00:00, 148.43it/s]\n",
      "Files:   4%|▍         | 178/4409 [1:38:37<49:59:10, 42.53s/it, docs=179,553, chunks=355,509, docs/s=30.3]01/20/2026 14:10:00 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:33<00:00, 45.00 examples/s]\n",
      "01/20/2026 14:10:34 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:09<00:00, 155.93it/s]\n",
      "01/20/2026 14:10:44 - INFO - \t Tokenize 945 inputs...\n",
      "Map: 100%|██████████| 945/945 [00:23<00:00, 39.83 examples/s]\n",
      "01/20/2026 14:11:08 - INFO - \t ***** Running Inference on 945 texts *****\n",
      "Inference: 100%|██████████| 945/945 [00:06<00:00, 136.46it/s]\n",
      "Files:   4%|▍         | 179/4409 [1:39:53<61:56:34, 52.72s/it, docs=181,998, chunks=360,304, docs/s=30.4]01/20/2026 14:11:16 - INFO - \t Tokenize 1138 inputs...\n",
      "Map: 100%|██████████| 1138/1138 [00:29<00:00, 39.20 examples/s]\n",
      "01/20/2026 14:11:46 - INFO - \t ***** Running Inference on 1138 texts *****\n",
      "Inference: 100%|██████████| 1138/1138 [00:08<00:00, 132.70it/s]\n",
      "Files:   4%|▍         | 180/4409 [1:40:32<57:05:21, 48.60s/it, docs=183,136, chunks=362,623, docs/s=30.4]01/20/2026 14:11:55 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:35<00:00, 42.58 examples/s]\n",
      "01/20/2026 14:12:31 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 147.14it/s]\n",
      "01/20/2026 14:12:42 - INFO - \t Tokenize 288 inputs...\n",
      "Map: 100%|██████████| 288/288 [00:07<00:00, 38.25 examples/s]\n",
      "01/20/2026 14:12:50 - INFO - \t ***** Running Inference on 288 texts *****\n",
      "Inference: 100%|██████████| 288/288 [00:02<00:00, 127.85it/s]\n",
      "Files:   4%|▍         | 181/4409 [1:41:30<60:20:43, 51.38s/it, docs=184,924, chunks=366,115, docs/s=30.4]01/20/2026 14:12:53 - INFO - \t Tokenize 1072 inputs...\n",
      "Map: 100%|██████████| 1072/1072 [00:26<00:00, 40.09 examples/s]\n",
      "01/20/2026 14:13:20 - INFO - \t ***** Running Inference on 1072 texts *****\n",
      "Inference: 100%|██████████| 1072/1072 [00:07<00:00, 139.01it/s]\n",
      "Files:   4%|▍         | 182/4409 [1:42:06<54:53:05, 46.74s/it, docs=185,996, chunks=368,233, docs/s=30.4]01/20/2026 14:13:29 - INFO - \t Tokenize 890 inputs...\n",
      "Map: 100%|██████████| 890/890 [00:23<00:00, 37.67 examples/s]\n",
      "01/20/2026 14:13:53 - INFO - \t ***** Running Inference on 890 texts *****\n",
      "Inference: 100%|██████████| 890/890 [00:06<00:00, 129.31it/s]\n",
      "Files:   4%|▍         | 183/4409 [1:42:38<49:44:28, 42.37s/it, docs=186,886, chunks=370,121, docs/s=30.3]01/20/2026 14:14:01 - INFO - \t Tokenize 1130 inputs...\n",
      "Map: 100%|██████████| 1130/1130 [00:30<00:00, 37.40 examples/s]\n",
      "01/20/2026 14:14:32 - INFO - \t ***** Running Inference on 1130 texts *****\n",
      "Inference: 100%|██████████| 1130/1130 [00:08<00:00, 128.27it/s]\n",
      "Files:   4%|▍         | 184/4409 [1:43:19<49:04:51, 41.82s/it, docs=188,016, chunks=372,596, docs/s=30.3]01/20/2026 14:14:41 - INFO - \t Tokenize 932 inputs...\n",
      "Map: 100%|██████████| 932/932 [00:25<00:00, 36.07 examples/s]\n",
      "01/20/2026 14:15:08 - INFO - \t ***** Running Inference on 932 texts *****\n",
      "Inference: 100%|██████████| 932/932 [00:07<00:00, 121.57it/s]\n",
      "Files:   4%|▍         | 185/4409 [1:43:54<46:38:36, 39.75s/it, docs=188,948, chunks=374,672, docs/s=30.3]01/20/2026 14:15:16 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:42<00:00, 35.04 examples/s]\n",
      "01/20/2026 14:16:00 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:13<00:00, 114.37it/s]\n",
      "01/20/2026 14:16:14 - INFO - \t Tokenize 496 inputs...\n",
      "Map: 100%|██████████| 496/496 [00:15<00:00, 32.72 examples/s]\n",
      "01/20/2026 14:16:30 - INFO - \t ***** Running Inference on 496 texts *****\n",
      "Inference: 100%|██████████| 496/496 [00:04<00:00, 109.14it/s]\n",
      "Files:   4%|▍         | 186/4409 [1:45:13<60:28:44, 51.56s/it, docs=190,944, chunks=379,180, docs/s=30.2]01/20/2026 14:16:36 - INFO - \t Tokenize 1279 inputs...\n",
      "Map: 100%|██████████| 1279/1279 [00:32<00:00, 39.26 examples/s]\n",
      "01/20/2026 14:17:09 - INFO - \t ***** Running Inference on 1279 texts *****\n",
      "Inference: 100%|██████████| 1279/1279 [00:09<00:00, 135.66it/s]\n",
      "Files:   4%|▍         | 187/4409 [1:45:56<57:39:58, 49.17s/it, docs=192,223, chunks=381,780, docs/s=30.2]01/20/2026 14:17:19 - INFO - \t Tokenize 1096 inputs...\n",
      "Map:   0%|          | 0/1096 [00:03<?, ? examples/s]\n",
      "Files:   4%|▍         | 187/4409 [1:46:01<39:53:35, 34.02s/it, docs=192,223, chunks=381,780, docs/s=30.2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mprocess_streaming_production\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mprocess_streaming_production\u001b[39m\u001b[34m(input_path, output_path, batch_size)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_df.is_empty():\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m chunks_df = \u001b[43mprocess_batch_threaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoref_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_pool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunks_df.is_empty():\n\u001b[32m    116\u001b[39m     file_chunks.append(chunks_df)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mprocess_batch_threaded\u001b[39m\u001b[34m(batch_df, coref_model, thread_pool)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m valid_texts:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m         preds = \u001b[43mcoref_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(valid_indices, preds):\n\u001b[32m     26\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/fastcoref/modeling.py:262\u001b[39m, in \u001b[36mCorefModel.predict\u001b[39m\u001b[34m(self, texts, is_split_into_words, max_tokens_in_batch, output_file)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m    260\u001b[39m     texts = [texts]\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    263\u001b[39m dataloader = \u001b[38;5;28mself\u001b[39m._prepare_batches(dataset, max_tokens_in_batch)\n\u001b[32m    265\u001b[39m preds = \u001b[38;5;28mself\u001b[39m._inference(dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/fastcoref/modeling.py:141\u001b[39m, in \u001b[36mCorefModel._create_dataset\u001b[39m\u001b[34m(self, texts, is_split_into_words)\u001b[39m\n\u001b[32m    138\u001b[39m     dataset[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m] = texts\n\u001b[32m    140\u001b[39m dataset = Dataset.from_dict(dataset)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokenizer\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnlp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/datasets/arrow_dataset.py:3343\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3341\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3342\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3343\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/datasets/arrow_dataset.py:3699\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3698\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3699\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3701\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/datasets/arrow_dataset.py:3649\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3648\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3649\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/datasets/arrow_dataset.py:3572\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3570\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3571\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/fastcoref/utilities/util.py:88\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(batch, tokenizer, nlp)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(batch, tokenizer, nlp):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nlp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m         tokenized_texts = \u001b[43mtokenize_with_spacy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     90\u001b[39m         tokenized_texts = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/fastcoref/utilities/util.py:128\u001b[39m, in \u001b[36mtokenize_with_spacy\u001b[39m\u001b[34m(texts, nlp)\u001b[39m\n\u001b[32m    126\u001b[39m disabled_pipe_names = [pipe_name \u001b[38;5;28;01mfor\u001b[39;00m pipe_name \u001b[38;5;129;01min\u001b[39;00m all_pipe_names \u001b[38;5;28;01mif\u001b[39;00m pipe_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_pipe_names]\n\u001b[32m    127\u001b[39m docs = nlp.pipe(texts, disable=disabled_pipe_names)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset_mapping\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenized_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtokens\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/spacy/language.py:1620\u001b[39m, in \u001b[36mLanguage.pipe\u001b[39m\u001b[34m(self, texts, as_tuples, batch_size, disable, component_cfg, n_process)\u001b[39m\n\u001b[32m   1618\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pipe \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[32m   1619\u001b[39m         docs = pipe(docs)\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1621\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/spacy/util.py:1761\u001b[39m, in \u001b[36m_pipe\u001b[39m\u001b[34m(docs, proc, name, default_error_handler, kwargs)\u001b[39m\n\u001b[32m   1751\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_pipe\u001b[39m(\n\u001b[32m   1752\u001b[39m     docs: Iterable[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m   1753\u001b[39m     proc: \u001b[33m\"\u001b[39m\u001b[33mPipeCallable\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1758\u001b[39m     kwargs: Mapping[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[32m   1759\u001b[39m ) -> Iterator[\u001b[33m\"\u001b[39m\u001b[33mDoc\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1760\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[33m\"\u001b[39m\u001b[33mpipe\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1761\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m proc.pipe(docs, **kwargs)\n\u001b[32m   1762\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1763\u001b[39m         \u001b[38;5;66;03m# We added some args for pipe that __call__ doesn't expect.\u001b[39;00m\n\u001b[32m   1764\u001b[39m         kwargs = \u001b[38;5;28mdict\u001b[39m(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/spacy/pipeline/trainable_pipe.pyx:75\u001b[39m, in \u001b[36mpipe\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/spacy/pipeline/tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ascii/gen-ai/.pixi/envs/pipeline/lib/python3.12/site-packages/thinc/layers/maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "process_streaming_production(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3131e853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastCoref model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/20/2026 14:36:49 - INFO - \t missing_keys: []\n",
      "01/20/2026 14:36:49 - INFO - \t unexpected_keys: []\n",
      "01/20/2026 14:36:49 - INFO - \t mismatched_keys: []\n",
      "01/20/2026 14:36:49 - INFO - \t error_msgs: []\n",
      "01/20/2026 14:36:49 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n",
      "Found 4409 parquet files\n",
      "Resuming: 191 files done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files:   0%|          | 0/4409 [00:00<?, ?it/s]01/20/2026 14:36:49 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [01:19<00:00, 18.81 examples/s]\n",
      "01/20/2026 14:38:08 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:12<00:00, 121.23it/s]\n",
      "01/20/2026 14:38:22 - INFO - \t Tokenize 778 inputs...\n",
      "Map: 100%|██████████| 778/778 [00:16<00:00, 46.19 examples/s]\n",
      "01/20/2026 14:38:39 - INFO - \t ***** Running Inference on 778 texts *****\n",
      "Inference: 100%|██████████| 778/778 [00:05<00:00, 154.29it/s]\n",
      "Files:   4%|▍         | 192/4409 [01:55<42:17,  1.66it/s, docs=2,278, chunks=4,602, docs/s=19.7, ETA=0.7h]01/20/2026 14:38:44 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:37<00:00, 40.33 examples/s]\n",
      "01/20/2026 14:39:22 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:11<00:00, 130.31it/s]\n",
      "01/20/2026 14:39:34 - INFO - \t Tokenize 208 inputs...\n",
      "Map: 100%|██████████| 208/208 [00:04<00:00, 48.11 examples/s]\n",
      "01/20/2026 14:39:38 - INFO - \t ***** Running Inference on 208 texts *****\n",
      "Inference: 100%|██████████| 208/208 [00:01<00:00, 154.99it/s]\n",
      "Files:   4%|▍         | 193/4409 [02:51<1:10:50,  1.01s/it, docs=3,986, chunks=7,960, docs/s=23.3, ETA=1.0h]01/20/2026 14:39:40 - INFO - \t Tokenize 1422 inputs...\n",
      "Map: 100%|██████████| 1422/1422 [00:36<00:00, 39.29 examples/s]\n",
      "01/20/2026 14:40:16 - INFO - \t ***** Running Inference on 1422 texts *****\n",
      "Inference: 100%|██████████| 1422/1422 [00:11<00:00, 127.30it/s]\n",
      "Files:   4%|▍         | 194/4409 [03:39<1:45:28,  1.50s/it, docs=5,408, chunks=10,830, docs/s=24.6, ETA=1.3h]01/20/2026 14:40:28 - INFO - \t Tokenize 1293 inputs...\n",
      "Map: 100%|██████████| 1293/1293 [00:31<00:00, 41.41 examples/s]\n",
      "01/20/2026 14:40:59 - INFO - \t ***** Running Inference on 1293 texts *****\n",
      "Inference: 100%|██████████| 1293/1293 [00:09<00:00, 137.87it/s]\n",
      "Files:   4%|▍         | 195/4409 [04:20<2:26:35,  2.09s/it, docs=6,701, chunks=13,379, docs/s=25.7, ETA=1.6h]01/20/2026 14:41:09 - INFO - \t Tokenize 1187 inputs...\n",
      "Map: 100%|██████████| 1187/1187 [00:32<00:00, 36.13 examples/s]\n",
      "01/20/2026 14:41:42 - INFO - \t ***** Running Inference on 1187 texts *****\n",
      "Inference: 100%|██████████| 1187/1187 [00:10<00:00, 118.53it/s]\n",
      "Files:   4%|▍         | 196/4409 [05:04<3:27:00,  2.95s/it, docs=7,888, chunks=15,941, docs/s=25.9, ETA=1.8h]01/20/2026 14:41:53 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:33<00:00, 44.62 examples/s]\n",
      "01/20/2026 14:42:27 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 144.96it/s]\n",
      "01/20/2026 14:42:38 - INFO - \t Tokenize 1159 inputs...\n",
      "Map: 100%|██████████| 1159/1159 [00:26<00:00, 44.41 examples/s]\n",
      "01/20/2026 14:43:04 - INFO - \t ***** Running Inference on 1159 texts *****\n",
      "Inference: 100%|██████████| 1159/1159 [00:07<00:00, 151.10it/s]\n",
      "Files:   4%|▍         | 197/4409 [06:24<6:00:13,  5.13s/it, docs=10,547, chunks=21,083, docs/s=27.5, ETA=2.3h]01/20/2026 14:43:13 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 43.74 examples/s]\n",
      "01/20/2026 14:43:47 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 149.68it/s]\n",
      "01/20/2026 14:43:58 - INFO - \t Tokenize 104 inputs...\n",
      "Map: 100%|██████████| 104/104 [00:02<00:00, 41.64 examples/s]\n",
      "01/20/2026 14:44:01 - INFO - \t ***** Running Inference on 104 texts *****\n",
      "Inference: 100%|██████████| 104/104 [00:00<00:00, 162.47it/s]\n",
      "Files:   4%|▍         | 198/4409 [07:12<8:00:06,  6.84s/it, docs=12,151, chunks=24,195, docs/s=28.1, ETA=2.6h]01/20/2026 14:44:02 - INFO - \t Tokenize 1500 inputs...\n",
      "Map: 100%|██████████| 1500/1500 [00:34<00:00, 44.06 examples/s]\n",
      "01/20/2026 14:44:36 - INFO - \t ***** Running Inference on 1500 texts *****\n",
      "Inference: 100%|██████████| 1500/1500 [00:10<00:00, 149.60it/s]\n",
      "01/20/2026 14:44:46 - INFO - \t Tokenize 862 inputs...\n",
      "Map: 100%|██████████| 862/862 [00:18<00:00, 47.52 examples/s]\n",
      "01/20/2026 14:45:05 - INFO - \t ***** Running Inference on 862 texts *****\n",
      "Inference: 100%|██████████| 862/862 [00:05<00:00, 167.42it/s]\n",
      "Files:   5%|▍         | 199/4409 [08:21<11:50:22, 10.12s/it, docs=14,513, chunks=28,725, docs/s=28.9, ETA=2.9h]01/20/2026 14:45:10 - INFO - \t Tokenize 1115 inputs...\n",
      "Map: 100%|██████████| 1115/1115 [00:28<00:00, 38.93 examples/s]\n",
      "01/20/2026 14:45:39 - INFO - \t ***** Running Inference on 1115 texts *****\n",
      "Inference: 100%|██████████| 1115/1115 [00:08<00:00, 128.25it/s]\n",
      "Files:   5%|▍         | 200/4409 [08:59<14:08:19, 12.09s/it, docs=15,628, chunks=30,950, docs/s=28.9, ETA=3.2h]01/20/2026 14:45:49 - INFO - \t Tokenize 1169 inputs...\n"
     ]
    }
   ],
   "source": [
    "process_streaming_fast(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ebe17df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_streaming(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Option 2: Polars streaming engine (for single large files)\n",
    "#process_with_streaming_engine(INPUT_PATH, OUTPUT_PATH, batch_size=BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
